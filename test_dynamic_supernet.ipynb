{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_padding(kernel_size):\n",
    "    if isinstance(kernel_size, tuple):\n",
    "        assert len(kernel_size) == 2, 'invalid kernel size: %s' % kernel_size\n",
    "        p1 = get_same_padding(kernel_size[0])\n",
    "        p2 = get_same_padding(kernel_size[1])\n",
    "        return p1, p2\n",
    "    assert isinstance(kernel_size, int), 'kernel size should be either `int` or `tuple`'\n",
    "    assert kernel_size % 2 > 0, 'kernel size should be odd number'\n",
    "    return kernel_size // 2\n",
    "\n",
    "\n",
    "def sub_filter_start_end(kernel_size, sub_kernel_size):\n",
    "    center = kernel_size // 2\n",
    "    dev = sub_kernel_size // 2\n",
    "    start, end = center - dev, center + dev + 1\n",
    "    assert end - start == sub_kernel_size\n",
    "    return start, end\n",
    "\n",
    "class DynamicConv2d(nn.Module):\n",
    "    KERNEL_TRANSFORM_MODE = 1  # None or 1\n",
    "\n",
    "    def __init__(self, max_in_channels, kernel_size_list, stride=1, dilation=1):\n",
    "        super(DynamicConv2d, self).__init__()\n",
    "\n",
    "        self.max_in_channels = max_in_channels\n",
    "        self.kernel_size_list = kernel_size_list\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            self.max_in_channels, self.max_in_channels, max(self.kernel_size_list), self.stride,\n",
    "            groups=self.max_in_channels, bias=False,\n",
    "        )\n",
    "\n",
    "        self._ks_set = list(set(self.kernel_size_list))\n",
    "        self._ks_set.sort()  # e.g., [3, 5, 7]\n",
    "        if self.KERNEL_TRANSFORM_MODE is not None:\n",
    "            # register scaling parameters\n",
    "            # 7to5_matrix, 5to3_matrix\n",
    "            scale_params = {}\n",
    "            for i in range(len(self._ks_set) - 1):\n",
    "                ks_small = self._ks_set[i]\n",
    "                ks_larger = self._ks_set[i + 1]\n",
    "                param_name = '%dto%d' % (ks_larger, ks_small)\n",
    "                scale_params['%s_matrix' % param_name] = Parameter(torch.eye(ks_small ** 2))\n",
    "            for name, param in scale_params.items():\n",
    "                self.register_parameter(name, param)\n",
    "\n",
    "        self.active_kernel_size = max(self.kernel_size_list)\n",
    "\n",
    "    def get_active_filter(self, in_channel, kernel_size):\n",
    "        out_channel = in_channel\n",
    "        max_kernel_size = max(self.kernel_size_list)\n",
    "\n",
    "        start, end = sub_filter_start_end(max_kernel_size, kernel_size)\n",
    "        filters = self.conv.weight[:out_channel, :in_channel, start:end, start:end]\n",
    "        if self.KERNEL_TRANSFORM_MODE is not None and kernel_size < max_kernel_size:\n",
    "            start_filter = self.conv.weight[:out_channel, :in_channel, :, :]  # start with max kernel\n",
    "            for i in range(len(self._ks_set) - 1, 0, -1):\n",
    "                src_ks = self._ks_set[i]\n",
    "                if src_ks <= kernel_size:\n",
    "                    break\n",
    "                target_ks = self._ks_set[i - 1]\n",
    "                start, end = sub_filter_start_end(src_ks, target_ks)\n",
    "                _input_filter = start_filter[:, :, start:end, start:end]\n",
    "                _input_filter = _input_filter.contiguous()\n",
    "                _input_filter = _input_filter.view(_input_filter.size(0), _input_filter.size(1), -1)\n",
    "                _input_filter = _input_filter.view(-1, _input_filter.size(2))\n",
    "                _input_filter = F.linear(\n",
    "                    _input_filter, self.__getattr__('%dto%d_matrix' % (src_ks, target_ks)),\n",
    "                )\n",
    "                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks ** 2)\n",
    "                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks, target_ks)\n",
    "                start_filter = _input_filter\n",
    "            filters = start_filter\n",
    "        return filters\n",
    "\n",
    "    def forward(self, x, kernel_size=None):\n",
    "        if kernel_size is None:\n",
    "            kernel_size = self.active_kernel_size\n",
    "        in_channel = x.size(1)\n",
    "\n",
    "        filters = self.get_active_filter(in_channel, kernel_size).contiguous()\n",
    "\n",
    "        padding = get_same_padding(kernel_size)\n",
    "        y = F.conv2d(\n",
    "            x, filters, None, self.stride, padding, self.dilation, in_channel\n",
    "        )\n",
    "        return y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 Subnet select 방식은 gumbel_softmax의 gradient에 영향을 주지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gumbel input:  tensor([[0.3260, 0.3021, 0.7876]], requires_grad=True)\n",
      "gumbel grad:  None\n",
      "tensor([[0., 0., 1.]], grad_fn=<AddBackward0>)\n",
      "dconv.weight.grad:  tensor([[-39.0841, -38.3947, -35.4702, -32.7624, -27.6951, -30.1235, -19.3788],\n",
      "        [-47.4388, -45.1476, -42.0656, -38.7817, -34.3943, -34.8257, -24.2159],\n",
      "        [-46.3515, -44.9403, -43.1118, -40.8910, -36.9794, -38.1342, -27.0899],\n",
      "        [-51.3097, -48.8564, -45.5509, -43.1278, -37.4699, -37.4104, -26.0948],\n",
      "        [-43.1267, -42.3219, -38.0915, -35.4915, -32.3780, -32.4965, -22.2510],\n",
      "        [-37.7014, -37.8889, -34.3843, -32.3811, -28.6648, -28.7608, -18.9653],\n",
      "        [-35.2465, -36.4609, -34.5478, -31.2372, -27.7822, -27.5038, -18.4004]])\n",
      "dconv.convert.grad:  torch.Size([25, 25]) None\n",
      "gumbel grad:  None\n"
     ]
    }
   ],
   "source": [
    "ks_list = [3,5,7]\n",
    "\n",
    "dconv = DynamicConv2d(64, ks_list, stride=1, dilation=1)\n",
    "\n",
    "gumbel_input = torch.randn(1, len(ks_list), requires_grad=True)\n",
    "\n",
    "print(\"gumbel input: \", gumbel_input)\n",
    "print(\"gumbel grad: \", gumbel_input.grad)\n",
    "hard_gumbel = F.gumbel_softmax(gumbel_input, tau=1, hard=True)\n",
    "print(hard_gumbel)\n",
    "\n",
    "out = dconv.forward(torch.randn(1, 64, 32, 32), kernel_size=ks_list[torch.argmax(hard_gumbel)])\n",
    "out.sum().backward()\n",
    "print(\"dconv.weight.grad: \", dconv.conv.weight.grad[0,0])\n",
    "print(\"dconv.convert.grad: \", dconv.get_parameter('7to5_matrix').shape, dconv.get_parameter('7to5_matrix').grad)\n",
    "print(\"gumbel grad: \", gumbel_input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8234, -0.3526, -0.5259], requires_grad=True)\n",
      "tensor([0., 1., 0.], grad_fn=<AddBackward0>)\n",
      "torch.Size([3])\n",
      "tensor([[ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [ 0.5743, -2.1588,  1.8553,  0.0300, -0.2108, -0.5413,  0.3284, -0.0719,\n",
      "          1.2868, -1.3926, -0.1215,  0.5518, -0.6554, -0.1868, -0.5725, -1.9607,\n",
      "          1.0544,  0.9271, -1.1055, -0.1890,  1.3631,  0.1581,  0.6159,  0.1158,\n",
      "          0.8314, -1.8248,  0.4751,  0.6885,  0.3055,  0.6019, -1.5743, -0.7610,\n",
      "         -0.0416,  1.4414, -0.8553, -0.2531, -1.0224, -0.0356,  1.8827, -1.2930,\n",
      "          1.0807,  0.9998,  0.4080, -0.3304, -1.2408, -0.5842, -1.0518,  0.5706,\n",
      "          1.5216, -1.0245, -0.5693, -1.3960,  0.8188, -0.2100,  0.2586, -0.4688,\n",
      "         -1.2704,  1.0545, -0.3688,  0.9219, -0.0830, -0.3836,  0.5476, -0.9365],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([-0.0793,  2.4606, -2.3813])\n"
     ]
    }
   ],
   "source": [
    "gumbel_input = torch.randn(len(ks_list), requires_grad=True)\n",
    "print(gumbel_input)\n",
    "test_weight = torch.randn(len(ks_list), 64)\n",
    "hard_gumbel = F.gumbel_softmax(gumbel_input, tau=1, hard=True)\n",
    "print(hard_gumbel)\n",
    "print(hard_gumbel.shape)\n",
    "out = hard_gumbel.unsqueeze(1) * test_weight\n",
    "print(out)\n",
    "out.sum().backward()\n",
    "print(gumbel_input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5to3_matrix\n",
      "7to5_matrix\n",
      "conv.weight\n"
     ]
    }
   ],
   "source": [
    "for n, p in dconv.named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicGumbelConv2d(nn.Module):\n",
    "    KERNEL_TRANSFORM_MODE = None  # None or 1\n",
    "\n",
    "    def __init__(self, max_in_channels, kernel_size_list, stride=1, dilation=1):\n",
    "        super(DynamicGumbelConv2d, self).__init__()\n",
    "\n",
    "        self.max_in_channels = max_in_channels\n",
    "        self.kernel_size_list = kernel_size_list\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            self.max_in_channels, self.max_in_channels, max(self.kernel_size_list), self.stride,\n",
    "            groups=self.max_in_channels, bias=False,\n",
    "        )\n",
    "\n",
    "        self._ks_set = list(set(self.kernel_size_list))\n",
    "        self._ks_set.sort()  # e.g., [3, 5, 7]\n",
    "        if self.KERNEL_TRANSFORM_MODE is not None:\n",
    "            # register scaling parameters\n",
    "            # 7to5_matrix, 5to3_matrix\n",
    "            scale_params = {}\n",
    "            for i in range(len(self._ks_set) - 1):\n",
    "                ks_small = self._ks_set[i]\n",
    "                ks_larger = self._ks_set[i + 1]\n",
    "                param_name = '%dto%d' % (ks_larger, ks_small)\n",
    "                scale_params['%s_matrix' % param_name] = Parameter(torch.eye(ks_small ** 2))\n",
    "            for name, param in scale_params.items():\n",
    "                self.register_parameter(name, param)\n",
    "\n",
    "        self.active_kernel_size = max(self.kernel_size_list)\n",
    "\n",
    "    def get_active_filter(self, in_channel, kernel_size):\n",
    "        out_channel = in_channel\n",
    "        max_kernel_size = max(self.kernel_size_list)\n",
    "\n",
    "        start, end = sub_filter_start_end(max_kernel_size, kernel_size)\n",
    "        filters = self.conv.weight[:out_channel, :in_channel, start:end, start:end]\n",
    "        if self.KERNEL_TRANSFORM_MODE is not None and kernel_size < max_kernel_size:\n",
    "            start_filter = self.conv.weight[:out_channel, :in_channel, :, :]  # start with max kernel\n",
    "            for i in range(len(self._ks_set) - 1, 0, -1):\n",
    "                src_ks = self._ks_set[i]\n",
    "                if src_ks <= kernel_size:\n",
    "                    break\n",
    "                target_ks = self._ks_set[i - 1]\n",
    "                start, end = sub_filter_start_end(src_ks, target_ks)\n",
    "                _input_filter = start_filter[:, :, start:end, start:end]\n",
    "                _input_filter = _input_filter.contiguous()\n",
    "                _input_filter = _input_filter.view(_input_filter.size(0), _input_filter.size(1), -1)\n",
    "                _input_filter = _input_filter.view(-1, _input_filter.size(2))\n",
    "                _input_filter = F.linear(\n",
    "                    _input_filter, self.__getattr__('%dto%d_matrix' % (src_ks, target_ks)),\n",
    "                )\n",
    "                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks ** 2)\n",
    "                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks, target_ks)\n",
    "                start_filter = _input_filter\n",
    "            filters = start_filter\n",
    "        return filters\n",
    "\n",
    "    def forward(self, x, kernel_size=None):\n",
    "        if kernel_size is None:\n",
    "            kernel_size = self.active_kernel_size\n",
    "        in_channel = x.size(1)\n",
    "\n",
    "        filters = self.get_active_filter(in_channel, kernel_size).contiguous()\n",
    "\n",
    "        padding = get_same_padding(kernel_size)\n",
    "        y = F.conv2d(\n",
    "            x, filters, None, self.stride, padding, self.dilation, in_channel\n",
    "        )\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.randn(4, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.4480e-38, 4.5860e-41, 4.9810e-38, 4.5860e-41, 5.6306e-38, 4.5860e-41,\n",
      "         4.9810e-38, 4.5860e-41, 5.7646e-38, 4.5860e-41, 5.7680e-38, 4.5860e-41,\n",
      "         5.6462e-38, 4.5860e-41, 5.7679e-38, 4.5860e-41, 5.4165e-38, 4.5860e-41,\n",
      "         5.8494e-38, 4.5860e-41],\n",
      "        [5.6407e-38, 4.5860e-41, 5.8585e-38, 4.5860e-41, 5.9143e-38, 4.5860e-41,\n",
      "         5.7646e-38, 4.5860e-41, 4.6185e-38, 4.5860e-41, 5.9194e-38, 4.5860e-41,\n",
      "         5.3741e-38, 4.5860e-41, 5.9150e-38, 4.5860e-41, 5.5330e-38, 4.5860e-41,\n",
      "         5.6407e-38, 4.5860e-41],\n",
      "        [4.9811e-38, 4.5860e-41, 5.4480e-38, 4.5860e-41, 5.8696e-38, 4.5860e-41,\n",
      "         5.6912e-38, 4.5860e-41, 5.9143e-38, 4.5860e-41, 5.6462e-38, 4.5860e-41,\n",
      "         5.9150e-38, 4.5860e-41, 5.6205e-38, 4.5860e-41, 5.7646e-38, 4.5860e-41,\n",
      "         4.9811e-38, 4.5860e-41],\n",
      "        [5.6407e-38, 4.5860e-41, 5.8585e-38, 4.5860e-41, 5.9143e-38, 4.5860e-41,\n",
      "         5.7646e-38, 4.5860e-41, 4.6185e-38, 4.5860e-41, 5.9194e-38, 4.5860e-41,\n",
      "         5.4480e-38, 4.5860e-41, 5.8696e-38, 4.5860e-41, 5.6912e-38, 4.5860e-41,\n",
      "         5.4480e-38, 4.5860e-41]])\n",
      "tensor([[ -2.0075,  -4.7176,  -5.3439,  -4.4562,  -4.8529,  -2.3614,  -1.7760,\n",
      "          -4.4296,  -7.6399,  -2.8446,  -2.5243,  -3.8023,  -1.3166,  -5.7751,\n",
      "          -5.7178,  -7.7085,  -1.6573,  -3.8653,  -4.9184,  -3.9052],\n",
      "        [ -2.7930,  -3.5472,  -0.9545,  -3.5364,  -4.0273,  -2.6175,  -3.3846,\n",
      "          -6.3682,  -5.0657,  -5.9720,  -4.8064,  -2.1951,  -4.9355,  -4.4109,\n",
      "          -6.7498,  -3.8518,  -2.3102,  -2.4505,  -1.8706,  -5.4732],\n",
      "        [ -1.0774,  -2.6631,  -3.6190,  -4.4745,  -2.4058,  -6.8189,  -2.6536,\n",
      "          -6.1724,  -4.1132,  -2.7987,  -3.5026,  -3.7911,  -1.6152,  -3.8122,\n",
      "          -5.1310,  -5.1451,  -4.2729,  -1.8661,  -5.3139,  -3.0674],\n",
      "        [ -2.1374,  -8.6168,  -3.7110,  -2.8335,  -2.5272,  -4.6448,  -2.0184,\n",
      "          -3.1341,  -3.9983,  -5.8668,  -3.2562, -10.3357,  -2.1167,  -4.2905,\n",
      "          -6.0733,  -1.5839,  -5.8070,  -2.1953,  -1.6979,  -4.3020]])\n",
      "tensor([[1.2015e-01, 7.9938e-03, 4.2731e-03, 1.0381e-02, 6.9817e-03, 8.4334e-02,\n",
      "         1.5145e-01, 1.0662e-02, 4.3010e-04, 5.2017e-02, 7.1658e-02, 1.9964e-02,\n",
      "         2.3976e-01, 2.7763e-03, 2.9401e-03, 4.0161e-04, 1.7054e-01, 1.8745e-02,\n",
      "         6.5393e-03, 1.8011e-02],\n",
      "        [5.3509e-02, 2.5171e-02, 3.3642e-01, 2.5443e-02, 1.5573e-02, 6.3776e-02,\n",
      "         2.9615e-02, 1.4987e-03, 5.5133e-03, 2.2275e-03, 7.1453e-03, 9.7298e-02,\n",
      "         6.2802e-03, 1.0612e-02, 1.0233e-03, 1.8560e-02, 8.6714e-02, 7.5364e-02,\n",
      "         1.3459e-01, 3.6683e-03],\n",
      "        [2.8493e-01, 5.8358e-02, 2.2435e-02, 9.5371e-03, 7.5479e-02, 9.1462e-04,\n",
      "         5.8913e-02, 1.7459e-03, 1.3687e-02, 5.0957e-02, 2.5205e-02, 1.8889e-02,\n",
      "         1.6642e-01, 1.8495e-02, 4.9463e-03, 4.8770e-03, 1.1667e-02, 1.2948e-01,\n",
      "         4.1195e-03, 3.8950e-02],\n",
      "        [1.0001e-01, 1.5347e-04, 2.0730e-02, 4.9856e-02, 6.7719e-02, 8.1481e-03,\n",
      "         1.1264e-01, 3.6911e-02, 1.5554e-02, 2.4008e-03, 3.2668e-02, 2.7511e-05,\n",
      "         1.0209e-01, 1.1613e-02, 1.9529e-03, 1.7393e-01, 2.5487e-03, 9.4373e-02,\n",
      "         1.5519e-01, 1.1480e-02]])\n"
     ]
    }
   ],
   "source": [
    "empty_g = torch.empty_like(g)\n",
    "print(empty_g)\n",
    "gumbel = empty_g.exponential_().log()\n",
    "out = g.log_softmax(dim=1) + gumbel\n",
    "print(out)\n",
    "print(torch.softmax(out, dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "x = torch.randn((2, 10, 4, 4), requires_grad=True)\n",
    "bit_mask_module = nn.Linear(10*4*4, n) # 10 channel\n",
    "conv = nn.Conv2d(10, n, 3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bit_mask = F.gumbel_softmax(bit_mask_module(x.view(x.size(0), -1)).view(-1, n).contiguous(), tau=1, hard=True)\n",
    "bit_mask = bit_mask.view(-1, n)\n",
    "bit_mask_l = bit_mask\n",
    "print(bit_mask)\n",
    "#conv_out_list = [F.conv2d(x, conv.weight, None, 1, 1, 1, 0) * bit_mask[:, i:i+1] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_mask_module.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17027/1385790786.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:478.)\n",
      "  print(bit_mask.grad)\n"
     ]
    }
   ],
   "source": [
    "bit_mask.sum().backward()\n",
    "print(bit_mask.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.]], grad_fn=<SliceBackward0>)\n",
      "tensor([[[[-0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0854, -0.1723],\n",
      "          [-1.4785,  0.3930]],\n",
      "\n",
      "         [[ 0.3277, -0.0673],\n",
      "          [-0.6838,  0.0519]]]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.],\n",
      "        [0.]], grad_fn=<SliceBackward0>)\n",
      "tensor([[[[-0., 0.],\n",
      "          [0., -0.]],\n",
      "\n",
      "         [[-0., -0.],\n",
      "          [0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., 0.],\n",
      "          [0., -0.]],\n",
      "\n",
      "         [[-0., -0.],\n",
      "          [0., 0.]]]], grad_fn=<MulBackward0>)\n",
      "tensor([[1.],\n",
      "        [0.]], grad_fn=<SliceBackward0>)\n",
      "tensor([[[[-0.6014, -0.0896],\n",
      "          [ 0.4359,  0.4022]],\n",
      "\n",
      "         [[ 0.6617,  0.1109],\n",
      "          [-0.3586,  0.8877]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000]]]], grad_fn=<MulBackward0>)\n",
      "tensor([[0.],\n",
      "        [0.]], grad_fn=<SliceBackward0>)\n",
      "tensor([[[[0., 0.],\n",
      "          [-0., 0.]],\n",
      "\n",
      "         [[-0., 0.],\n",
      "          [-0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0.],\n",
      "          [-0., 0.]],\n",
      "\n",
      "         [[-0., 0.],\n",
      "          [-0., -0.]]]], grad_fn=<MulBackward0>)\n",
      "tensor([[[[[-0.0000, -0.0000],\n",
      "           [-0.0000,  0.0000]],\n",
      "\n",
      "          [[ 0.0000, -0.0000],\n",
      "           [-0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "         [[[-0.0854, -0.1723],\n",
      "           [-1.4785,  0.3930]],\n",
      "\n",
      "          [[ 0.3277, -0.0673],\n",
      "           [-0.6838,  0.0519]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.0000,  0.0000],\n",
      "           [ 0.0000, -0.0000]],\n",
      "\n",
      "          [[-0.0000, -0.0000],\n",
      "           [ 0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "         [[[-0.0000,  0.0000],\n",
      "           [ 0.0000, -0.0000]],\n",
      "\n",
      "          [[-0.0000, -0.0000],\n",
      "           [ 0.0000,  0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-0.6014, -0.0896],\n",
      "           [ 0.4359,  0.4022]],\n",
      "\n",
      "          [[ 0.6617,  0.1109],\n",
      "           [-0.3586,  0.8877]]],\n",
      "\n",
      "\n",
      "         [[[-0.0000, -0.0000],\n",
      "           [ 0.0000,  0.0000]],\n",
      "\n",
      "          [[ 0.0000,  0.0000],\n",
      "           [-0.0000,  0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 0.0000,  0.0000],\n",
      "           [-0.0000,  0.0000]],\n",
      "\n",
      "          [[-0.0000,  0.0000],\n",
      "           [-0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "         [[[ 0.0000,  0.0000],\n",
      "           [-0.0000,  0.0000]],\n",
      "\n",
      "          [[-0.0000,  0.0000],\n",
      "           [-0.0000, -0.0000]]]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_list = []\n",
    "for i in range(n):\n",
    "    print(bit_mask[:, i:i+1])\n",
    "    out = F.conv2d(x, conv.weight, None, 1, 0, 1, 1)[:,i,:,:] * bit_mask[:, i].unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "    print(out)\n",
    "    out_list.append(out)\n",
    "\n",
    "total_out = torch.stack(out_list, dim=0)\n",
    "print(total_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2658, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = total_out.sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17027/3870397565.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:478.)\n",
      "  bit_mask.grad\n"
     ]
    }
   ],
   "source": [
    "bit_mask.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False, groups=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicSeparableConv2d(nn.Module):\n",
    "    KERNEL_TRANSFORM_MODE = 1  # None or 1\n",
    "\n",
    "    def __init__(self, max_in_channels, kernel_size_list, stride=1, dilation=1):\n",
    "        super(DynamicSeparableConv2d, self).__init__()\n",
    "\n",
    "        self.max_in_channels = max_in_channels\n",
    "        self.kernel_size_list = kernel_size_list\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            self.max_in_channels, self.max_in_channels, max(self.kernel_size_list), self.stride,\n",
    "            groups=self.max_in_channels, bias=False,\n",
    "        )\n",
    "\n",
    "        self._ks_set = list(set(self.kernel_size_list))\n",
    "        self._ks_set.sort()  # e.g., [3, 5, 7]\n",
    "        if self.KERNEL_TRANSFORM_MODE is not None:\n",
    "            # register scaling parameters\n",
    "            # 7to5_matrix, 5to3_matrix\n",
    "            scale_params = {}\n",
    "            for i in range(len(self._ks_set) - 1):\n",
    "                ks_small = self._ks_set[i]\n",
    "                ks_larger = self._ks_set[i + 1]\n",
    "                param_name = '%dto%d' % (ks_larger, ks_small)\n",
    "                scale_params['%s_matrix' % param_name] = Parameter(torch.eye(ks_small ** 2))\n",
    "            for name, param in scale_params.items():\n",
    "                self.register_parameter(name, param)\n",
    "\n",
    "        self.active_kernel_size = max(self.kernel_size_list)\n",
    "\n",
    "    def get_active_filter(self, in_channel, kernel_size):\n",
    "        out_channel = in_channel\n",
    "        max_kernel_size = max(self.kernel_size_list)\n",
    "\n",
    "        start, end = sub_filter_start_end(max_kernel_size, kernel_size)\n",
    "        filters = self.conv.weight[:out_channel, :in_channel, start:end, start:end]\n",
    "        if self.KERNEL_TRANSFORM_MODE is not None and kernel_size < max_kernel_size:\n",
    "            start_filter = self.conv.weight[:out_channel, :in_channel, :, :]  # start with max kernel\n",
    "            for i in range(len(self._ks_set) - 1, 0, -1):\n",
    "                src_ks = self._ks_set[i]\n",
    "                if src_ks <= kernel_size:\n",
    "                    break\n",
    "                target_ks = self._ks_set[i - 1]\n",
    "                start, end = sub_filter_start_end(src_ks, target_ks)\n",
    "                _input_filter = start_filter[:, :, start:end, start:end]\n",
    "                _input_filter = _input_filter.contiguous()\n",
    "                _input_filter = _input_filter.view(_input_filter.size(0), _input_filter.size(1), -1)\n",
    "                _input_filter = _input_filter.view(-1, _input_filter.size(2))\n",
    "                _input_filter = F.linear(\n",
    "                    _input_filter, self.__getattr__('%dto%d_matrix' % (src_ks, target_ks)),\n",
    "                )\n",
    "                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks ** 2)\n",
    "                _input_filter = _input_filter.view(filters.size(0), filters.size(1), target_ks, target_ks)\n",
    "                start_filter = _input_filter\n",
    "            filters = start_filter\n",
    "        return filters\n",
    "\n",
    "    def forward(self, x, gumbel_input=None):\n",
    "        \n",
    "        #if kernel_size is None:\n",
    "        #    kernel_size = self.active_kernel_size\n",
    "        in_channel = x.size(1)\n",
    "        \n",
    "        out_list = []\n",
    "        for i in range(len(self.kernel_size_list)):\n",
    "            kernel_size = self.kernel_size_list[i]\n",
    "            filters = self.get_active_filter(in_channel, kernel_size).contiguous()\n",
    "            padding = get_same_padding(kernel_size)\n",
    "            y = F.conv2d(\n",
    "                x, filters, None, self.stride, padding, self.dilation, in_channel\n",
    "            )\n",
    "            out = y * gumbel_input[:, i].unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "            print(gumbel_input[:, i])\n",
    "            print(out)\n",
    "            out_list.append(out)\n",
    "        y = torch.sum(torch.stack(out_list, dim=0), dim=0)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "tensor([[1., 0., 0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "in_c = 4\n",
    "k = 4\n",
    "module = DynamicSeparableConv2d(in_c, [3,5,7])\n",
    "inputs = torch.randn(1, in_c, k, k)\n",
    "gumbel_select = nn.Linear(k*k*in_c, 3) # self.kernel_list\n",
    "\n",
    "gumbel_input = gumbel_select(inputs.view(inputs.size(0), -1))\n",
    "print(gumbel_input.size())\n",
    "bit_mask = F.gumbel_softmax(gumbel_input, tau=1, hard=True, dim=-1)\n",
    "print(bit_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<SelectBackward0>)\n",
      "tensor([[[[ 0.1409,  0.0017, -0.1980, -0.2792],\n",
      "          [ 0.1436, -0.1170,  0.2453,  0.0059],\n",
      "          [-0.3516,  0.2223, -0.0338,  0.1091],\n",
      "          [-0.1531,  0.0832, -0.2318,  0.1527]],\n",
      "\n",
      "         [[ 0.1355, -0.0392,  0.0702,  0.1168],\n",
      "          [-0.0683,  0.1784,  0.3267, -0.1909],\n",
      "          [-0.1062,  0.1773, -0.0282, -0.0032],\n",
      "          [ 0.2182,  0.0144, -0.1456,  0.0809]],\n",
      "\n",
      "         [[-0.0575, -0.2015,  0.0263, -0.0784],\n",
      "          [ 0.1602,  0.1498,  0.0932, -0.0224],\n",
      "          [ 0.0466,  0.0282,  0.0581,  0.0196],\n",
      "          [-0.0937,  0.0789,  0.0055,  0.0410]],\n",
      "\n",
      "         [[ 0.0316, -0.2561, -0.4396, -0.2923],\n",
      "          [ 0.0712, -0.0032, -0.0657, -0.3710],\n",
      "          [-0.2168, -0.3756, -0.4276,  0.4448],\n",
      "          [ 0.0349, -0.1824,  0.0534,  0.0140]]]], grad_fn=<MulBackward0>)\n",
      "tensor([0.], grad_fn=<SelectBackward0>)\n",
      "tensor([[[[0., 0., -0., -0.],\n",
      "          [0., -0., 0., -0.],\n",
      "          [-0., 0., -0., 0.],\n",
      "          [-0., 0., -0., 0.]],\n",
      "\n",
      "         [[0., -0., 0., 0.],\n",
      "          [0., 0., 0., -0.],\n",
      "          [-0., 0., -0., -0.],\n",
      "          [0., -0., -0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [-0., -0., 0., -0.],\n",
      "          [-0., 0., 0., 0.],\n",
      "          [-0., -0., -0., -0.]],\n",
      "\n",
      "         [[0., -0., -0., 0.],\n",
      "          [-0., -0., -0., -0.],\n",
      "          [-0., -0., -0., 0.],\n",
      "          [0., -0., 0., -0.]]]], grad_fn=<MulBackward0>)\n",
      "tensor([0.], grad_fn=<SelectBackward0>)\n",
      "tensor([[[[0., 0., -0., -0.],\n",
      "          [0., -0., 0., 0.],\n",
      "          [-0., 0., -0., 0.],\n",
      "          [-0., 0., -0., 0.]],\n",
      "\n",
      "         [[-0., -0., 0., 0.],\n",
      "          [0., 0., 0., -0.],\n",
      "          [-0., 0., -0., -0.],\n",
      "          [0., -0., -0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., -0., 0., 0.],\n",
      "          [-0., 0., 0., 0.],\n",
      "          [-0., 0., -0., 0.]],\n",
      "\n",
      "         [[0., -0., -0., 0.],\n",
      "          [-0., -0., -0., -0.],\n",
      "          [-0., -0., -0., 0.],\n",
      "          [0., -0., 0., -0.]]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = module(inputs, bit_mask)\n",
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicSeparableConv2d(\n",
       "  (conv): Conv2d(4, 4, kernel_size=(7, 7), stride=(1, 1), groups=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8496e-03,  6.7819e-03,  2.9089e-03, -6.6991e-03,  3.7747e-03,\n",
       "         -7.8763e-03,  1.0054e-02,  3.2461e-03,  4.3712e-03, -8.5085e-03,\n",
       "          5.2003e-03, -1.8398e-03,  1.0541e-02, -2.6015e-03, -4.7269e-03,\n",
       "          2.3385e-05, -1.8948e-03,  4.7095e-03,  4.5827e-03,  5.8954e-03,\n",
       "          5.8511e-03, -7.8198e-04, -3.6422e-05,  6.3703e-03, -6.4089e-03,\n",
       "          5.8479e-03,  6.4663e-03, -1.1468e-02, -7.1738e-03,  7.1592e-03,\n",
       "          2.2607e-03, -3.4356e-03,  6.4294e-03,  7.5568e-03,  4.6905e-03,\n",
       "         -8.2747e-03,  7.3102e-03, -2.1480e-03,  4.4802e-03,  1.4163e-03,\n",
       "         -1.4185e-02, -7.3711e-03, -5.7679e-03, -3.7398e-03,  3.4935e-03,\n",
       "          3.5776e-03, -1.9053e-03,  1.2204e-03, -1.3589e-03, -1.0452e-02,\n",
       "         -1.1415e-03, -6.8882e-03,  2.2557e-03,  1.5920e-02,  3.4395e-03,\n",
       "          1.6593e-02, -7.3449e-03, -4.2015e-03,  1.8793e-02,  4.7848e-03,\n",
       "          5.5519e-03, -2.2328e-03,  9.7191e-04, -5.8595e-03],\n",
       "        [-9.9698e-03,  1.7564e-02,  7.5335e-03, -1.7350e-02,  9.7757e-03,\n",
       "         -2.0398e-02,  2.6039e-02,  8.4068e-03,  1.1321e-02, -2.2036e-02,\n",
       "          1.3468e-02, -4.7648e-03,  2.7299e-02, -6.7374e-03, -1.2242e-02,\n",
       "          6.0563e-05, -4.9073e-03,  1.2197e-02,  1.1869e-02,  1.5268e-02,\n",
       "          1.5153e-02, -2.0252e-03, -9.4326e-05,  1.6498e-02, -1.6598e-02,\n",
       "          1.5145e-02,  1.6747e-02, -2.9701e-02, -1.8579e-02,  1.8541e-02,\n",
       "          5.8548e-03, -8.8977e-03,  1.6651e-02,  1.9571e-02,  1.2148e-02,\n",
       "         -2.1430e-02,  1.8932e-02, -5.5631e-03,  1.1603e-02,  3.6679e-03,\n",
       "         -3.6737e-02, -1.9090e-02, -1.4938e-02, -9.6855e-03,  9.0477e-03,\n",
       "          9.2655e-03, -4.9345e-03,  3.1607e-03, -3.5193e-03, -2.7069e-02,\n",
       "         -2.9564e-03, -1.7839e-02,  5.8419e-03,  4.1231e-02,  8.9076e-03,\n",
       "          4.2972e-02, -1.9022e-02, -1.0881e-02,  4.8671e-02,  1.2392e-02,\n",
       "          1.4379e-02, -5.7825e-03,  2.5171e-03, -1.5175e-02],\n",
       "        [ 1.3819e-02, -2.4346e-02, -1.0442e-02,  2.4049e-02, -1.3550e-02,\n",
       "          2.8275e-02, -3.6093e-02, -1.1653e-02, -1.5692e-02,  3.0544e-02,\n",
       "         -1.8668e-02,  6.6046e-03, -3.7839e-02,  9.3390e-03,  1.6969e-02,\n",
       "         -8.3949e-05,  6.8021e-03, -1.6907e-02, -1.6451e-02, -2.1164e-02,\n",
       "         -2.1004e-02,  2.8072e-03,  1.3075e-04, -2.2868e-02,  2.3007e-02,\n",
       "         -2.0993e-02, -2.3213e-02,  4.1170e-02,  2.5753e-02, -2.5700e-02,\n",
       "         -8.1156e-03,  1.2333e-02, -2.3081e-02, -2.7128e-02, -1.6838e-02,\n",
       "          2.9705e-02, -2.6243e-02,  7.7112e-03, -1.6083e-02, -5.0842e-03,\n",
       "          5.0923e-02,  2.6461e-02,  2.0706e-02,  1.3425e-02, -1.2541e-02,\n",
       "         -1.2843e-02,  6.8398e-03, -4.3811e-03,  4.8782e-03,  3.7521e-02,\n",
       "          4.0979e-03,  2.4727e-02, -8.0976e-03, -5.7152e-02, -1.2347e-02,\n",
       "         -5.9565e-02,  2.6367e-02,  1.5083e-02, -6.7465e-02, -1.7177e-02,\n",
       "         -1.9931e-02,  8.0153e-03, -3.4890e-03,  2.1035e-02]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_select.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5to3_matrix\n",
      "tensor([[ 0.2090, -0.1782,  0.2345,  0.3628,  0.0185,  0.2426, -0.6996, -0.2479,\n",
      "         -0.5354],\n",
      "        [ 0.1860, -0.0771, -0.0367,  0.7314, -0.0662,  0.0943, -0.7062, -0.3619,\n",
      "         -0.4093],\n",
      "        [ 0.1132,  0.0156, -0.1465,  0.9133, -0.1156, -0.0082, -0.7651, -0.3789,\n",
      "         -0.4070],\n",
      "        [ 0.2113, -0.1941,  0.2523,  0.4045,  0.0205,  0.2236, -0.9038, -0.3480,\n",
      "         -0.6915],\n",
      "        [ 0.1801, -0.1052,  0.0112,  0.6236, -0.0426,  0.0857, -0.7950, -0.4085,\n",
      "         -0.5068],\n",
      "        [ 0.0237,  0.0977, -0.0388,  0.9219, -0.1299,  0.0492, -0.6579, -0.1234,\n",
      "         -0.3739],\n",
      "        [ 0.2191, -0.1045, -0.1405,  0.6949, -0.0622,  0.0299, -0.6748, -0.4879,\n",
      "         -0.3502],\n",
      "        [ 0.1782, -0.0413, -0.4880,  0.8495, -0.1154, -0.2612, -0.7898, -0.8179,\n",
      "         -0.3098],\n",
      "        [-0.0906,  0.2495, -0.5967,  1.1062, -0.2216, -0.4134, -0.6181, -0.4926,\n",
      "         -0.1532]])\n",
      "7to5_matrix\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-3.0886e-01,  7.1541e-02, -2.9492e-01, -5.5076e-02, -1.4939e-01,\n",
      "         -3.4865e-01,  2.0896e-01, -1.7816e-01,  2.3451e-01,  2.0040e-01,\n",
      "         -2.9145e-01,  3.6285e-01,  1.8492e-02,  2.4260e-01, -1.4726e-01,\n",
      "          1.0048e-01, -6.9961e-01, -2.4795e-01, -5.3540e-01, -1.1975e-01,\n",
      "         -4.0451e-01,  2.2578e-02, -4.5470e-01,  3.1325e-03,  3.1700e-01],\n",
      "        [-7.8708e-01,  5.9243e-02, -4.8729e-02,  3.0321e-02,  2.6011e-02,\n",
      "         -5.6976e-01,  1.8597e-01, -7.7072e-02, -3.6740e-02, -1.4349e-02,\n",
      "         -5.5353e-01,  7.3141e-01, -6.6198e-02,  9.4273e-02, -8.8951e-02,\n",
      "          2.4811e-01, -7.0619e-01, -3.6187e-01, -4.0934e-01, -4.6483e-01,\n",
      "         -6.7837e-01,  3.5791e-02, -5.3895e-01,  4.0563e-01,  5.8928e-01],\n",
      "        [-9.7139e-01,  6.0169e-03, -3.4031e-02, -4.2214e-02,  2.2151e-02,\n",
      "         -6.2595e-01,  1.1325e-01,  1.5603e-02, -1.4648e-01, -7.5346e-02,\n",
      "         -6.4511e-01,  9.1329e-01, -1.1557e-01, -8.2381e-03, -1.4272e-01,\n",
      "          3.3955e-01, -7.6507e-01, -3.7893e-01, -4.0698e-01, -5.1676e-01,\n",
      "         -8.7783e-01,  1.5527e-02, -6.0548e-01,  5.2971e-01,  6.6354e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-2.8045e-01,  1.0712e-01, -4.3791e-01, -1.0343e-01, -2.5496e-01,\n",
      "         -3.8260e-01,  2.1129e-01, -1.9415e-01,  2.5226e-01,  3.1735e-01,\n",
      "         -3.5947e-01,  4.0449e-01,  2.0498e-02,  2.2362e-01, -2.8236e-01,\n",
      "          1.5580e-01, -9.0382e-01, -3.4796e-01, -6.9153e-01, -9.6711e-02,\n",
      "         -4.8031e-01, -1.3060e-02, -5.8475e-01, -7.3285e-02,  3.3748e-01],\n",
      "        [-6.0067e-01,  1.0124e-01, -1.8714e-01, -2.2349e-03, -8.1509e-02,\n",
      "         -5.0925e-01,  1.8010e-01, -1.0522e-01,  1.1224e-02,  1.2000e-01,\n",
      "         -5.2160e-01,  6.2356e-01, -4.2598e-02,  8.5666e-02, -1.9852e-01,\n",
      "          2.4617e-01, -7.9497e-01, -4.0849e-01, -5.0681e-01, -3.6516e-01,\n",
      "         -6.0669e-01, -3.0127e-04, -5.7598e-01,  2.4148e-01,  5.0818e-01],\n",
      "        [-9.9233e-01, -1.7210e-01, -7.4644e-02, -2.3903e-01, -1.3934e-02,\n",
      "         -5.2040e-01,  2.3714e-02,  9.7663e-02, -3.8792e-02, -1.6193e-01,\n",
      "         -4.7702e-01,  9.2195e-01, -1.2993e-01,  4.9169e-02, -8.4303e-02,\n",
      "          2.8402e-01, -6.5791e-01, -1.2336e-01, -3.7391e-01, -2.9931e-01,\n",
      "         -9.8570e-01,  3.8869e-02, -5.1078e-01,  5.0700e-01,  5.5903e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-7.6077e-01,  1.5890e-01,  3.3749e-02,  1.6869e-01,  8.3308e-02,\n",
      "         -6.0253e-01,  2.1914e-01, -1.0451e-01, -1.4047e-01, -2.6168e-03,\n",
      "         -6.2885e-01,  6.9485e-01, -6.2204e-02,  2.9922e-02, -9.1590e-02,\n",
      "          2.7056e-01, -6.7483e-01, -4.8790e-01, -3.5020e-01, -6.0327e-01,\n",
      "         -5.6767e-01,  2.3547e-02, -5.3731e-01,  4.4242e-01,  6.2809e-01],\n",
      "        [-8.8647e-01,  3.1290e-01,  1.2505e-01,  3.1836e-01,  1.1934e-01,\n",
      "         -7.3377e-01,  1.7821e-01, -4.1342e-02, -4.8799e-01,  2.2451e-02,\n",
      "         -9.2088e-01,  8.4946e-01, -1.1543e-01, -2.6124e-01, -2.6019e-01,\n",
      "          4.6431e-01, -7.8980e-01, -8.1793e-01, -3.0979e-01, -9.1234e-01,\n",
      "         -6.2133e-01, -5.6745e-02, -6.8196e-01,  6.0648e-01,  7.8553e-01],\n",
      "        [-1.1795e+00, -4.9765e-03,  1.7236e-01, -2.2601e-02,  1.0935e-01,\n",
      "         -6.4173e-01, -9.0562e-02,  2.4947e-01, -5.9669e-01, -2.3429e-01,\n",
      "         -8.2343e-01,  1.1062e+00, -2.2160e-01, -4.1339e-01, -2.4330e-01,\n",
      "          5.3390e-01, -6.1808e-01, -4.9262e-01, -1.5323e-01, -7.2945e-01,\n",
      "         -1.0133e+00, -6.4545e-02, -5.9049e-01,  8.0393e-01,  7.3335e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "conv.weight\n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -1.6561, -0.9743, -0.4209,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -2.0699, -1.3911,  0.5201,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -1.3175, -1.5016,  0.9054,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -2.3618, -2.4645, -2.7804,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -2.6511, -2.3113, -3.5513,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -1.6983, -0.5990, -1.5949,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.1282,  1.2370,  1.1796,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.7936,  0.4144,  0.8070,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  1.6122,  1.7543,  1.3187,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -2.0494, -3.9158, -4.7464,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -2.6022, -3.7138, -3.8292,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -4.2706, -6.2695, -6.2099,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "for n, m in module.named_parameters():\n",
    "    print(n)\n",
    "    print(m.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pdh/torch/NAS/TinyML_NAS/test_dynamic_supernet.ipynb 셀 26\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/test_dynamic_supernet.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMyModule\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/test_dynamic_supernet.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/test_dynamic_supernet.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from mcunet.mcunet.tinynas.nn.modules import MBInvertedConvLayer, ConvLayer, LinearLayer\n",
    "from mcunet.mcunet.tinynas.elastic_nn.modules.dynamic_op import *\n",
    "from mcunet.mcunet.utils import adjust_bn_according_to_idx, copy_bn, make_divisible, SEModule, MyModule, val2list, \\\n",
    "    get_net_device, build_activation\n",
    "\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def module_str(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    def build_from_config(config):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "def val2list(val, repeat_time=1):\n",
    "    if isinstance(val, list) or isinstance(val, np.ndarray):\n",
    "        return val\n",
    "    elif isinstance(val, tuple):\n",
    "        return list(val)\n",
    "    else:\n",
    "        return [val for _ in range(repeat_time)]\n",
    "\n",
    "\n",
    "class DynamicMBConvLayer(MyModule):\n",
    "\n",
    "    def __init__(self, in_channel_list, out_channel_list,\n",
    "                 kernel_size_list=3, expand_ratio_list=6, stride=1, act_func='relu6', use_se=False):\n",
    "        super(DynamicMBConvLayer, self).__init__()\n",
    "\n",
    "        self.in_channel_list = in_channel_list\n",
    "        self.out_channel_list = out_channel_list\n",
    "\n",
    "        self.kernel_size_list = val2list(kernel_size_list, 1)\n",
    "        self.expand_ratio_list = val2list(expand_ratio_list, 1)\n",
    "\n",
    "        self.stride = stride\n",
    "        self.act_func = act_func\n",
    "        self.use_se = use_se\n",
    "\n",
    "        # build modules\n",
    "        max_middle_channel = round(max(self.in_channel_list) * max(self.expand_ratio_list))\n",
    "        if max(self.expand_ratio_list) == 1:\n",
    "            self.inverted_bottleneck = None\n",
    "        else:\n",
    "            self.inverted_bottleneck = nn.Sequential(OrderedDict([\n",
    "                ('conv', DynamicPointConv2d(max(self.in_channel_list), max_middle_channel)),\n",
    "                ('bn', DynamicBatchNorm2d(max_middle_channel)),\n",
    "                ('act', build_activation(self.act_func, inplace=True)),\n",
    "            ]))\n",
    "\n",
    "        self.depth_conv = nn.Sequential(OrderedDict([\n",
    "            ('conv', DynamicSeparableConv2d(max_middle_channel, self.kernel_size_list, self.stride)),\n",
    "            ('bn', DynamicBatchNorm2d(max_middle_channel)),\n",
    "            ('act', build_activation(self.act_func, inplace=True))\n",
    "        ]))\n",
    "        if self.use_se:\n",
    "            self.depth_conv.add_module('se', DynamicSE(max_middle_channel))\n",
    "\n",
    "        self.point_linear = nn.Sequential(OrderedDict([\n",
    "            ('conv', DynamicPointConv2d(max_middle_channel, max(self.out_channel_list))),\n",
    "            ('bn', DynamicBatchNorm2d(max(self.out_channel_list))),\n",
    "        ]))\n",
    "\n",
    "        self.active_kernel_size = max(self.kernel_size_list)\n",
    "        self.active_expand_ratio = max(self.expand_ratio_list)\n",
    "        self.active_out_channel = max(self.out_channel_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_channel = x.size(1)\n",
    "\n",
    "        if self.inverted_bottleneck is not None:\n",
    "            self.inverted_bottleneck.conv.active_out_channel = \\\n",
    "                make_divisible(round(in_channel * self.active_expand_ratio), 8)\n",
    "\n",
    "        self.depth_conv.conv.active_kernel_size = self.active_kernel_size\n",
    "        self.point_linear.conv.active_out_channel = self.active_out_channel\n",
    "\n",
    "        if self.inverted_bottleneck is not None:\n",
    "            x = self.inverted_bottleneck(x)\n",
    "        x = self.depth_conv(x)\n",
    "        x = self.point_linear(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def module_str(self):\n",
    "        if self.use_se:\n",
    "            return 'SE(O%d, E%.1f, K%d)' % (self.active_out_channel, self.active_expand_ratio, self.active_kernel_size)\n",
    "        else:\n",
    "            return '(O%d, E%.1f, K%d)' % (self.active_out_channel, self.active_expand_ratio, self.active_kernel_size)\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        return {\n",
    "            'name': DynamicMBConvLayer.__name__,\n",
    "            'in_channel_list': self.in_channel_list,\n",
    "            'out_channel_list': self.out_channel_list,\n",
    "            'kernel_size_list': self.kernel_size_list,\n",
    "            'expand_ratio_list': self.expand_ratio_list,\n",
    "            'stride': self.stride,\n",
    "            'act_func': self.act_func,\n",
    "            'use_se': self.use_se,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def build_from_config(config):\n",
    "        return DynamicMBConvLayer(**config)\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    def get_active_subnet(self, in_channel, preserve_weight=True):\n",
    "        middle_channel = make_divisible(round(in_channel * self.active_expand_ratio), 8)\n",
    "\n",
    "        # build the new layer\n",
    "        sub_layer = MBInvertedConvLayer(\n",
    "            in_channel, self.active_out_channel, self.active_kernel_size, self.stride, self.active_expand_ratio,\n",
    "            act_func=self.act_func, mid_channels=middle_channel, use_se=self.use_se,\n",
    "        )\n",
    "        sub_layer = sub_layer.to(get_net_device(self))\n",
    "\n",
    "        if not preserve_weight:\n",
    "            return sub_layer\n",
    "\n",
    "        # copy weight from current layer\n",
    "        if sub_layer.inverted_bottleneck is not None:\n",
    "            sub_layer.inverted_bottleneck.conv.weight.data.copy_(\n",
    "                self.inverted_bottleneck.conv.conv.weight.data[:middle_channel, :in_channel, :, :]\n",
    "            )\n",
    "            copy_bn(sub_layer.inverted_bottleneck.bn, self.inverted_bottleneck.bn.bn)\n",
    "\n",
    "        sub_layer.depth_conv.conv.weight.data.copy_(\n",
    "            self.depth_conv.conv.get_active_filter(middle_channel, self.active_kernel_size).data\n",
    "        )\n",
    "        copy_bn(sub_layer.depth_conv.bn, self.depth_conv.bn.bn)\n",
    "\n",
    "        if self.use_se:\n",
    "            se_mid = make_divisible(middle_channel // SEModule.REDUCTION, divisor=8)\n",
    "            sub_layer.depth_conv.se.fc.reduce.weight.data.copy_(\n",
    "                self.depth_conv.se.fc.reduce.weight.data[:se_mid, :middle_channel, :, :]\n",
    "            )\n",
    "            sub_layer.depth_conv.se.fc.reduce.bias.data.copy_(self.depth_conv.se.fc.reduce.bias.data[:se_mid])\n",
    "\n",
    "            sub_layer.depth_conv.se.fc.expand.weight.data.copy_(\n",
    "                self.depth_conv.se.fc.expand.weight.data[:middle_channel, :se_mid, :, :]\n",
    "            )\n",
    "            sub_layer.depth_conv.se.fc.expand.bias.data.copy_(self.depth_conv.se.fc.expand.bias.data[:middle_channel])\n",
    "\n",
    "        sub_layer.point_linear.conv.weight.data.copy_(\n",
    "            self.point_linear.conv.conv.weight.data[:self.active_out_channel, :middle_channel, :, :]\n",
    "        )\n",
    "        copy_bn(sub_layer.point_linear.bn, self.point_linear.bn.bn)\n",
    "\n",
    "        return sub_layer\n",
    "\n",
    "    def re_organize_middle_weights(self, expand_ratio_stage=0):\n",
    "        importance = torch.sum(torch.abs(self.point_linear.conv.conv.weight.data), dim=(0, 2, 3))  # over input ch\n",
    "        if expand_ratio_stage > 0:\n",
    "            sorted_expand_list = copy.deepcopy(self.expand_ratio_list)\n",
    "            sorted_expand_list.sort(reverse=True)\n",
    "            target_width = sorted_expand_list[expand_ratio_stage]\n",
    "            target_width = round(max(self.in_channel_list) * target_width)\n",
    "            importance[target_width:] = torch.arange(0, target_width - importance.size(0), -1)\n",
    "\n",
    "        sorted_importance, sorted_idx = torch.sort(importance, dim=0, descending=True)\n",
    "        self.point_linear.conv.conv.weight.data = torch.index_select(\n",
    "            self.point_linear.conv.conv.weight.data, 1, sorted_idx\n",
    "        )\n",
    "\n",
    "        adjust_bn_according_to_idx(self.depth_conv.bn.bn, sorted_idx)\n",
    "        self.depth_conv.conv.conv.weight.data = torch.index_select(\n",
    "            self.depth_conv.conv.conv.weight.data, 0, sorted_idx\n",
    "        )\n",
    "\n",
    "        if self.use_se:\n",
    "            # se expand: output dim 0 reorganize\n",
    "            se_expand = self.depth_conv.se.fc.expand\n",
    "            se_expand.weight.data = torch.index_select(se_expand.weight.data, 0, sorted_idx)\n",
    "            se_expand.bias.data = torch.index_select(se_expand.bias.data, 0, sorted_idx)\n",
    "            # se reduce: input dim 1 reorganize\n",
    "            se_reduce = self.depth_conv.se.fc.reduce\n",
    "            se_reduce.weight.data = torch.index_select(se_reduce.weight.data, 1, sorted_idx)\n",
    "            # middle weight reorganize\n",
    "            se_importance = torch.sum(torch.abs(se_expand.weight.data), dim=(0, 2, 3))\n",
    "            se_importance, se_idx = torch.sort(se_importance, dim=0, descending=True)\n",
    "\n",
    "            se_expand.weight.data = torch.index_select(se_expand.weight.data, 1, se_idx)\n",
    "            se_reduce.weight.data = torch.index_select(se_reduce.weight.data, 0, se_idx)\n",
    "            se_reduce.bias.data = torch.index_select(se_reduce.bias.data, 0, se_idx)\n",
    "\n",
    "        # TODO if inverted_bottleneck is None, the previous layer should be reorganized accordingly\n",
    "        if self.inverted_bottleneck is not None:\n",
    "            adjust_bn_according_to_idx(self.inverted_bottleneck.bn.bn, sorted_idx)\n",
    "            self.inverted_bottleneck.conv.conv.weight.data = torch.index_select(\n",
    "                self.inverted_bottleneck.conv.conv.weight.data, 0, sorted_idx\n",
    "            )\n",
    "            return None\n",
    "        else:\n",
    "            return sorted_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

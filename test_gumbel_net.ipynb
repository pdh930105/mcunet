{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "sys.path.append('./mcunet')\n",
    "\n",
    "from mcunet.gumbel_module.gumbel_net import GumbelMCUNets\n",
    "\n",
    "from mcunet.tinynas.nn.modules import MBInvertedConvLayer\n",
    "from mcunet.tinynas.nn.networks import MobileInvertedResidualBlock\n",
    "from mcunet.model_zoo import build_model\n",
    "\n",
    "from mcunet.utils import MyModule, MyNetwork, SEModule, build_activation, get_same_padding, sub_filter_start_end\n",
    "from mcunet.tinynas.nn.modules import ZeroLayer, set_layer_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model, img_size, desc = build_model(net_id='mcunet-in4', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'name': 'MobileInvertedResidualBlock', 'mobile_inverted_conv': {'name': 'MBInvertedConvLayer', 'in_channels': 32, 'out_channels': 16, 'kernel_size': 3, 'stride': 1, 'expand_ratio': 1, 'mid_channels': None, 'act_func': 'relu6', 'use_se': False}, 'shortcut': None}\n",
      "1 {'name': 'MobileInvertedResidualBlock', 'mobile_inverted_conv': {'name': 'MBInvertedConvLayer', 'in_channels': 16, 'out_channels': 24, 'kernel_size': 7, 'stride': 2, 'expand_ratio': 3, 'mid_channels': 48, 'act_func': 'relu6', 'use_se': False}, 'shortcut': None}\n",
      "load first_conv.conv.weight params (torch.Size([32, 3, 3, 3]))\n",
      "load first_conv.bn.weight params (torch.Size([32]))\n",
      "load first_conv.bn.bias params (torch.Size([32]))\n",
      "load blocks.0.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([32, 1, 3, 3]))\n",
      "load blocks.0.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([32]))\n",
      "load blocks.0.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([32]))\n",
      "load blocks.0.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([16, 32, 1, 1]))\n",
      "load blocks.0.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([16]))\n",
      "load blocks.0.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([16]))\n",
      "load blocks.1.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([48, 16, 1, 1]))\n",
      "load blocks.1.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([48]))\n",
      "load blocks.1.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([48]))\n",
      "load blocks.1.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([48, 1, 7, 7]))\n",
      "load blocks.1.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([48]))\n",
      "load blocks.1.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([48]))\n",
      "load blocks.1.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([24, 48, 1, 1]))\n",
      "load blocks.1.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([24]))\n",
      "load blocks.1.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([24]))\n",
      "load blocks.2.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([120, 24, 1, 1]))\n",
      "load blocks.2.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([120]))\n",
      "load blocks.2.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([120]))\n",
      "load blocks.2.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([120, 1, 3, 3]))\n",
      "load blocks.2.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([120]))\n",
      "load blocks.2.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([120]))\n",
      "load blocks.2.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([24, 120, 1, 1]))\n",
      "load blocks.2.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([24]))\n",
      "load blocks.2.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([24]))\n",
      "load blocks.3.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([96, 24, 1, 1]))\n",
      "load blocks.3.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([96]))\n",
      "load blocks.3.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([96]))\n",
      "load blocks.3.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([96, 1, 5, 5]))\n",
      "load blocks.3.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([96]))\n",
      "load blocks.3.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([96]))\n",
      "load blocks.3.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([24, 96, 1, 1]))\n",
      "load blocks.3.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([24]))\n",
      "load blocks.3.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([24]))\n",
      "load blocks.4.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([120, 24, 1, 1]))\n",
      "load blocks.4.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([120]))\n",
      "load blocks.4.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([120]))\n",
      "load blocks.4.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([120, 1, 7, 7]))\n",
      "load blocks.4.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([120]))\n",
      "load blocks.4.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([120]))\n",
      "load blocks.4.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([40, 120, 1, 1]))\n",
      "load blocks.4.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([40]))\n",
      "load blocks.4.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([40]))\n",
      "load blocks.5.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([160, 40, 1, 1]))\n",
      "load blocks.5.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([160]))\n",
      "load blocks.5.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([160]))\n",
      "load blocks.5.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([160, 1, 3, 3]))\n",
      "load blocks.5.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([160]))\n",
      "load blocks.5.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([160]))\n",
      "load blocks.5.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([40, 160, 1, 1]))\n",
      "load blocks.5.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([40]))\n",
      "load blocks.5.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([40]))\n",
      "load blocks.6.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([160, 40, 1, 1]))\n",
      "load blocks.6.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([160]))\n",
      "load blocks.6.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([160]))\n",
      "load blocks.6.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([160, 1, 7, 7]))\n",
      "load blocks.6.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([160]))\n",
      "load blocks.6.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([160]))\n",
      "load blocks.6.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([40, 160, 1, 1]))\n",
      "load blocks.6.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([40]))\n",
      "load blocks.6.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([40]))\n",
      "load blocks.7.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([120, 40, 1, 1]))\n",
      "load blocks.7.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([120]))\n",
      "load blocks.7.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([120]))\n",
      "load blocks.7.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([120, 1, 7, 7]))\n",
      "load blocks.7.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([120]))\n",
      "load blocks.7.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([120]))\n",
      "load blocks.7.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([80, 120, 1, 1]))\n",
      "load blocks.7.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([80]))\n",
      "load blocks.7.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([80]))\n",
      "load blocks.8.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([240, 80, 1, 1]))\n",
      "load blocks.8.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([240]))\n",
      "load blocks.8.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([240]))\n",
      "load blocks.8.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([240, 1, 3, 3]))\n",
      "load blocks.8.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([240]))\n",
      "load blocks.8.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([240]))\n",
      "load blocks.8.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([80, 240, 1, 1]))\n",
      "load blocks.8.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([80]))\n",
      "load blocks.8.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([80]))\n",
      "load blocks.9.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([240, 80, 1, 1]))\n",
      "load blocks.9.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([240]))\n",
      "load blocks.9.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([240]))\n",
      "load blocks.9.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([240, 1, 7, 7]))\n",
      "load blocks.9.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([240]))\n",
      "load blocks.9.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([240]))\n",
      "load blocks.9.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([80, 240, 1, 1]))\n",
      "load blocks.9.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([80]))\n",
      "load blocks.9.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([80]))\n",
      "load blocks.10.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([320, 80, 1, 1]))\n",
      "load blocks.10.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([320]))\n",
      "load blocks.10.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([320]))\n",
      "load blocks.10.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([320, 1, 3, 3]))\n",
      "load blocks.10.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([320]))\n",
      "load blocks.10.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([320]))\n",
      "load blocks.10.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([96, 320, 1, 1]))\n",
      "load blocks.10.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([96]))\n",
      "load blocks.10.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([96]))\n",
      "load blocks.11.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([288, 96, 1, 1]))\n",
      "load blocks.11.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([288]))\n",
      "load blocks.11.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([288]))\n",
      "load blocks.11.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([288, 1, 5, 5]))\n",
      "load blocks.11.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([288]))\n",
      "load blocks.11.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([288]))\n",
      "load blocks.11.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([96, 288, 1, 1]))\n",
      "load blocks.11.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([96]))\n",
      "load blocks.11.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([96]))\n",
      "load blocks.12.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([288, 96, 1, 1]))\n",
      "load blocks.12.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([288]))\n",
      "load blocks.12.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([288]))\n",
      "load blocks.12.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([288, 1, 5, 5]))\n",
      "load blocks.12.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([288]))\n",
      "load blocks.12.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([288]))\n",
      "load blocks.12.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([96, 288, 1, 1]))\n",
      "load blocks.12.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([96]))\n",
      "load blocks.12.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([96]))\n",
      "load blocks.13.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([384, 96, 1, 1]))\n",
      "load blocks.13.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([384]))\n",
      "load blocks.13.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([384]))\n",
      "load blocks.13.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([384, 1, 7, 7]))\n",
      "load blocks.13.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([384]))\n",
      "load blocks.13.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([384]))\n",
      "load blocks.13.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([192, 384, 1, 1]))\n",
      "load blocks.13.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([192]))\n",
      "load blocks.13.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([192]))\n",
      "load blocks.14.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([576, 192, 1, 1]))\n",
      "load blocks.14.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([576]))\n",
      "load blocks.14.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([576]))\n",
      "load blocks.14.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([576, 1, 7, 7]))\n",
      "load blocks.14.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([576]))\n",
      "load blocks.14.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([576]))\n",
      "load blocks.14.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([192, 576, 1, 1]))\n",
      "load blocks.14.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([192]))\n",
      "load blocks.14.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([192]))\n",
      "load blocks.15.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([576, 192, 1, 1]))\n",
      "load blocks.15.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([576]))\n",
      "load blocks.15.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([576]))\n",
      "load blocks.15.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([576, 1, 5, 5]))\n",
      "load blocks.15.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([576]))\n",
      "load blocks.15.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([576]))\n",
      "load blocks.15.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([192, 576, 1, 1]))\n",
      "load blocks.15.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([192]))\n",
      "load blocks.15.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([192]))\n",
      "load blocks.16.mobile_inverted_conv.inverted_bottleneck.conv.weight params (torch.Size([768, 192, 1, 1]))\n",
      "load blocks.16.mobile_inverted_conv.inverted_bottleneck.bn.weight params (torch.Size([768]))\n",
      "load blocks.16.mobile_inverted_conv.inverted_bottleneck.bn.bias params (torch.Size([768]))\n",
      "load blocks.16.mobile_inverted_conv.depth_conv.conv.weight params (torch.Size([768, 1, 5, 5]))\n",
      "load blocks.16.mobile_inverted_conv.depth_conv.bn.weight params (torch.Size([768]))\n",
      "load blocks.16.mobile_inverted_conv.depth_conv.bn.bias params (torch.Size([768]))\n",
      "load blocks.16.mobile_inverted_conv.point_linear.conv.weight params (torch.Size([320, 768, 1, 1]))\n",
      "load blocks.16.mobile_inverted_conv.point_linear.bn.weight params (torch.Size([320]))\n",
      "load blocks.16.mobile_inverted_conv.point_linear.bn.bias params (torch.Size([320]))\n",
      "load classifier.linear.weight params (torch.Size([1000, 320]))\n",
      "load classifier.linear.bias params (torch.Size([1000]))\n"
     ]
    }
   ],
   "source": [
    "gubmel_config = {'global_expand_ratio_list':[1,3,5,6], 'global_kernel_size_list':[3,5,7], 'gumbel_feature_extract_block':2}\n",
    "gumbel_model = GumbelMCUNets.build_from_config(ori_model.config, gubmel_config)\n",
    "gumbel_model.load_pretrained_mcunet_param(ori_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_gumbel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.test_layer = nn.Linear(20, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gumbel_input = self.test_layer(x)\n",
    "        if self.training:\n",
    "            gumbel_out = F.gumbel_softmax(gumbel_input, tau=1, hard=True, eps=1e-10, dim=-1)\n",
    "        else:\n",
    "            index = gumbel_input.max(dim=-1, keepdim=True)[1]\n",
    "            gumbel_out = torch.zeros_like(gumbel_input, memory_format=torch.legacy_contiguous_format).scatter_(-1, index, 1.0)\n",
    "        \n",
    "        return gumbel_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], grad_fn=<AddBackward0>)\n",
      "1 iter ->  tensor([[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]], grad_fn=<AddBackward0>)\n",
      "2 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]], grad_fn=<AddBackward0>)\n",
      "3 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]], grad_fn=<AddBackward0>)\n",
      "4 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.]], grad_fn=<AddBackward0>)\n",
      "0 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]])\n",
      "1 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]])\n",
      "2 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]])\n",
      "3 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]])\n",
      "4 iter ->  tensor([[0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(5, 20)\n",
    "gumbel = test_gumbel()\n",
    "\n",
    "# train\n",
    "gumbel.train()\n",
    "for i in range(5):\n",
    "    out = gumbel(inputs)\n",
    "    print(f\"{i} iter -> \", out)\n",
    "\n",
    "# test\n",
    "gumbel.eval()\n",
    "for i in range(5):\n",
    "    out = gumbel(inputs)\n",
    "    print(f\"{i} iter -> \", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(16, 3, 160, 160)\n",
    "\n",
    "out = gumbel_model.forward_original(inputs)\n",
    "out2 = ori_model.forward(inputs)\n",
    "print(out - out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gumbel_model.forward(inputs)\n",
    "\n",
    "gumbel_model.eval()\n",
    "print(gumbel_model.training)\n",
    "out_origin = gumbel_model.forward(inputs)\n",
    "for i in range(10):\n",
    "    print(out_origin - gumbel_model.forward(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291112064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pdh/.local/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::max\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/pdh/.local/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::zeros_like\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/pdh/.local/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scatter_\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/pdh/.local/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/home/pdh/.local/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::pad\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "from torchprofile import profile_macs\n",
    "\n",
    "total_mac = profile_macs(gumbel_model.cuda(), torch.randn(2, 3,160, 160).cuda())\n",
    "print(total_mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchprofile.utils.flatten import Flatten\n",
    "import warnings\n",
    "with warnings.catch_warnings(record=True):\n",
    "    graph, _ = torch.jit._get_trace_graph(Flatten(gumbel_model.cuda()), torch.randn(2,3,160,160).cuda(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input.1 Float [2, 3, 160, 160]\n",
      "1 Float [32, 3, 3, 3]\n",
      "input.3 Float [2, 32, 80, 80]\n",
      "2 Float [32]\n",
      "3 Float [32]\n",
      "4 Float [32]\n",
      "5 Float [32]\n",
      "input.5 Float [2, 32, 80, 80]\n",
      "input.7 Float [2, 32, 80, 80]\n",
      "7 Float [32, 1, 3, 3]\n",
      "input.9 Float [2, 32, 80, 80]\n",
      "8 Float [32]\n",
      "9 Float [32]\n",
      "10 Float [32]\n",
      "11 Float [32]\n",
      "input.11 Float [2, 32, 80, 80]\n",
      "input.13 Float [2, 32, 80, 80]\n",
      "13 Float [16, 32, 1, 1]\n",
      "input.15 Float [2, 16, 80, 80]\n",
      "14 Float [16]\n",
      "15 Float [16]\n",
      "16 Float [16]\n",
      "17 Float [16]\n",
      "input.17 Float [2, 16, 80, 80]\n",
      "19 Float [48, 16, 1, 1]\n",
      "input.19 Float [2, 48, 80, 80]\n",
      "20 Float [48]\n",
      "21 Float [48]\n",
      "22 Float [48]\n",
      "23 Float [48]\n",
      "input.21 Float [2, 48, 80, 80]\n",
      "input.23 Float [2, 48, 80, 80]\n",
      "25 Float [48, 1, 7, 7]\n",
      "input.25 Float [2, 48, 40, 40]\n",
      "26 Float [48]\n",
      "27 Float [48]\n",
      "28 Float [48]\n",
      "29 Float [48]\n",
      "input.27 Float [2, 48, 40, 40]\n",
      "input.29 Float [2, 48, 40, 40]\n",
      "31 Float [24, 48, 1, 1]\n",
      "input.31 Float [2, 24, 40, 40]\n",
      "32 Float [24]\n",
      "33 Float [24]\n",
      "34 Float [24]\n",
      "35 Float [24]\n",
      "input.33 Float [2, 24, 40, 40]\n",
      "1218 Float [2, 24, 8, 8]\n",
      "1221 Float [2, 1536]\n",
      "343 Float [256, 1536]\n",
      "344 Float [256]\n",
      "input.35 Float [2, 256]\n",
      "1225 Float [2, 256]\n",
      "345 Float [48, 256]\n",
      "346 Float [48]\n",
      "1226 Float [2, 48]\n",
      "1230 Float [2, 48]\n",
      "1235 Float [2, 48]\n",
      "1240 Float [2, 3]\n",
      "1240 Float [2, 3]\n",
      "1250 Float [2, 3]\n",
      "1244 Long [2, 1]\n",
      "input.33 Float [2, 24, 40, 40]\n",
      "37 Float [120, 24, 1, 1]\n",
      "input.37 Float [2, 120, 40, 40]\n",
      "38 Float [120]\n",
      "39 Float [120]\n",
      "40 Float [120]\n",
      "41 Float [120]\n",
      "input.39 Float [2, 120, 40, 40]\n",
      "1253 Float [2, 3]\n",
      "1292 Float [2, 3]\n",
      "1295 Float [2]\n",
      "1297 Float [2, 1]\n",
      "1299 Float [2, 1, 1]\n",
      "1287 Float [2, 120, 40, 40]\n",
      "1301 Float [2, 1, 1, 1]\n",
      "37 Float [120, 24, 1, 1]\n",
      "1307 Float [24, 24, 1, 1]\n",
      "1312 Float [24, 24, 1, 1]\n",
      "1317 Float [24, 24, 1, 1]\n",
      "input.33 Float [2, 24, 40, 40]\n",
      "1322 Float [24, 24, 1, 1]\n",
      "40 Float [120]\n",
      "41 Float [120]\n",
      "38 Float [120]\n",
      "39 Float [120]\n",
      "out.1 Float [2, 24, 40, 40]\n",
      "1357 Float [24]\n",
      "1362 Float [24]\n",
      "1347 Float [24]\n",
      "1352 Float [24]\n",
      "input.41 Float [2, 24, 40, 40]\n",
      "1253 Float [2, 3]\n",
      "1375 Float [2, 3]\n",
      "1378 Float [2]\n",
      "1380 Float [2, 1]\n",
      "1382 Float [2, 1, 1]\n",
      "1370 Float [2, 24, 40, 40]\n",
      "1384 Float [2, 1, 1, 1]\n",
      "1302 Float [2, 120, 40, 40]\n",
      "1385 Float [2, 24, 40, 40]\n",
      "1388 Long []\n",
      "1391 Long []\n",
      "1393 Long []\n",
      "1385 Float [2, 24, 40, 40]\n",
      "1302 Float [2, 120, 40, 40]\n",
      "1403 Float [2, 120, 40, 40]\n",
      "37 Float [120, 24, 1, 1]\n",
      "1410 Float [72, 24, 1, 1]\n",
      "1415 Float [72, 24, 1, 1]\n",
      "1420 Float [72, 24, 1, 1]\n",
      "input.33 Float [2, 24, 40, 40]\n",
      "1425 Float [72, 24, 1, 1]\n",
      "40 Float [120]\n",
      "41 Float [120]\n",
      "38 Float [120]\n",
      "39 Float [120]\n",
      "out.3 Float [2, 72, 40, 40]\n",
      "1460 Float [72]\n",
      "1465 Float [72]\n",
      "1450 Float [72]\n",
      "1455 Float [72]\n",
      "input.43 Float [2, 72, 40, 40]\n",
      "1253 Float [2, 3]\n",
      "1478 Float [2, 3]\n",
      "1481 Float [2]\n",
      "1483 Float [2, 1]\n",
      "1485 Float [2, 1, 1]\n",
      "1473 Float [2, 72, 40, 40]\n",
      "1487 Float [2, 1, 1, 1]\n",
      "1405 Float [2, 120, 40, 40]\n",
      "1488 Float [2, 72, 40, 40]\n",
      "1491 Long []\n",
      "1494 Long []\n",
      "1496 Long []\n",
      "1488 Float [2, 72, 40, 40]\n",
      "1405 Float [2, 120, 40, 40]\n",
      "1506 Float [2, 120, 40, 40]\n",
      "input.45 Float [2, 120, 40, 40]\n",
      "43 Float [120, 1, 3, 3]\n",
      "input.47 Float [2, 120, 40, 40]\n",
      "44 Float [120]\n",
      "45 Float [120]\n",
      "46 Float [120]\n",
      "47 Float [120]\n",
      "input.49 Float [2, 120, 40, 40]\n",
      "input.51 Float [2, 120, 40, 40]\n",
      "49 Float [24, 120, 1, 1]\n",
      "input.53 Float [2, 24, 40, 40]\n",
      "50 Float [24]\n",
      "51 Float [24]\n",
      "52 Float [24]\n",
      "53 Float [24]\n",
      "1561 Float [2, 24, 40, 40]\n",
      "input.33 Float [2, 24, 40, 40]\n",
      "1230 Float [2, 48]\n",
      "1568 Float [2, 48]\n",
      "1573 Float [2, 2]\n",
      "1573 Float [2, 2]\n",
      "1583 Float [2, 2]\n",
      "1577 Long [2, 1]\n",
      "input.55 Float [2, 24, 40, 40]\n",
      "55 Float [96, 24, 1, 1]\n",
      "input.57 Float [2, 96, 40, 40]\n",
      "56 Float [96]\n",
      "57 Float [96]\n",
      "58 Float [96]\n",
      "59 Float [96]\n",
      "input.59 Float [2, 96, 40, 40]\n",
      "1620 Float [2, 96, 40, 40]\n",
      "61 Float [96, 1, 5, 5]\n",
      "input.61 Float [2, 96, 40, 40]\n",
      "62 Float [96]\n",
      "63 Float [96]\n",
      "64 Float [96]\n",
      "65 Float [96]\n",
      "input.63 Float [2, 96, 40, 40]\n",
      "1586 Float [2, 2]\n",
      "1656 Float [2, 2]\n",
      "1659 Float [2]\n",
      "1661 Float [2, 1]\n",
      "1663 Float [2, 1, 1]\n",
      "1651 Float [2, 96, 40, 40]\n",
      "1665 Float [2, 1, 1, 1]\n",
      "61 Float [96, 1, 5, 5]\n",
      "1671 Float [96, 1, 5, 5]\n",
      "1676 Float [96, 1, 5, 5]\n",
      "1681 Float [96, 1, 3, 5]\n",
      "1686 Float [96, 1, 3, 3]\n",
      "1688 Float [96, 1, 3, 3]\n",
      "1691 Long []\n",
      "1688 Float [96, 1, 3, 3]\n",
      "1694 Long []\n",
      "1688 Float [96, 1, 3, 3]\n",
      "1699 Float [96, 1, 9]\n",
      "73 Float [9, 9]\n",
      "74 Float [9]\n",
      "1700 Float [96, 1, 9]\n",
      "1703 Long []\n",
      "1700 Float [96, 1, 9]\n",
      "1706 Long []\n",
      "1700 Float [96, 1, 9]\n",
      "1620 Float [2, 96, 40, 40]\n",
      "1712 Float [96, 1, 3, 3]\n",
      "input.65 Float [2, 96, 40, 40]\n",
      "62 Float [96]\n",
      "63 Float [96]\n",
      "64 Float [96]\n",
      "65 Float [96]\n",
      "input.67 Float [2, 96, 40, 40]\n",
      "1586 Float [2, 2]\n",
      "1748 Float [2, 2]\n",
      "1751 Float [2]\n",
      "1753 Float [2, 1]\n",
      "1755 Float [2, 1, 1]\n",
      "1743 Float [2, 96, 40, 40]\n",
      "1757 Float [2, 1, 1, 1]\n",
      "1666 Float [2, 96, 40, 40]\n",
      "1758 Float [2, 96, 40, 40]\n",
      "input.69 Float [2, 96, 40, 40]\n",
      "67 Float [24, 96, 1, 1]\n",
      "input.71 Float [2, 24, 40, 40]\n",
      "68 Float [24]\n",
      "69 Float [24]\n",
      "70 Float [24]\n",
      "71 Float [24]\n",
      "1785 Float [2, 24, 40, 40]\n",
      "input.55 Float [2, 24, 40, 40]\n",
      "input.73 Float [2, 24, 40, 40]\n",
      "75 Float [120, 24, 1, 1]\n",
      "input.75 Float [2, 120, 40, 40]\n",
      "76 Float [120]\n",
      "77 Float [120]\n",
      "78 Float [120]\n",
      "79 Float [120]\n",
      "input.77 Float [2, 120, 40, 40]\n",
      "input.79 Float [2, 120, 40, 40]\n",
      "81 Float [120, 1, 7, 7]\n",
      "input.81 Float [2, 120, 20, 20]\n",
      "82 Float [120]\n",
      "83 Float [120]\n",
      "84 Float [120]\n",
      "85 Float [120]\n",
      "input.83 Float [2, 120, 20, 20]\n",
      "input.85 Float [2, 120, 20, 20]\n",
      "87 Float [40, 120, 1, 1]\n",
      "input.87 Float [2, 40, 20, 20]\n",
      "88 Float [40]\n",
      "89 Float [40]\n",
      "90 Float [40]\n",
      "91 Float [40]\n",
      "input.89 Float [2, 40, 20, 20]\n",
      "97 Float [160, 40, 1, 1]\n",
      "input.91 Float [2, 160, 20, 20]\n",
      "98 Float [160]\n",
      "99 Float [160]\n",
      "100 Float [160]\n",
      "101 Float [160]\n",
      "input.93 Float [2, 160, 20, 20]\n",
      "input.95 Float [2, 160, 20, 20]\n",
      "103 Float [160, 1, 3, 3]\n",
      "input.97 Float [2, 160, 20, 20]\n",
      "104 Float [160]\n",
      "105 Float [160]\n",
      "106 Float [160]\n",
      "107 Float [160]\n",
      "input.99 Float [2, 160, 20, 20]\n",
      "input.101 Float [2, 160, 20, 20]\n",
      "109 Float [40, 160, 1, 1]\n",
      "input.103 Float [2, 40, 20, 20]\n",
      "110 Float [40]\n",
      "111 Float [40]\n",
      "112 Float [40]\n",
      "113 Float [40]\n",
      "1972 Float [2, 40, 20, 20]\n",
      "input.89 Float [2, 40, 20, 20]\n",
      "1230 Float [2, 48]\n",
      "1979 Float [2, 48]\n",
      "1984 Float [2, 3]\n",
      "1984 Float [2, 3]\n",
      "1994 Float [2, 3]\n",
      "1988 Long [2, 1]\n",
      "input.105 Float [2, 40, 20, 20]\n",
      "115 Float [160, 40, 1, 1]\n",
      "input.107 Float [2, 160, 20, 20]\n",
      "116 Float [160]\n",
      "117 Float [160]\n",
      "118 Float [160]\n",
      "119 Float [160]\n",
      "input.109 Float [2, 160, 20, 20]\n",
      "2031 Float [2, 160, 20, 20]\n",
      "121 Float [160, 1, 7, 7]\n",
      "input.111 Float [2, 160, 20, 20]\n",
      "122 Float [160]\n",
      "123 Float [160]\n",
      "124 Float [160]\n",
      "125 Float [160]\n",
      "input.113 Float [2, 160, 20, 20]\n",
      "1997 Float [2, 3]\n",
      "2067 Float [2, 3]\n",
      "2070 Float [2]\n",
      "2072 Float [2, 1]\n",
      "2074 Float [2, 1, 1]\n",
      "2062 Float [2, 160, 20, 20]\n",
      "2076 Float [2, 1, 1, 1]\n",
      "121 Float [160, 1, 7, 7]\n",
      "2082 Float [160, 1, 7, 7]\n",
      "2087 Float [160, 1, 7, 7]\n",
      "2092 Float [160, 1, 5, 7]\n",
      "2097 Float [160, 1, 5, 5]\n",
      "2099 Float [160, 1, 5, 5]\n",
      "2102 Long []\n",
      "2099 Float [160, 1, 5, 5]\n",
      "2105 Long []\n",
      "2099 Float [160, 1, 5, 5]\n",
      "2110 Float [160, 1, 25]\n",
      "133 Float [25, 25]\n",
      "134 Float [25]\n",
      "2111 Float [160, 1, 25]\n",
      "2114 Long []\n",
      "2111 Float [160, 1, 25]\n",
      "2117 Long []\n",
      "2111 Float [160, 1, 25]\n",
      "2031 Float [2, 160, 20, 20]\n",
      "2123 Float [160, 1, 5, 5]\n",
      "input.115 Float [2, 160, 20, 20]\n",
      "122 Float [160]\n",
      "123 Float [160]\n",
      "124 Float [160]\n",
      "125 Float [160]\n",
      "input.117 Float [2, 160, 20, 20]\n",
      "1997 Float [2, 3]\n",
      "2159 Float [2, 3]\n",
      "2162 Float [2]\n",
      "2164 Float [2, 1]\n",
      "2166 Float [2, 1, 1]\n",
      "2154 Float [2, 160, 20, 20]\n",
      "2168 Float [2, 1, 1, 1]\n",
      "2077 Float [2, 160, 20, 20]\n",
      "2169 Float [2, 160, 20, 20]\n",
      "121 Float [160, 1, 7, 7]\n",
      "2176 Float [160, 1, 7, 7]\n",
      "2181 Float [160, 1, 7, 7]\n",
      "2186 Float [160, 1, 3, 7]\n",
      "2191 Float [160, 1, 3, 3]\n",
      "2193 Float [160, 1, 3, 3]\n",
      "2196 Long []\n",
      "2193 Float [160, 1, 3, 3]\n",
      "2199 Long []\n",
      "2193 Float [160, 1, 3, 3]\n",
      "2204 Float [160, 1, 9]\n",
      "135 Float [9, 9]\n",
      "136 Float [9]\n",
      "2205 Float [160, 1, 9]\n",
      "2208 Long []\n",
      "2205 Float [160, 1, 9]\n",
      "2211 Long []\n",
      "2205 Float [160, 1, 9]\n",
      "2031 Float [2, 160, 20, 20]\n",
      "2217 Float [160, 1, 3, 3]\n",
      "input.119 Float [2, 160, 20, 20]\n",
      "122 Float [160]\n",
      "123 Float [160]\n",
      "124 Float [160]\n",
      "125 Float [160]\n",
      "input.121 Float [2, 160, 20, 20]\n",
      "1997 Float [2, 3]\n",
      "2253 Float [2, 3]\n",
      "2256 Float [2]\n",
      "2258 Float [2, 1]\n",
      "2260 Float [2, 1, 1]\n",
      "2248 Float [2, 160, 20, 20]\n",
      "2262 Float [2, 1, 1, 1]\n",
      "2171 Float [2, 160, 20, 20]\n",
      "2263 Float [2, 160, 20, 20]\n",
      "input.123 Float [2, 160, 20, 20]\n",
      "127 Float [40, 160, 1, 1]\n",
      "input.125 Float [2, 40, 20, 20]\n",
      "128 Float [40]\n",
      "129 Float [40]\n",
      "130 Float [40]\n",
      "131 Float [40]\n",
      "2290 Float [2, 40, 20, 20]\n",
      "input.105 Float [2, 40, 20, 20]\n",
      "input.127 Float [2, 40, 20, 20]\n",
      "137 Float [120, 40, 1, 1]\n",
      "input.129 Float [2, 120, 20, 20]\n",
      "138 Float [120]\n",
      "139 Float [120]\n",
      "140 Float [120]\n",
      "141 Float [120]\n",
      "input.131 Float [2, 120, 20, 20]\n",
      "input.133 Float [2, 120, 20, 20]\n",
      "143 Float [120, 1, 7, 7]\n",
      "input.135 Float [2, 120, 10, 10]\n",
      "144 Float [120]\n",
      "145 Float [120]\n",
      "146 Float [120]\n",
      "147 Float [120]\n",
      "input.137 Float [2, 120, 10, 10]\n",
      "input.139 Float [2, 120, 10, 10]\n",
      "149 Float [80, 120, 1, 1]\n",
      "input.141 Float [2, 80, 10, 10]\n",
      "150 Float [80]\n",
      "151 Float [80]\n",
      "152 Float [80]\n",
      "153 Float [80]\n",
      "1230 Float [2, 48]\n",
      "2401 Float [2, 48]\n",
      "2406 Float [2, 2]\n",
      "2406 Float [2, 2]\n",
      "2416 Float [2, 2]\n",
      "2410 Long [2, 1]\n",
      "2396 Float [2, 80, 10, 10]\n",
      "159 Float [240, 80, 1, 1]\n",
      "input.143 Float [2, 240, 10, 10]\n",
      "160 Float [240]\n",
      "161 Float [240]\n",
      "162 Float [240]\n",
      "163 Float [240]\n",
      "input.145 Float [2, 240, 10, 10]\n",
      "2419 Float [2, 2]\n",
      "2458 Float [2, 2]\n",
      "2461 Float [2]\n",
      "2463 Float [2, 1]\n",
      "2465 Float [2, 1, 1]\n",
      "2453 Float [2, 240, 10, 10]\n",
      "2467 Float [2, 1, 1, 1]\n",
      "159 Float [240, 80, 1, 1]\n",
      "2473 Float [80, 80, 1, 1]\n",
      "2478 Float [80, 80, 1, 1]\n",
      "2483 Float [80, 80, 1, 1]\n",
      "2396 Float [2, 80, 10, 10]\n",
      "2488 Float [80, 80, 1, 1]\n",
      "162 Float [240]\n",
      "163 Float [240]\n",
      "160 Float [240]\n",
      "161 Float [240]\n",
      "out.5 Float [2, 80, 10, 10]\n",
      "2523 Float [80]\n",
      "2528 Float [80]\n",
      "2513 Float [80]\n",
      "2518 Float [80]\n",
      "input.147 Float [2, 80, 10, 10]\n",
      "2419 Float [2, 2]\n",
      "2541 Float [2, 2]\n",
      "2544 Float [2]\n",
      "2546 Float [2, 1]\n",
      "2548 Float [2, 1, 1]\n",
      "2536 Float [2, 80, 10, 10]\n",
      "2550 Float [2, 1, 1, 1]\n",
      "2468 Float [2, 240, 10, 10]\n",
      "2551 Float [2, 80, 10, 10]\n",
      "2554 Long []\n",
      "2557 Long []\n",
      "2559 Long []\n",
      "2551 Float [2, 80, 10, 10]\n",
      "2468 Float [2, 240, 10, 10]\n",
      "2569 Float [2, 240, 10, 10]\n",
      "input.149 Float [2, 240, 10, 10]\n",
      "165 Float [240, 1, 3, 3]\n",
      "input.151 Float [2, 240, 10, 10]\n",
      "166 Float [240]\n",
      "167 Float [240]\n",
      "168 Float [240]\n",
      "169 Float [240]\n",
      "input.153 Float [2, 240, 10, 10]\n",
      "input.155 Float [2, 240, 10, 10]\n",
      "171 Float [80, 240, 1, 1]\n",
      "input.157 Float [2, 80, 10, 10]\n",
      "172 Float [80]\n",
      "173 Float [80]\n",
      "174 Float [80]\n",
      "175 Float [80]\n",
      "2624 Float [2, 80, 10, 10]\n",
      "2396 Float [2, 80, 10, 10]\n",
      "1230 Float [2, 48]\n",
      "2631 Float [2, 48]\n",
      "2636 Float [2, 5]\n",
      "2636 Float [2, 5]\n",
      "2646 Float [2, 5]\n",
      "2640 Long [2, 1]\n",
      "2626 Float [2, 80, 10, 10]\n",
      "177 Float [240, 80, 1, 1]\n",
      "input.159 Float [2, 240, 10, 10]\n",
      "178 Float [240]\n",
      "179 Float [240]\n",
      "180 Float [240]\n",
      "181 Float [240]\n",
      "input.161 Float [2, 240, 10, 10]\n",
      "2649 Float [2, 5]\n",
      "2688 Float [2, 5]\n",
      "2691 Float [2]\n",
      "2693 Float [2, 1]\n",
      "2695 Float [2, 1, 1]\n",
      "2683 Float [2, 240, 10, 10]\n",
      "2697 Float [2, 1, 1, 1]\n",
      "177 Float [240, 80, 1, 1]\n",
      "2703 Float [80, 80, 1, 1]\n",
      "2708 Float [80, 80, 1, 1]\n",
      "2713 Float [80, 80, 1, 1]\n",
      "2626 Float [2, 80, 10, 10]\n",
      "2718 Float [80, 80, 1, 1]\n",
      "180 Float [240]\n",
      "181 Float [240]\n",
      "178 Float [240]\n",
      "179 Float [240]\n",
      "out.7 Float [2, 80, 10, 10]\n",
      "2753 Float [80]\n",
      "2758 Float [80]\n",
      "2743 Float [80]\n",
      "2748 Float [80]\n",
      "input.163 Float [2, 80, 10, 10]\n",
      "2649 Float [2, 5]\n",
      "2771 Float [2, 5]\n",
      "2774 Float [2]\n",
      "2776 Float [2, 1]\n",
      "2778 Float [2, 1, 1]\n",
      "2766 Float [2, 80, 10, 10]\n",
      "2780 Float [2, 1, 1, 1]\n",
      "2698 Float [2, 240, 10, 10]\n",
      "2781 Float [2, 80, 10, 10]\n",
      "2784 Long []\n",
      "2787 Long []\n",
      "2789 Long []\n",
      "2781 Float [2, 80, 10, 10]\n",
      "2698 Float [2, 240, 10, 10]\n",
      "2799 Float [2, 240, 10, 10]\n",
      "2801 Float [2, 240, 10, 10]\n",
      "183 Float [240, 1, 7, 7]\n",
      "input.165 Float [2, 240, 10, 10]\n",
      "184 Float [240]\n",
      "185 Float [240]\n",
      "186 Float [240]\n",
      "187 Float [240]\n",
      "input.167 Float [2, 240, 10, 10]\n",
      "2649 Float [2, 5]\n",
      "2837 Float [2, 5]\n",
      "2840 Float [2]\n",
      "2842 Float [2, 1]\n",
      "2844 Float [2, 1, 1]\n",
      "2832 Float [2, 240, 10, 10]\n",
      "2846 Float [2, 1, 1, 1]\n",
      "183 Float [240, 1, 7, 7]\n",
      "2852 Float [240, 1, 7, 7]\n",
      "2857 Float [240, 1, 7, 7]\n",
      "2862 Float [240, 1, 5, 7]\n",
      "2867 Float [240, 1, 5, 5]\n",
      "2869 Float [240, 1, 5, 5]\n",
      "2872 Long []\n",
      "2869 Float [240, 1, 5, 5]\n",
      "2875 Long []\n",
      "2869 Float [240, 1, 5, 5]\n",
      "2880 Float [240, 1, 25]\n",
      "195 Float [25, 25]\n",
      "196 Float [25]\n",
      "2881 Float [240, 1, 25]\n",
      "2884 Long []\n",
      "2881 Float [240, 1, 25]\n",
      "2887 Long []\n",
      "2881 Float [240, 1, 25]\n",
      "2801 Float [2, 240, 10, 10]\n",
      "2893 Float [240, 1, 5, 5]\n",
      "input.169 Float [2, 240, 10, 10]\n",
      "184 Float [240]\n",
      "185 Float [240]\n",
      "186 Float [240]\n",
      "187 Float [240]\n",
      "input.171 Float [2, 240, 10, 10]\n",
      "2649 Float [2, 5]\n",
      "2929 Float [2, 5]\n",
      "2932 Float [2]\n",
      "2934 Float [2, 1]\n",
      "2936 Float [2, 1, 1]\n",
      "2924 Float [2, 240, 10, 10]\n",
      "2938 Float [2, 1, 1, 1]\n",
      "2847 Float [2, 240, 10, 10]\n",
      "2939 Float [2, 240, 10, 10]\n",
      "183 Float [240, 1, 7, 7]\n",
      "2946 Float [240, 1, 7, 7]\n",
      "2951 Float [240, 1, 7, 7]\n",
      "2956 Float [240, 1, 3, 7]\n",
      "2961 Float [240, 1, 3, 3]\n",
      "2963 Float [240, 1, 3, 3]\n",
      "2966 Long []\n",
      "2963 Float [240, 1, 3, 3]\n",
      "2969 Long []\n",
      "2963 Float [240, 1, 3, 3]\n",
      "2974 Float [240, 1, 9]\n",
      "197 Float [9, 9]\n",
      "198 Float [9]\n",
      "2975 Float [240, 1, 9]\n",
      "2978 Long []\n",
      "2975 Float [240, 1, 9]\n",
      "2981 Long []\n",
      "2975 Float [240, 1, 9]\n",
      "2801 Float [2, 240, 10, 10]\n",
      "2987 Float [240, 1, 3, 3]\n",
      "input.173 Float [2, 240, 10, 10]\n",
      "184 Float [240]\n",
      "185 Float [240]\n",
      "186 Float [240]\n",
      "187 Float [240]\n",
      "input.175 Float [2, 240, 10, 10]\n",
      "2649 Float [2, 5]\n",
      "3023 Float [2, 5]\n",
      "3026 Float [2]\n",
      "3028 Float [2, 1]\n",
      "3030 Float [2, 1, 1]\n",
      "3018 Float [2, 240, 10, 10]\n",
      "3032 Float [2, 1, 1, 1]\n",
      "2941 Float [2, 240, 10, 10]\n",
      "3033 Float [2, 240, 10, 10]\n",
      "input.177 Float [2, 240, 10, 10]\n",
      "189 Float [80, 240, 1, 1]\n",
      "input.179 Float [2, 80, 10, 10]\n",
      "190 Float [80]\n",
      "191 Float [80]\n",
      "192 Float [80]\n",
      "193 Float [80]\n",
      "3060 Float [2, 80, 10, 10]\n",
      "2626 Float [2, 80, 10, 10]\n",
      "input.181 Float [2, 80, 10, 10]\n",
      "199 Float [320, 80, 1, 1]\n",
      "input.183 Float [2, 320, 10, 10]\n",
      "200 Float [320]\n",
      "201 Float [320]\n",
      "202 Float [320]\n",
      "203 Float [320]\n",
      "input.185 Float [2, 320, 10, 10]\n",
      "input.187 Float [2, 320, 10, 10]\n",
      "205 Float [320, 1, 3, 3]\n",
      "input.189 Float [2, 320, 10, 10]\n",
      "206 Float [320]\n",
      "207 Float [320]\n",
      "208 Float [320]\n",
      "209 Float [320]\n",
      "input.191 Float [2, 320, 10, 10]\n",
      "input.193 Float [2, 320, 10, 10]\n",
      "211 Float [96, 320, 1, 1]\n",
      "input.195 Float [2, 96, 10, 10]\n",
      "212 Float [96]\n",
      "213 Float [96]\n",
      "214 Float [96]\n",
      "215 Float [96]\n",
      "1230 Float [2, 48]\n",
      "3148 Float [2, 48]\n",
      "3153 Float [2, 4]\n",
      "3153 Float [2, 4]\n",
      "3163 Float [2, 4]\n",
      "3157 Long [2, 1]\n",
      "3143 Float [2, 96, 10, 10]\n",
      "217 Float [288, 96, 1, 1]\n",
      "input.197 Float [2, 288, 10, 10]\n",
      "218 Float [288]\n",
      "219 Float [288]\n",
      "220 Float [288]\n",
      "221 Float [288]\n",
      "input.199 Float [2, 288, 10, 10]\n",
      "3166 Float [2, 4]\n",
      "3205 Float [2, 4]\n",
      "3208 Float [2]\n",
      "3210 Float [2, 1]\n",
      "3212 Float [2, 1, 1]\n",
      "3200 Float [2, 288, 10, 10]\n",
      "3214 Float [2, 1, 1, 1]\n",
      "217 Float [288, 96, 1, 1]\n",
      "3220 Float [96, 96, 1, 1]\n",
      "3225 Float [96, 96, 1, 1]\n",
      "3230 Float [96, 96, 1, 1]\n",
      "3143 Float [2, 96, 10, 10]\n",
      "3235 Float [96, 96, 1, 1]\n",
      "220 Float [288]\n",
      "221 Float [288]\n",
      "218 Float [288]\n",
      "219 Float [288]\n",
      "out.9 Float [2, 96, 10, 10]\n",
      "3270 Float [96]\n",
      "3275 Float [96]\n",
      "3260 Float [96]\n",
      "3265 Float [96]\n",
      "input.201 Float [2, 96, 10, 10]\n",
      "3166 Float [2, 4]\n",
      "3288 Float [2, 4]\n",
      "3291 Float [2]\n",
      "3293 Float [2, 1]\n",
      "3295 Float [2, 1, 1]\n",
      "3283 Float [2, 96, 10, 10]\n",
      "3297 Float [2, 1, 1, 1]\n",
      "3215 Float [2, 288, 10, 10]\n",
      "3298 Float [2, 96, 10, 10]\n",
      "3301 Long []\n",
      "3304 Long []\n",
      "3306 Long []\n",
      "3298 Float [2, 96, 10, 10]\n",
      "3215 Float [2, 288, 10, 10]\n",
      "3316 Float [2, 288, 10, 10]\n",
      "3318 Float [2, 288, 10, 10]\n",
      "223 Float [288, 1, 5, 5]\n",
      "input.203 Float [2, 288, 10, 10]\n",
      "224 Float [288]\n",
      "225 Float [288]\n",
      "226 Float [288]\n",
      "227 Float [288]\n",
      "input.205 Float [2, 288, 10, 10]\n",
      "3166 Float [2, 4]\n",
      "3354 Float [2, 4]\n",
      "3357 Float [2]\n",
      "3359 Float [2, 1]\n",
      "3361 Float [2, 1, 1]\n",
      "3349 Float [2, 288, 10, 10]\n",
      "3363 Float [2, 1, 1, 1]\n",
      "223 Float [288, 1, 5, 5]\n",
      "3369 Float [288, 1, 5, 5]\n",
      "3374 Float [288, 1, 5, 5]\n",
      "3379 Float [288, 1, 3, 5]\n",
      "3384 Float [288, 1, 3, 3]\n",
      "3386 Float [288, 1, 3, 3]\n",
      "3389 Long []\n",
      "3386 Float [288, 1, 3, 3]\n",
      "3392 Long []\n",
      "3386 Float [288, 1, 3, 3]\n",
      "3397 Float [288, 1, 9]\n",
      "235 Float [9, 9]\n",
      "236 Float [9]\n",
      "3398 Float [288, 1, 9]\n",
      "3401 Long []\n",
      "3398 Float [288, 1, 9]\n",
      "3404 Long []\n",
      "3398 Float [288, 1, 9]\n",
      "3318 Float [2, 288, 10, 10]\n",
      "3410 Float [288, 1, 3, 3]\n",
      "input.207 Float [2, 288, 10, 10]\n",
      "224 Float [288]\n",
      "225 Float [288]\n",
      "226 Float [288]\n",
      "227 Float [288]\n",
      "input.209 Float [2, 288, 10, 10]\n",
      "3166 Float [2, 4]\n",
      "3446 Float [2, 4]\n",
      "3449 Float [2]\n",
      "3451 Float [2, 1]\n",
      "3453 Float [2, 1, 1]\n",
      "3441 Float [2, 288, 10, 10]\n",
      "3455 Float [2, 1, 1, 1]\n",
      "3364 Float [2, 288, 10, 10]\n",
      "3456 Float [2, 288, 10, 10]\n",
      "input.211 Float [2, 288, 10, 10]\n",
      "229 Float [96, 288, 1, 1]\n",
      "input.213 Float [2, 96, 10, 10]\n",
      "230 Float [96]\n",
      "231 Float [96]\n",
      "232 Float [96]\n",
      "233 Float [96]\n",
      "3483 Float [2, 96, 10, 10]\n",
      "3143 Float [2, 96, 10, 10]\n",
      "1230 Float [2, 48]\n",
      "3490 Float [2, 48]\n",
      "3495 Float [2, 4]\n",
      "3495 Float [2, 4]\n",
      "3505 Float [2, 4]\n",
      "3499 Long [2, 1]\n",
      "3485 Float [2, 96, 10, 10]\n",
      "237 Float [288, 96, 1, 1]\n",
      "input.215 Float [2, 288, 10, 10]\n",
      "238 Float [288]\n",
      "239 Float [288]\n",
      "240 Float [288]\n",
      "241 Float [288]\n",
      "input.217 Float [2, 288, 10, 10]\n",
      "3508 Float [2, 4]\n",
      "3547 Float [2, 4]\n",
      "3550 Float [2]\n",
      "3552 Float [2, 1]\n",
      "3554 Float [2, 1, 1]\n",
      "3542 Float [2, 288, 10, 10]\n",
      "3556 Float [2, 1, 1, 1]\n",
      "237 Float [288, 96, 1, 1]\n",
      "3562 Float [96, 96, 1, 1]\n",
      "3567 Float [96, 96, 1, 1]\n",
      "3572 Float [96, 96, 1, 1]\n",
      "3485 Float [2, 96, 10, 10]\n",
      "3577 Float [96, 96, 1, 1]\n",
      "240 Float [288]\n",
      "241 Float [288]\n",
      "238 Float [288]\n",
      "239 Float [288]\n",
      "out.11 Float [2, 96, 10, 10]\n",
      "3612 Float [96]\n",
      "3617 Float [96]\n",
      "3602 Float [96]\n",
      "3607 Float [96]\n",
      "input.219 Float [2, 96, 10, 10]\n",
      "3508 Float [2, 4]\n",
      "3630 Float [2, 4]\n",
      "3633 Float [2]\n",
      "3635 Float [2, 1]\n",
      "3637 Float [2, 1, 1]\n",
      "3625 Float [2, 96, 10, 10]\n",
      "3639 Float [2, 1, 1, 1]\n",
      "3557 Float [2, 288, 10, 10]\n",
      "3640 Float [2, 96, 10, 10]\n",
      "3643 Long []\n",
      "3646 Long []\n",
      "3648 Long []\n",
      "3640 Float [2, 96, 10, 10]\n",
      "3557 Float [2, 288, 10, 10]\n",
      "3658 Float [2, 288, 10, 10]\n",
      "3660 Float [2, 288, 10, 10]\n",
      "243 Float [288, 1, 5, 5]\n",
      "input.221 Float [2, 288, 10, 10]\n",
      "244 Float [288]\n",
      "245 Float [288]\n",
      "246 Float [288]\n",
      "247 Float [288]\n",
      "input.223 Float [2, 288, 10, 10]\n",
      "3508 Float [2, 4]\n",
      "3696 Float [2, 4]\n",
      "3699 Float [2]\n",
      "3701 Float [2, 1]\n",
      "3703 Float [2, 1, 1]\n",
      "3691 Float [2, 288, 10, 10]\n",
      "3705 Float [2, 1, 1, 1]\n",
      "243 Float [288, 1, 5, 5]\n",
      "3711 Float [288, 1, 5, 5]\n",
      "3716 Float [288, 1, 5, 5]\n",
      "3721 Float [288, 1, 3, 5]\n",
      "3726 Float [288, 1, 3, 3]\n",
      "3728 Float [288, 1, 3, 3]\n",
      "3731 Long []\n",
      "3728 Float [288, 1, 3, 3]\n",
      "3734 Long []\n",
      "3728 Float [288, 1, 3, 3]\n",
      "3739 Float [288, 1, 9]\n",
      "255 Float [9, 9]\n",
      "256 Float [9]\n",
      "3740 Float [288, 1, 9]\n",
      "3743 Long []\n",
      "3740 Float [288, 1, 9]\n",
      "3746 Long []\n",
      "3740 Float [288, 1, 9]\n",
      "3660 Float [2, 288, 10, 10]\n",
      "3752 Float [288, 1, 3, 3]\n",
      "input.225 Float [2, 288, 10, 10]\n",
      "244 Float [288]\n",
      "245 Float [288]\n",
      "246 Float [288]\n",
      "247 Float [288]\n",
      "input.227 Float [2, 288, 10, 10]\n",
      "3508 Float [2, 4]\n",
      "3788 Float [2, 4]\n",
      "3791 Float [2]\n",
      "3793 Float [2, 1]\n",
      "3795 Float [2, 1, 1]\n",
      "3783 Float [2, 288, 10, 10]\n",
      "3797 Float [2, 1, 1, 1]\n",
      "3706 Float [2, 288, 10, 10]\n",
      "3798 Float [2, 288, 10, 10]\n",
      "input.229 Float [2, 288, 10, 10]\n",
      "249 Float [96, 288, 1, 1]\n",
      "input.231 Float [2, 96, 10, 10]\n",
      "250 Float [96]\n",
      "251 Float [96]\n",
      "252 Float [96]\n",
      "253 Float [96]\n",
      "3825 Float [2, 96, 10, 10]\n",
      "3485 Float [2, 96, 10, 10]\n",
      "input.233 Float [2, 96, 10, 10]\n",
      "257 Float [384, 96, 1, 1]\n",
      "input.235 Float [2, 384, 10, 10]\n",
      "258 Float [384]\n",
      "259 Float [384]\n",
      "260 Float [384]\n",
      "261 Float [384]\n",
      "input.237 Float [2, 384, 10, 10]\n",
      "input.239 Float [2, 384, 10, 10]\n",
      "263 Float [384, 1, 7, 7]\n",
      "input.241 Float [2, 384, 5, 5]\n",
      "264 Float [384]\n",
      "265 Float [384]\n",
      "266 Float [384]\n",
      "267 Float [384]\n",
      "input.243 Float [2, 384, 5, 5]\n",
      "input.245 Float [2, 384, 5, 5]\n",
      "269 Float [192, 384, 1, 1]\n",
      "input.247 Float [2, 192, 5, 5]\n",
      "270 Float [192]\n",
      "271 Float [192]\n",
      "272 Float [192]\n",
      "273 Float [192]\n",
      "1230 Float [2, 48]\n",
      "3936 Float [2, 48]\n",
      "3941 Float [2, 5]\n",
      "3941 Float [2, 5]\n",
      "3951 Float [2, 5]\n",
      "3945 Long [2, 1]\n",
      "3931 Float [2, 192, 5, 5]\n",
      "279 Float [576, 192, 1, 1]\n",
      "input.249 Float [2, 576, 5, 5]\n",
      "280 Float [576]\n",
      "281 Float [576]\n",
      "282 Float [576]\n",
      "283 Float [576]\n",
      "input.251 Float [2, 576, 5, 5]\n",
      "3954 Float [2, 5]\n",
      "3993 Float [2, 5]\n",
      "3996 Float [2]\n",
      "3998 Float [2, 1]\n",
      "4000 Float [2, 1, 1]\n",
      "3988 Float [2, 576, 5, 5]\n",
      "4002 Float [2, 1, 1, 1]\n",
      "279 Float [576, 192, 1, 1]\n",
      "4008 Float [192, 192, 1, 1]\n",
      "4013 Float [192, 192, 1, 1]\n",
      "4018 Float [192, 192, 1, 1]\n",
      "3931 Float [2, 192, 5, 5]\n",
      "4023 Float [192, 192, 1, 1]\n",
      "282 Float [576]\n",
      "283 Float [576]\n",
      "280 Float [576]\n",
      "281 Float [576]\n",
      "out.13 Float [2, 192, 5, 5]\n",
      "4058 Float [192]\n",
      "4063 Float [192]\n",
      "4048 Float [192]\n",
      "4053 Float [192]\n",
      "input.253 Float [2, 192, 5, 5]\n",
      "3954 Float [2, 5]\n",
      "4076 Float [2, 5]\n",
      "4079 Float [2]\n",
      "4081 Float [2, 1]\n",
      "4083 Float [2, 1, 1]\n",
      "4071 Float [2, 192, 5, 5]\n",
      "4085 Float [2, 1, 1, 1]\n",
      "4003 Float [2, 576, 5, 5]\n",
      "4086 Float [2, 192, 5, 5]\n",
      "4089 Long []\n",
      "4092 Long []\n",
      "4094 Long []\n",
      "4086 Float [2, 192, 5, 5]\n",
      "4003 Float [2, 576, 5, 5]\n",
      "4104 Float [2, 576, 5, 5]\n",
      "4106 Float [2, 576, 5, 5]\n",
      "285 Float [576, 1, 7, 7]\n",
      "input.255 Float [2, 576, 5, 5]\n",
      "286 Float [576]\n",
      "287 Float [576]\n",
      "288 Float [576]\n",
      "289 Float [576]\n",
      "input.257 Float [2, 576, 5, 5]\n",
      "3954 Float [2, 5]\n",
      "4142 Float [2, 5]\n",
      "4145 Float [2]\n",
      "4147 Float [2, 1]\n",
      "4149 Float [2, 1, 1]\n",
      "4137 Float [2, 576, 5, 5]\n",
      "4151 Float [2, 1, 1, 1]\n",
      "285 Float [576, 1, 7, 7]\n",
      "4157 Float [576, 1, 7, 7]\n",
      "4162 Float [576, 1, 7, 7]\n",
      "4167 Float [576, 1, 5, 7]\n",
      "4172 Float [576, 1, 5, 5]\n",
      "4174 Float [576, 1, 5, 5]\n",
      "4177 Long []\n",
      "4174 Float [576, 1, 5, 5]\n",
      "4180 Long []\n",
      "4174 Float [576, 1, 5, 5]\n",
      "4185 Float [576, 1, 25]\n",
      "297 Float [25, 25]\n",
      "298 Float [25]\n",
      "4186 Float [576, 1, 25]\n",
      "4189 Long []\n",
      "4186 Float [576, 1, 25]\n",
      "4192 Long []\n",
      "4186 Float [576, 1, 25]\n",
      "4106 Float [2, 576, 5, 5]\n",
      "4198 Float [576, 1, 5, 5]\n",
      "input.259 Float [2, 576, 5, 5]\n",
      "286 Float [576]\n",
      "287 Float [576]\n",
      "288 Float [576]\n",
      "289 Float [576]\n",
      "input.261 Float [2, 576, 5, 5]\n",
      "3954 Float [2, 5]\n",
      "4234 Float [2, 5]\n",
      "4237 Float [2]\n",
      "4239 Float [2, 1]\n",
      "4241 Float [2, 1, 1]\n",
      "4229 Float [2, 576, 5, 5]\n",
      "4243 Float [2, 1, 1, 1]\n",
      "4152 Float [2, 576, 5, 5]\n",
      "4244 Float [2, 576, 5, 5]\n",
      "285 Float [576, 1, 7, 7]\n",
      "4251 Float [576, 1, 7, 7]\n",
      "4256 Float [576, 1, 7, 7]\n",
      "4261 Float [576, 1, 3, 7]\n",
      "4266 Float [576, 1, 3, 3]\n",
      "4268 Float [576, 1, 3, 3]\n",
      "4271 Long []\n",
      "4268 Float [576, 1, 3, 3]\n",
      "4274 Long []\n",
      "4268 Float [576, 1, 3, 3]\n",
      "4279 Float [576, 1, 9]\n",
      "299 Float [9, 9]\n",
      "300 Float [9]\n",
      "4280 Float [576, 1, 9]\n",
      "4283 Long []\n",
      "4280 Float [576, 1, 9]\n",
      "4286 Long []\n",
      "4280 Float [576, 1, 9]\n",
      "4106 Float [2, 576, 5, 5]\n",
      "4292 Float [576, 1, 3, 3]\n",
      "input.263 Float [2, 576, 5, 5]\n",
      "286 Float [576]\n",
      "287 Float [576]\n",
      "288 Float [576]\n",
      "289 Float [576]\n",
      "input.265 Float [2, 576, 5, 5]\n",
      "3954 Float [2, 5]\n",
      "4328 Float [2, 5]\n",
      "4331 Float [2]\n",
      "4333 Float [2, 1]\n",
      "4335 Float [2, 1, 1]\n",
      "4323 Float [2, 576, 5, 5]\n",
      "4337 Float [2, 1, 1, 1]\n",
      "4246 Float [2, 576, 5, 5]\n",
      "4338 Float [2, 576, 5, 5]\n",
      "input.267 Float [2, 576, 5, 5]\n",
      "291 Float [192, 576, 1, 1]\n",
      "input.269 Float [2, 192, 5, 5]\n",
      "292 Float [192]\n",
      "293 Float [192]\n",
      "294 Float [192]\n",
      "295 Float [192]\n",
      "4365 Float [2, 192, 5, 5]\n",
      "3931 Float [2, 192, 5, 5]\n",
      "1230 Float [2, 48]\n",
      "4372 Float [2, 48]\n",
      "4377 Float [2, 4]\n",
      "4377 Float [2, 4]\n",
      "4387 Float [2, 4]\n",
      "4381 Long [2, 1]\n",
      "4367 Float [2, 192, 5, 5]\n",
      "301 Float [576, 192, 1, 1]\n",
      "input.271 Float [2, 576, 5, 5]\n",
      "302 Float [576]\n",
      "303 Float [576]\n",
      "304 Float [576]\n",
      "305 Float [576]\n",
      "input.273 Float [2, 576, 5, 5]\n",
      "4390 Float [2, 4]\n",
      "4429 Float [2, 4]\n",
      "4432 Float [2]\n",
      "4434 Float [2, 1]\n",
      "4436 Float [2, 1, 1]\n",
      "4424 Float [2, 576, 5, 5]\n",
      "4438 Float [2, 1, 1, 1]\n",
      "301 Float [576, 192, 1, 1]\n",
      "4444 Float [192, 192, 1, 1]\n",
      "4449 Float [192, 192, 1, 1]\n",
      "4454 Float [192, 192, 1, 1]\n",
      "4367 Float [2, 192, 5, 5]\n",
      "4459 Float [192, 192, 1, 1]\n",
      "304 Float [576]\n",
      "305 Float [576]\n",
      "302 Float [576]\n",
      "303 Float [576]\n",
      "out Float [2, 192, 5, 5]\n",
      "4494 Float [192]\n",
      "4499 Float [192]\n",
      "4484 Float [192]\n",
      "4489 Float [192]\n",
      "input.275 Float [2, 192, 5, 5]\n",
      "4390 Float [2, 4]\n",
      "4512 Float [2, 4]\n",
      "4515 Float [2]\n",
      "4517 Float [2, 1]\n",
      "4519 Float [2, 1, 1]\n",
      "4507 Float [2, 192, 5, 5]\n",
      "4521 Float [2, 1, 1, 1]\n",
      "4439 Float [2, 576, 5, 5]\n",
      "4522 Float [2, 192, 5, 5]\n",
      "4525 Long []\n",
      "4528 Long []\n",
      "4530 Long []\n",
      "4522 Float [2, 192, 5, 5]\n",
      "4439 Float [2, 576, 5, 5]\n",
      "4540 Float [2, 576, 5, 5]\n",
      "4542 Float [2, 576, 5, 5]\n",
      "307 Float [576, 1, 5, 5]\n",
      "input.277 Float [2, 576, 5, 5]\n",
      "308 Float [576]\n",
      "309 Float [576]\n",
      "310 Float [576]\n",
      "311 Float [576]\n",
      "input.279 Float [2, 576, 5, 5]\n",
      "4390 Float [2, 4]\n",
      "4578 Float [2, 4]\n",
      "4581 Float [2]\n",
      "4583 Float [2, 1]\n",
      "4585 Float [2, 1, 1]\n",
      "4573 Float [2, 576, 5, 5]\n",
      "4587 Float [2, 1, 1, 1]\n",
      "307 Float [576, 1, 5, 5]\n",
      "4593 Float [576, 1, 5, 5]\n",
      "4598 Float [576, 1, 5, 5]\n",
      "4603 Float [576, 1, 3, 5]\n",
      "4608 Float [576, 1, 3, 3]\n",
      "4610 Float [576, 1, 3, 3]\n",
      "4613 Long []\n",
      "4610 Float [576, 1, 3, 3]\n",
      "4616 Long []\n",
      "4610 Float [576, 1, 3, 3]\n",
      "4621 Float [576, 1, 9]\n",
      "319 Float [9, 9]\n",
      "320 Float [9]\n",
      "4622 Float [576, 1, 9]\n",
      "4625 Long []\n",
      "4622 Float [576, 1, 9]\n",
      "4628 Long []\n",
      "4622 Float [576, 1, 9]\n",
      "4542 Float [2, 576, 5, 5]\n",
      "4634 Float [576, 1, 3, 3]\n",
      "input.281 Float [2, 576, 5, 5]\n",
      "308 Float [576]\n",
      "309 Float [576]\n",
      "310 Float [576]\n",
      "311 Float [576]\n",
      "input.283 Float [2, 576, 5, 5]\n",
      "4390 Float [2, 4]\n",
      "4670 Float [2, 4]\n",
      "4673 Float [2]\n",
      "4675 Float [2, 1]\n",
      "4677 Float [2, 1, 1]\n",
      "4665 Float [2, 576, 5, 5]\n",
      "4679 Float [2, 1, 1, 1]\n",
      "4588 Float [2, 576, 5, 5]\n",
      "4680 Float [2, 576, 5, 5]\n",
      "input.285 Float [2, 576, 5, 5]\n",
      "313 Float [192, 576, 1, 1]\n",
      "input.287 Float [2, 192, 5, 5]\n",
      "314 Float [192]\n",
      "315 Float [192]\n",
      "316 Float [192]\n",
      "317 Float [192]\n",
      "4707 Float [2, 192, 5, 5]\n",
      "4367 Float [2, 192, 5, 5]\n",
      "input.289 Float [2, 192, 5, 5]\n",
      "321 Float [768, 192, 1, 1]\n",
      "input.291 Float [2, 768, 5, 5]\n",
      "322 Float [768]\n",
      "323 Float [768]\n",
      "324 Float [768]\n",
      "325 Float [768]\n",
      "input.293 Float [2, 768, 5, 5]\n",
      "input.295 Float [2, 768, 5, 5]\n",
      "327 Float [768, 1, 5, 5]\n",
      "input.297 Float [2, 768, 5, 5]\n",
      "328 Float [768]\n",
      "329 Float [768]\n",
      "330 Float [768]\n",
      "331 Float [768]\n",
      "input.299 Float [2, 768, 5, 5]\n",
      "input.301 Float [2, 768, 5, 5]\n",
      "333 Float [320, 768, 1, 1]\n",
      "input Float [2, 320, 5, 5]\n",
      "334 Float [320]\n",
      "335 Float [320]\n",
      "336 Float [320]\n",
      "337 Float [320]\n",
      "4813 Float [2, 320, 5, 5]\n",
      "4818 Float [2, 320, 5]\n",
      "4823 Float [2, 320]\n",
      "341 Float [1000, 320]\n",
      "342 Float [1000]\n"
     ]
    }
   ],
   "source": [
    "variables = dict()\n",
    "for x in graph.nodes():\n",
    "    for v in list(x.inputs()):\n",
    "        if 'tensor' in v.type().kind().lower():\n",
    "            print(v.debugName(), v.type().scalarType(), v.type().sizes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%1302 : Float(2, 120, 40, 40, strides=[192000, 1600, 40, 1], requires_grad=1, device=cuda:0) = aten::mul_(%1287, %1301) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:172:0\n",
      "\n",
      "%1385 : Float(2, 24, 40, 40, strides=[38400, 1600, 40, 1], requires_grad=1, device=cuda:0) = aten::mul_(%1370, %1384) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:177:0\n",
      "\n",
      "%1488 : Float(2, 72, 40, 40, strides=[115200, 1600, 40, 1], requires_grad=1, device=cuda:0) = aten::mul_(%1473, %1487) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:177:0\n",
      "\n",
      "%1666 : Float(2, 96, 40, 40, strides=[153600, 1600, 40, 1], requires_grad=1, device=cuda:0) = aten::mul_(%1651, %1665) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:142:0\n",
      "\n",
      "%1758 : Float(2, 96, 40, 40, strides=[153600, 1600, 40, 1], requires_grad=1, device=cuda:0) = aten::mul_(%1743, %1757) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:153:0\n",
      "\n",
      "%2077 : Float(2, 160, 20, 20, strides=[64000, 400, 20, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2062, %2076) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:142:0\n",
      "\n",
      "%2169 : Float(2, 160, 20, 20, strides=[64000, 400, 20, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2154, %2168) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:153:0\n",
      "\n",
      "%2263 : Float(2, 160, 20, 20, strides=[64000, 400, 20, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2248, %2262) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:153:0\n",
      "\n",
      "%2468 : Float(2, 240, 10, 10, strides=[24000, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2453, %2467) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:172:0\n",
      "\n",
      "%2551 : Float(2, 80, 10, 10, strides=[8000, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2536, %2550) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:177:0\n",
      "\n",
      "%2698 : Float(2, 240, 10, 10, strides=[24000, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2683, %2697) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:192:0\n",
      "\n",
      "%2781 : Float(2, 80, 10, 10, strides=[8000, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2766, %2780) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:197:0\n",
      "\n",
      "%2847 : Float(2, 240, 10, 10, strides=[24000, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2832, %2846) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:207:0\n",
      "\n",
      "%2939 : Float(2, 240, 10, 10, strides=[24000, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%2924, %2938) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:218:0\n",
      "\n",
      "%3033 : Float(2, 240, 10, 10, strides=[24000, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3018, %3032) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:218:0\n",
      "\n",
      "%3215 : Float(2, 288, 10, 10, strides=[28800, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3200, %3214) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:192:0\n",
      "\n",
      "%3298 : Float(2, 96, 10, 10, strides=[9600, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3283, %3297) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:197:0\n",
      "\n",
      "%3364 : Float(2, 288, 10, 10, strides=[28800, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3349, %3363) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:207:0\n",
      "\n",
      "%3456 : Float(2, 288, 10, 10, strides=[28800, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3441, %3455) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:218:0\n",
      "\n",
      "%3557 : Float(2, 288, 10, 10, strides=[28800, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3542, %3556) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:192:0\n",
      "\n",
      "%3640 : Float(2, 96, 10, 10, strides=[9600, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3625, %3639) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:197:0\n",
      "\n",
      "%3706 : Float(2, 288, 10, 10, strides=[28800, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3691, %3705) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:207:0\n",
      "\n",
      "%3798 : Float(2, 288, 10, 10, strides=[28800, 100, 10, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3783, %3797) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:218:0\n",
      "\n",
      "%4003 : Float(2, 576, 5, 5, strides=[14400, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%3988, %4002) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:192:0\n",
      "\n",
      "%4086 : Float(2, 192, 5, 5, strides=[4800, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%4071, %4085) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:197:0\n",
      "\n",
      "%4152 : Float(2, 576, 5, 5, strides=[14400, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%4137, %4151) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:207:0\n",
      "\n",
      "%4244 : Float(2, 576, 5, 5, strides=[14400, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%4229, %4243) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:218:0\n",
      "\n",
      "%4338 : Float(2, 576, 5, 5, strides=[14400, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%4323, %4337) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:218:0\n",
      "\n",
      "%4439 : Float(2, 576, 5, 5, strides=[14400, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%4424, %4438) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:192:0\n",
      "\n",
      "%4522 : Float(2, 192, 5, 5, strides=[4800, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%4507, %4521) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:197:0\n",
      "\n",
      "%4588 : Float(2, 576, 5, 5, strides=[14400, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%4573, %4587) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:207:0\n",
      "\n",
      "%4680 : Float(2, 576, 5, 5, strides=[14400, 25, 5, 1], requires_grad=1, device=cuda:0) = aten::mul_(%4665, %4679) # /home/pdh/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:218:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in graph.nodes():\n",
    "    if 'mul_' in x.kind().lower():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ori model size : 6.6 MB\n",
      "Gumbel model size : 8.2 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                             Kernel Shape     Output Shape     Param #          Mult-Adds        Param %\n",
       "==================================================================================================================================\n",
       "GumbelMCUNets                                      --               [4, 1000]        --               --                    --\n",
       "ConvLayer: 1-1                                   3                [4, 32, 80, 80]  --               --                    --\n",
       "    Conv2d: 2-1                                 [3, 3]           [4, 32, 80, 80]  864              22,118,400         0.04%\n",
       "    BatchNorm2d: 2-2                            --               [4, 32, 80, 80]  64               256                0.00%\n",
       "    ReLU6: 2-3                                  --               [4, 32, 80, 80]  --               --                    --\n",
       "ModuleList: 1-8                                  --               --               (recursive)      --               (recursive)\n",
       "    MobileInvertedResidualBlock: 2-4            --               [4, 16, 80, 80]  896              20,480,384         0.04%\n",
       "    MobileInvertedResidualBlock: 2-5            --               [4, 24, 40, 40]  4,512            42,087,360         0.21%\n",
       "AdaptiveAvgPool2d: 1-3                           --               [4, 24, 8, 8]    --               --                    --\n",
       "Flatten: 1-4                                     --               [4, 1536]        --               --                    --\n",
       "Linear: 1-5                                      --               [4, 256]         393,472          1,573,888         18.36%\n",
       "Dropout: 1-6                                     --               [4, 256]         --               --                    --\n",
       "Linear: 1-7                                      --               [4, 48]          12,336           49,344             0.58%\n",
       "ModuleList: 1-8                                  --               --               (recursive)      --               (recursive)\n",
       "    MobileGumbelInvertedResidualBlock: 2-6      --               [4, 24, 40, 40]  7,368            25,346,112         0.34%\n",
       "    MobileGumbelInvertedResidualBlock: 2-7      --               [4, 24, 40, 40]  7,530            29,502,336         0.35%\n",
       "    MobileGumbelInvertedResidualBlock: 2-8      --               [4, 40, 20, 20]  14,860           35,522,240         0.69%\n",
       "    MobileGumbelInvertedResidualBlock: 2-9      --               [4, 40, 20, 20]  14,960           22,786,880         0.70%\n",
       "    MobileGumbelInvertedResidualBlock: 2-10     --               [4, 40, 20, 20]  22,100           20,603,840         1.03%\n",
       "    MobileGumbelInvertedResidualBlock: 2-11     --               [4, 80, 10, 10]  21,660           13,874,560         1.01%\n",
       "    MobileGumbelInvertedResidualBlock: 2-12     --               [4, 80, 10, 10]  41,680           8,548,480          1.94%\n",
       "    MobileGumbelInvertedResidualBlock: 2-13     --               [4, 80, 10, 10]  52,020           7,865,920          2.43%\n",
       "    MobileGumbelInvertedResidualBlock: 2-14     --               [4, 96, 10, 10]  60,672           23,685,888         2.83%\n",
       "    MobileGumbelInvertedResidualBlock: 2-15     --               [4, 96, 10, 10]  63,930           11,092,800         2.98%\n",
       "    MobileGumbelInvertedResidualBlock: 2-16     --               [4, 96, 10, 10]  63,930           11,092,800         2.98%\n",
       "    MobileGumbelInvertedResidualBlock: 2-17     --               [4, 192, 5, 5]   132,068          24,007,680         6.16%\n",
       "    MobileGumbelInvertedResidualBlock: 2-18     --               [4, 192, 5, 5]   252,836          11,505,408        11.80%\n",
       "    MobileGumbelInvertedResidualBlock: 2-19     --               [4, 192, 5, 5]   238,362          11,126,400        11.12%\n",
       "    MobileGumbelInvertedResidualBlock: 2-20     --               [4, 320, 5, 5]   416,218          41,256,448        19.42%\n",
       "LinearLayer: 1-9                                 --               [4, 1000]        --               --                    --\n",
       "    Linear: 2-21                                --               [4, 1000]        321,000          1,284,000         14.98%\n",
       "==================================================================================================================================\n",
       "Total params: 2,143,338\n",
       "Trainable params: 2,143,338\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 385.41\n",
       "==================================================================================================================================\n",
       "Input size (MB): 1.23\n",
       "Forward/backward pass size (MB): 172.48\n",
       "Params size (MB): 6.98\n",
       "Estimated Total Size (MB): 180.69\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_model_size = sum([p.numel() for p in ori_model.parameters()]) * 4 / 2**20\n",
    "gumbel_model_size = sum([p.numel() for p in gumbel_model.parameters()]) * 4 / 2**20\n",
    "print(\"Ori model size : %.1f MB\" % ori_model_size)\n",
    "print(\"Gumbel model size : %.1f MB\" % gumbel_model_size)\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(gumbel_model, input_size=(4, 3, 160, 160), col_width=16, col_names=['kernel_size', 'output_size', 'num_params', 'mult_adds', 'params_percent'], depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before forward grad :  None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb  10\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbefore forward grad : \u001b[39m\u001b[39m\"\u001b[39m, gumbel_model\u001b[39m.\u001b[39mgumbel_fc1\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mgrad)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m out \u001b[39m=\u001b[39m gumbel_model(torch\u001b[39m.\u001b[39;49mrandn(\u001b[39m32\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m160\u001b[39;49m, \u001b[39m160\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m out\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mafter forward grad : \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, gumbel_model\u001b[39m.\u001b[39mgumbel_fc1\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1186\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:353\u001b[0m, in \u001b[0;36mGumbelMCUNets.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 353\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfirst_conv(x)\n\u001b[1;32m    354\u001b[0m     \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):            \n\u001b[1;32m    355\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgumbel_feature_extract_block:\n\u001b[1;32m    356\u001b[0m             \u001b[39m# feautre map and gumbel output extract\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1186\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/torch/NAS/TinyML_NAS/mcunet/mcunet/tinynas/nn/modules/layers.py:100\u001b[0m, in \u001b[0;36mMy2DLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     98\u001b[0m     \u001b[39m# similar to nn.Sequential\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m--> 100\u001b[0m         x \u001b[39m=\u001b[39m module(x)\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1186\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "print(\"before forward grad : \", gumbel_model.gumbel_fc1.weight.grad)\n",
    "out = gumbel_model(torch.randn(32, 3, 160, 160))\n",
    "\n",
    "out.sum().backward()\n",
    "\n",
    "print(\"after forward grad : \\n\", gumbel_model.gumbel_fc1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_conv.conv.weight\n",
      "first_conv.bn.weight\n",
      "first_conv.bn.bias\n",
      "blocks.0.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.0.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.0.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.0.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.0.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.0.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.1.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.1.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.1.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.1.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.1.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.1.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.1.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.1.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.1.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.2.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.2.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.2.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.2.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.2.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.2.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.2.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.2.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.2.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.3.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.3.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.3.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.3.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.3.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.3.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.3.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.3.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.3.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.4.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.4.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.4.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.4.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.4.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.4.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.4.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.4.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.4.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.5.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.5.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.5.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.5.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.5.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.5.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.5.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.5.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.5.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.6.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.6.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.6.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.6.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.6.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.6.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.6.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.6.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.6.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.7.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.7.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.7.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.7.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.7.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.7.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.7.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.7.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.7.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.8.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.8.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.8.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.8.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.8.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.8.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.8.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.8.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.8.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.9.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.9.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.9.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.9.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.9.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.9.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.9.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.9.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.9.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.10.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.10.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.10.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.10.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.10.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.10.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.10.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.10.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.10.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.11.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.11.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.11.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.11.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.11.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.11.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.11.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.11.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.11.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.12.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.12.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.12.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.12.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.12.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.12.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.12.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.12.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.12.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.13.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.13.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.13.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.13.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.13.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.13.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.13.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.13.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.13.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.14.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.14.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.14.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.14.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.14.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.14.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.14.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.14.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.14.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.15.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.15.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.15.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.15.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.15.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.15.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.15.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.15.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.15.mobile_inverted_conv.point_linear.bn.bias\n",
      "blocks.16.mobile_inverted_conv.inverted_bottleneck.conv.weight\n",
      "blocks.16.mobile_inverted_conv.inverted_bottleneck.bn.weight\n",
      "blocks.16.mobile_inverted_conv.inverted_bottleneck.bn.bias\n",
      "blocks.16.mobile_inverted_conv.depth_conv.conv.weight\n",
      "blocks.16.mobile_inverted_conv.depth_conv.bn.weight\n",
      "blocks.16.mobile_inverted_conv.depth_conv.bn.bias\n",
      "blocks.16.mobile_inverted_conv.point_linear.conv.weight\n",
      "blocks.16.mobile_inverted_conv.point_linear.bn.weight\n",
      "blocks.16.mobile_inverted_conv.point_linear.bn.bias\n",
      "classifier.linear.weight\n",
      "classifier.linear.bias\n"
     ]
    }
   ],
   "source": [
    "for n, p in net.named_parameters():\n",
    "    if has_deep_attr(model, n):\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProxylessNASNets(\n",
       "  (first_conv): ConvLayer(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU6(inplace=True)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(48, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(24, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)\n",
       "          (bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(120, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (3): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (4): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(24, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(120, 120, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=120, bias=False)\n",
       "          (bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (6): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(40, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=160, bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(160, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (7): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(120, 120, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=120, bias=False)\n",
       "          (bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (9): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)\n",
       "          (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (10): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(320, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (12): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (13): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=384, bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (15): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (shortcut): IdentityLayer()\n",
       "    )\n",
       "    (16): MobileInvertedResidualBlock(\n",
       "      (mobile_inverted_conv): MBInvertedConvLayer(\n",
       "        (inverted_bottleneck): Sequential(\n",
       "          (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (depth_conv): Sequential(\n",
       "          (conv): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "          (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): ReLU6(inplace=True)\n",
       "        )\n",
       "        (point_linear): Sequential(\n",
       "          (conv): Conv2d(768, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): LinearLayer(\n",
       "    (linear): Linear(in_features=320, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'MBGumbelInvertedConvLayer',\n",
       " 'in_channels': 16,\n",
       " 'out_channels': 24,\n",
       " 'kernel_size': 7,\n",
       " 'kernel_size_list': [7, 5, 3],\n",
       " 'stride': 2,\n",
       " 'expand_ratio': 3,\n",
       " 'expand_ratio_list': [1, 3],\n",
       " 'mid_channels': 48,\n",
       " 'act_func': 'relu6',\n",
       " 'use_se': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbconv_test = MBGumbelInvertedConvLayer.build_from_config(m.mobile_inverted_conv.config)\n",
    "mbconv_test.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(2, 16, 32, 32)\n",
    "gumbel_inputs = torch.randn(2, 4, 8, 8)\n",
    "gumbel_inputs.requires_grad = True\n",
    "gumbel_layer = nn.Linear(4*8*8, 5)\n",
    "gumbel_output = gumbel_layer(gumbel_inputs.view(2, -1))\n",
    "gumbel_index = F.gumbel_softmax(gumbel_output, tau=1, hard=True)\n",
    "print(gumbel_index)\n",
    "out = mbconv_test.forward(torch.randn(2, 16, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.]], grad_fn=<AddBackward0>)\n",
      "gumbel shape :  torch.Size([2, 5])\n",
      "1 6 5 7 torch.Size([48, 1, 7, 7])\n",
      "1 4 3 5 torch.Size([48, 1, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(2, 16, 32, 32)\n",
    "gumbel_inputs = torch.randn(2, 4, 8, 8)\n",
    "gumbel_inputs.requires_grad = True\n",
    "gumbel_layer = nn.Linear(4*8*8, 5)\n",
    "gumbel_output = gumbel_layer(gumbel_inputs.view(2, -1))\n",
    "gumbel_index = F.gumbel_softmax(gumbel_output, tau=1, hard=True)\n",
    "print(gumbel_index)\n",
    "out = mbconv_test.forward(torch.randn(2, 16, 32, 32), gumbel_index)\n",
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_mbconv_test_weight = copy.deepcopy(mbconv_test.depth_conv.conv.weight)\n",
    "print(original_mbconv_test_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.mobile_inverted_conv.depth_conv.conv.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in m.mobile_inverted_conv.named_parameters():\n",
    "    if has_deep_attr(mbconv_test, n):\n",
    "        print(n, p)\n",
    "        set_deep_attr(mbconv_test, n, p)\n",
    "        print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in m.mobile_inverted_conv.named_parameters():\n",
    "    if has_deep_attr(mbconv_test, n):\n",
    "        print(n)\n",
    "        print(get_deep_attr(mbconv_test, n) - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbconv_test.forward(torch.randn(1,32,16,16), gumbel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_layer = nn.BatchNorm2d(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 12, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 12\n",
    "out = F.batch_norm(x, bn_layer.running_mean[:feature_dim], bn_layer.running_var[:feature_dim], bn_layer.weight[:feature_dim], bn_layer.bias[:feature_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, img_size, desc = build_model(net_id='mcunet-in4', pretrained=True)\n",
    "\n",
    "backup_model = copy.deepcopy(model)\n",
    "model_copy = build_model(net_id='mcunet-in4', pretrained=False)[0]\n",
    "\n",
    "for (n1, p1), (n2, p2) in zip(backup_model.named_parameters(), model_copy.named_parameters()):\n",
    "    if n1 == n2:\n",
    "        print((p1 - p2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if has_deep_attr(model_copy, n):\n",
    "        print(n)\n",
    "        set_deep_attr(model_copy, n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n1, p1), (n2, p2) in zip(backup_model.named_parameters(), model_copy.named_parameters()):\n",
    "    if n1 == n2:\n",
    "        print((p1-p2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

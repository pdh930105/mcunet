{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "sys.path.append('./mcunet')\n",
    "\n",
    "from mcunet.gumbel_module.gumbel_net import GumbelMCUNet\n",
    "from mcunet.gumbel_module.gumbel_layer import MBGumbelInvertedConvLayer, MobileGumbelInvertedResidualBlock, count_conv_gumbel_flops\n",
    "from mcunet.tinynas.nn.modules import MBInvertedConvLayer\n",
    "from mcunet.tinynas.nn.networks import MobileInvertedResidualBlock\n",
    "from mcunet.model_zoo import build_model\n",
    "\n",
    "from mcunet.utils import MyModule, MyNetwork, SEModule, build_activation, get_same_padding, sub_filter_start_end, rm_bn_from_net, get_deep_attr, has_deep_attr\n",
    "from mcunet.tinynas.nn.modules import ZeroLayer, set_layer_from_config\n",
    "\n",
    "from fvcore.nn import FlopCountAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained mcumodel to gumbel net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::hardtanh_ encountered 1 time(s)\n",
      "Unsupported operator aten::hardtanh_ encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Log Static & Dynamic Flops : 367427584, 1670131200\n"
     ]
    }
   ],
   "source": [
    "ori_model, img_size, desc = build_model(net_id='mcunet-in4', pretrained=True)\n",
    "gubmel_config = {'global_expand_ratio_list':[1,3,4,5,6], 'global_kernel_size_list':[3,5,7], 'gumbel_feature_extract_block_idx':2}\n",
    "gumbel_model = GumbelMCUNet.build_from_config(ori_model.config, gubmel_config)\n",
    "gumbel_model.load_pretrained_mcunet_param(ori_model)\n",
    "\n",
    "inputs = torch.randn(16, 3, 160, 160).cuda()\n",
    "gumbel_model = gumbel_model.cuda()\n",
    "output, gumbel_list = gumbel_model.forward(inputs.cuda())\n",
    "#out = gumbel_model.forward_original(inputs)\n",
    "#out2 = ori_model.forward(inputs)\n",
    "#print((out - out2).sum())\n",
    "gumbel_model.set_static_flops(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model, img_size, desc = build_model(net_id='mcunet-in4', pretrained=True)\n",
    "\n",
    "rm_bn_from_net(ori_model)\n",
    "flops = FlopCountAnalysis(ori_model, torch.randn(1, 3, 160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module :  blocks.2.mobile_inverted_conv\n",
      "fvcore vs custom flops : 10944000, 10944000.0 tensor(0.)\n",
      "========================================\n",
      "module :  blocks.3.mobile_inverted_conv\n",
      "fvcore vs custom flops : 11212800, 11212800.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (3), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 40 / 40\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([96, 24, 1, 1])\n",
      "depth conv weight shape : torch.Size([96, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([24, 96, 1, 1])\n",
      "max expand size : 4\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800., 3686400.])\n",
      "dw convert flops :,  tensor([   0., 7776.])\n",
      "dw blocks flops : tensor([3840000., 1382400.])\n",
      "pw blocks flops : tensor([ 921600., 2764800., 3686400.])\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.3.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 3686400})\n",
      "blocks.3.mobile_inverted_conv.depth_conv.conv Counter({'conv': 3840000})\n",
      "blocks.3.mobile_inverted_conv.point_linear.conv Counter({'conv': 3686400})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 921600., 2764800., 3686400.]) dw conv : tensor([3840000., 1382400.]) pw conv : tensor([ 921600., 2764800., 3686400.])\n",
      "expand ratio list :  tensor([1, 3, 4])\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1., 0.]])\n",
      "fvcore 11212800  vs custom flops 2803200 :  8409600\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1., 0.]])\n",
      "fvcore 11212800  vs custom flops 8409600 :  2803200\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 1., 0.]])\n",
      "fvcore 11212800  vs custom flops 11212800 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 1.]])\n",
      "fvcore 11212800  vs custom flops 2196576 :  9016224\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 1.]])\n",
      "fvcore 11212800  vs custom flops 6574176 :  4638624\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 1.]])\n",
      "fvcore 11212800  vs custom flops 8762976 :  2449824\n",
      "========================================\n",
      "module :  blocks.4.mobile_inverted_conv\n",
      "fvcore vs custom flops : 8880000, 8880000.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (4), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 40 / 40\n",
      "stride : 2\n",
      "inverted conv input shape : torch.Size([120, 24, 1, 1])\n",
      "depth conv weight shape : torch.Size([120, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([40, 120, 1, 1])\n",
      "max expand size : 5\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800., 3686400., 4608000.])\n",
      "dw convert flops :,  tensor([    0., 75000., 84720.])\n",
      "dw blocks flops : tensor([2352000., 1200000.,  432000.])\n",
      "pw blocks flops : tensor([ 384000., 1152000., 1536000., 1920000.])\n",
      "gumbel idx :  tensor([[0., 0., 0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.4.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 4608000})\n",
      "blocks.4.mobile_inverted_conv.depth_conv.conv Counter({'conv': 2352000})\n",
      "blocks.4.mobile_inverted_conv.point_linear.conv Counter({'conv': 1920000})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 921600., 2764800., 3686400., 4608000.]) dw conv : tensor([2352000., 1200000.,  432000.]) pw conv : tensor([ 384000., 1152000., 1536000., 1920000.])\n",
      "expand ratio list :  tensor([1, 3, 4, 5])\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.2000])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 1., 0., 0.]])\n",
      "fvcore 8880000  vs custom flops 1776000 :  7104000\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.6000])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 1., 0., 0.]])\n",
      "fvcore 8880000  vs custom flops 5328000 :  3552000\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.8000])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 1., 0., 0.]])\n",
      "fvcore 8880000  vs custom flops 7104000 :  1776000\n",
      "gumbel idx :  tensor([[0., 0., 0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 0., 1., 1., 0., 0.]])\n",
      "fvcore 8880000  vs custom flops 8880000 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.2000])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 0., 1., 0.]])\n",
      "fvcore 8880000  vs custom flops 1620600 :  7259400\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.6000])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 0., 1., 0.]])\n",
      "fvcore 8880000  vs custom flops 4711800 :  4168200\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.8000])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 0., 1., 0.]])\n",
      "fvcore 8880000  vs custom flops 6257400 :  2622600\n",
      "gumbel idx :  tensor([[0., 0., 0., 1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 0., 1.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 0., 1., 0., 1., 0.]])\n",
      "fvcore 8880000  vs custom flops 7803000 :  1077000\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.2000])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 0., 0., 1.]])\n",
      "fvcore 8880000  vs custom flops 1476720 :  7403280\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.6000])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 0., 0., 1.]])\n",
      "fvcore 8880000  vs custom flops 4260720 :  4619280\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.8000])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 0., 0., 1.]])\n",
      "fvcore 8880000  vs custom flops 5652720 :  3227280\n",
      "gumbel idx :  tensor([[0., 0., 0., 1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 0., 1.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 0., 1., 0., 0., 1.]])\n",
      "fvcore 8880000  vs custom flops 7044720 :  1835280\n",
      "========================================\n",
      "module :  blocks.5.mobile_inverted_conv\n",
      "fvcore vs custom flops : 5696000, 5696000.0 tensor(0.)\n",
      "========================================\n",
      "module :  blocks.6.mobile_inverted_conv\n",
      "fvcore vs custom flops : 8256000, 8256000.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (3), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 20 / 20\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([160, 40, 1, 1])\n",
      "depth conv weight shape : torch.Size([160, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([40, 160, 1, 1])\n",
      "max expand size : 4\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 640000., 1920000., 2560000.])\n",
      "dw convert flops :,  tensor([     0., 100000., 112960.])\n",
      "dw blocks flops : tensor([3136000., 1600000.,  576000.])\n",
      "pw blocks flops : tensor([ 640000., 1920000., 2560000.])\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.6.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2560000})\n",
      "blocks.6.mobile_inverted_conv.depth_conv.conv Counter({'conv': 3136000})\n",
      "blocks.6.mobile_inverted_conv.point_linear.conv Counter({'conv': 2560000})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 640000., 1920000., 2560000.]) dw conv : tensor([3136000., 1600000.,  576000.]) pw conv : tensor([ 640000., 1920000., 2560000.])\n",
      "expand ratio list :  tensor([1, 3, 4])\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1., 0., 0.]])\n",
      "fvcore 8256000  vs custom flops 2064000 :  6192000\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1., 0., 0.]])\n",
      "fvcore 8256000  vs custom flops 6192000 :  2064000\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 1., 0., 0.]])\n",
      "fvcore 8256000  vs custom flops 8256000 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 1., 0.]])\n",
      "fvcore 8256000  vs custom flops 1780000 :  6476000\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 1., 0.]])\n",
      "fvcore 8256000  vs custom flops 5140000 :  3116000\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 1., 0.]])\n",
      "fvcore 8256000  vs custom flops 6820000 :  1436000\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 0., 1.]])\n",
      "fvcore 8256000  vs custom flops 1536960 :  6719040\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 0., 1.]])\n",
      "fvcore 8256000  vs custom flops 4384960 :  3871040\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 0., 1.]])\n",
      "fvcore 8256000  vs custom flops 5808960 :  2447040\n",
      "========================================\n",
      "module :  blocks.7.mobile_inverted_conv\n",
      "fvcore vs custom flops : 3468000, 3468000.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 20 / 20\n",
      "stride : 2\n",
      "inverted conv input shape : torch.Size([120, 40, 1, 1])\n",
      "depth conv weight shape : torch.Size([120, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([80, 120, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 640000., 1920000.])\n",
      "dw convert flops :,  tensor([    0., 75000., 84720.])\n",
      "dw blocks flops : tensor([588000., 300000., 108000.])\n",
      "pw blocks flops : tensor([320000., 960000.])\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.7.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 1920000})\n",
      "blocks.7.mobile_inverted_conv.depth_conv.conv Counter({'conv': 588000})\n",
      "blocks.7.mobile_inverted_conv.point_linear.conv Counter({'conv': 960000})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 640000., 1920000.]) dw conv : tensor([588000., 300000., 108000.]) pw conv : tensor([320000., 960000.])\n",
      "expand ratio list :  tensor([1, 3])\n",
      "gumbel idx :  tensor([[1., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 1., 0., 0.]])\n",
      "fvcore 3468000  vs custom flops 1156000 :  2312000\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 1., 0., 0.]])\n",
      "fvcore 3468000  vs custom flops 3468000 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1., 0.]])\n",
      "fvcore 3468000  vs custom flops 1135000 :  2333000\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1., 0.]])\n",
      "fvcore 3468000  vs custom flops 3255000 :  213000\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 1.]])\n",
      "fvcore 3468000  vs custom flops 1080720 :  2387280\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 1.]])\n",
      "fvcore 3468000  vs custom flops 3072720 :  395280\n",
      "========================================\n",
      "module :  blocks.8.mobile_inverted_conv\n",
      "fvcore vs custom flops : 4056000, 4056000.0 tensor(0.)\n",
      "========================================\n",
      "module :  blocks.9.mobile_inverted_conv\n",
      "fvcore vs custom flops : 5016000, 5016000.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 10 / 10\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([240, 80, 1, 1])\n",
      "depth conv weight shape : torch.Size([240, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([80, 240, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 640000., 1920000.])\n",
      "dw convert flops :,  tensor([     0., 150000., 169440.])\n",
      "dw blocks flops : tensor([1176000.,  600000.,  216000.])\n",
      "pw blocks flops : tensor([ 640000., 1920000.])\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.9.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 1920000})\n",
      "blocks.9.mobile_inverted_conv.depth_conv.conv Counter({'conv': 1176000})\n",
      "blocks.9.mobile_inverted_conv.point_linear.conv Counter({'conv': 1920000})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 640000., 1920000.]) dw conv : tensor([1176000.,  600000.,  216000.]) pw conv : tensor([ 640000., 1920000.])\n",
      "expand ratio list :  tensor([1, 3])\n",
      "gumbel idx :  tensor([[1., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 1., 0., 0.]])\n",
      "fvcore 5016000  vs custom flops 1672000 :  3344000\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 1., 0., 0.]])\n",
      "fvcore 5016000  vs custom flops 5016000 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1., 0.]])\n",
      "fvcore 5016000  vs custom flops 1630000 :  3386000\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1., 0.]])\n",
      "fvcore 5016000  vs custom flops 4590000 :  426000\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 1.]])\n",
      "fvcore 5016000  vs custom flops 1521440 :  3494560\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 1.]])\n",
      "fvcore 5016000  vs custom flops 4225440 :  790560\n",
      "========================================\n",
      "module :  blocks.10.mobile_inverted_conv\n",
      "fvcore vs custom flops : 5920000, 5920000.0 tensor(0.)\n",
      "========================================\n",
      "module :  blocks.11.mobile_inverted_conv\n",
      "fvcore vs custom flops : 6249600, 6249600.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 10 / 10\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([288, 96, 1, 1])\n",
      "depth conv weight shape : torch.Size([288, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([96, 288, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800.])\n",
      "dw convert flops :,  tensor([    0., 23328.])\n",
      "dw blocks flops : tensor([720000., 259200.])\n",
      "pw blocks flops : tensor([ 921600., 2764800.])\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.11.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2764800})\n",
      "blocks.11.mobile_inverted_conv.depth_conv.conv Counter({'conv': 720000})\n",
      "blocks.11.mobile_inverted_conv.point_linear.conv Counter({'conv': 2764800})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 921600., 2764800.]) dw conv : tensor([720000., 259200.]) pw conv : tensor([ 921600., 2764800.])\n",
      "expand ratio list :  tensor([1, 3])\n",
      "gumbel idx :  tensor([[1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 1., 0.]])\n",
      "fvcore 6249600  vs custom flops 2083200 :  4166400\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 1., 0.]])\n",
      "fvcore 6249600  vs custom flops 6249600 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1.]])\n",
      "fvcore 6249600  vs custom flops 1952928 :  4296672\n",
      "gumbel idx :  tensor([[0., 1., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1.]])\n",
      "fvcore 6249600  vs custom flops 5812128 :  437472\n",
      "========================================\n",
      "module :  blocks.12.mobile_inverted_conv\n",
      "fvcore vs custom flops : 6249600, 6249600.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 10 / 10\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([288, 96, 1, 1])\n",
      "depth conv weight shape : torch.Size([288, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([96, 288, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800.])\n",
      "dw convert flops :,  tensor([    0., 23328.])\n",
      "dw blocks flops : tensor([720000., 259200.])\n",
      "pw blocks flops : tensor([ 921600., 2764800.])\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.12.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2764800})\n",
      "blocks.12.mobile_inverted_conv.depth_conv.conv Counter({'conv': 720000})\n",
      "blocks.12.mobile_inverted_conv.point_linear.conv Counter({'conv': 2764800})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 921600., 2764800.]) dw conv : tensor([720000., 259200.]) pw conv : tensor([ 921600., 2764800.])\n",
      "expand ratio list :  tensor([1, 3])\n",
      "gumbel idx :  tensor([[1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 1., 0.]])\n",
      "fvcore 6249600  vs custom flops 2083200 :  4166400\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 1., 0.]])\n",
      "fvcore 6249600  vs custom flops 6249600 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1.]])\n",
      "fvcore 6249600  vs custom flops 1952928 :  4296672\n",
      "gumbel idx :  tensor([[0., 1., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1.]])\n",
      "fvcore 6249600  vs custom flops 5812128 :  437472\n",
      "========================================\n",
      "module :  blocks.13.mobile_inverted_conv\n",
      "fvcore vs custom flops : 6000000, 6000000.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (3), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 10 / 10\n",
      "stride : 2\n",
      "inverted conv input shape : torch.Size([384, 96, 1, 1])\n",
      "depth conv weight shape : torch.Size([384, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([192, 384, 1, 1])\n",
      "max expand size : 4\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800., 3686400.])\n",
      "dw convert flops :,  tensor([     0., 240000., 271104.])\n",
      "dw blocks flops : tensor([470400., 240000.,  86400.])\n",
      "pw blocks flops : tensor([ 460800., 1382400., 1843200.])\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.13.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 3686400})\n",
      "blocks.13.mobile_inverted_conv.depth_conv.conv Counter({'conv': 470400})\n",
      "blocks.13.mobile_inverted_conv.point_linear.conv Counter({'conv': 1843200})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 921600., 2764800., 3686400.]) dw conv : tensor([470400., 240000.,  86400.]) pw conv : tensor([ 460800., 1382400., 1843200.])\n",
      "expand ratio list :  tensor([1, 3, 4])\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1., 0., 0.]])\n",
      "fvcore 6000000  vs custom flops 1500000 :  4500000\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1., 0., 0.]])\n",
      "fvcore 6000000  vs custom flops 4500000 :  1500000\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 1., 0., 0.]])\n",
      "fvcore 6000000  vs custom flops 6000000 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 1., 0.]])\n",
      "fvcore 6000000  vs custom flops 1682400 :  4317600\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 1., 0.]])\n",
      "fvcore 6000000  vs custom flops 4567200 :  1432800\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 1., 0.]])\n",
      "fvcore 6000000  vs custom flops 6009600 :  -9600\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 0., 1.]])\n",
      "fvcore 6000000  vs custom flops 1675104 :  4324896\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 0., 1.]])\n",
      "fvcore 6000000  vs custom flops 4483104 :  1516896\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 0., 1.]])\n",
      "fvcore 6000000  vs custom flops 5887104 :  112896\n",
      "========================================\n",
      "module :  blocks.14.mobile_inverted_conv\n",
      "fvcore vs custom flops : 6235200, 6235200.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 5 / 5\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([576, 192, 1, 1])\n",
      "depth conv weight shape : torch.Size([576, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([192, 576, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800.])\n",
      "dw convert flops :,  tensor([     0., 360000., 406656.])\n",
      "dw blocks flops : tensor([705600., 360000., 129600.])\n",
      "pw blocks flops : tensor([ 921600., 2764800.])\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.14.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2764800})\n",
      "blocks.14.mobile_inverted_conv.depth_conv.conv Counter({'conv': 705600})\n",
      "blocks.14.mobile_inverted_conv.point_linear.conv Counter({'conv': 2764800})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 921600., 2764800.]) dw conv : tensor([705600., 360000., 129600.]) pw conv : tensor([ 921600., 2764800.])\n",
      "expand ratio list :  tensor([1, 3])\n",
      "gumbel idx :  tensor([[1., 0., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 1., 0., 0.]])\n",
      "fvcore 6235200  vs custom flops 2078400 :  4156800\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 1., 0., 0.]])\n",
      "fvcore 6235200  vs custom flops 6235200 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1., 0.]])\n",
      "fvcore 6235200  vs custom flops 2323200 :  3912000\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1., 0.]])\n",
      "fvcore 6235200  vs custom flops 6249600 :  -14400\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 1.]])\n",
      "fvcore 6235200  vs custom flops 2293056 :  3942144\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([7, 5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 1.]])\n",
      "fvcore 6235200  vs custom flops 6065856 :  169344\n",
      "========================================\n",
      "module :  blocks.15.mobile_inverted_conv\n",
      "fvcore vs custom flops : 5889600, 5889600.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 5 / 5\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([576, 192, 1, 1])\n",
      "depth conv weight shape : torch.Size([576, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([192, 576, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800.])\n",
      "dw convert flops :,  tensor([    0., 46656.])\n",
      "dw blocks flops : tensor([360000., 129600.])\n",
      "pw blocks flops : tensor([ 921600., 2764800.])\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.15.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2764800})\n",
      "blocks.15.mobile_inverted_conv.depth_conv.conv Counter({'conv': 360000})\n",
      "blocks.15.mobile_inverted_conv.point_linear.conv Counter({'conv': 2764800})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 921600., 2764800.]) dw conv : tensor([360000., 129600.]) pw conv : tensor([ 921600., 2764800.])\n",
      "expand ratio list :  tensor([1, 3])\n",
      "gumbel idx :  tensor([[1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 1., 0.]])\n",
      "fvcore 5889600  vs custom flops 1963200 :  3926400\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 1., 0.]])\n",
      "fvcore 5889600  vs custom flops 5889600 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([0.3333])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1.]])\n",
      "fvcore 5889600  vs custom flops 1933056 :  3956544\n",
      "gumbel idx :  tensor([[0., 1., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1.]])\n",
      "fvcore 5889600  vs custom flops 5705856 :  183744\n",
      "========================================\n",
      "module :  blocks.16.mobile_inverted_conv\n",
      "fvcore vs custom flops : 10310400, 10310400.0 tensor(0.)\n",
      "max gumbel test\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (3), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 5 / 5\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([768, 192, 1, 1])\n",
      "depth conv weight shape : torch.Size([768, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([320, 768, 1, 1])\n",
      "max expand size : 4\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800., 3686400.])\n",
      "dw convert flops :,  tensor([    0., 62208.])\n",
      "dw blocks flops : tensor([480000., 172800.])\n",
      "pw blocks flops : tensor([1536000., 4608000., 6144000.])\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "fvcore vs custom flops :  0\n",
      "blocks.16.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 3686400})\n",
      "blocks.16.mobile_inverted_conv.depth_conv.conv Counter({'conv': 480000})\n",
      "blocks.16.mobile_inverted_conv.point_linear.conv Counter({'conv': 6144000})\n",
      "FLOPS\n",
      "inverted conv : tensor([ 921600., 2764800., 3686400.]) dw conv : tensor([480000., 172800.]) pw conv : tensor([1536000., 4608000., 6144000.])\n",
      "expand ratio list :  tensor([1, 3, 4])\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 1., 0.]])\n",
      "fvcore 10310400  vs custom flops 2577600 :  7732800\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 1., 0.]])\n",
      "fvcore 10310400  vs custom flops 7732800 :  2577600\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[1., 0.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 1., 0.]])\n",
      "fvcore 10310400  vs custom flops 10310400 :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[1., 0., 0.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([0.2500])\n",
      "gumbel indices :  tensor([[1., 0., 0., 0., 1.]])\n",
      "fvcore 10310400  vs custom flops 2563008 :  7747392\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 1., 0.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([0.7500])\n",
      "gumbel indices :  tensor([[0., 1., 0., 0., 1.]])\n",
      "fvcore 10310400  vs custom flops 7564608 :  2745792\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 1.]])\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4]), kernel_size_list: tensor([5, 3])\n",
      "gumbel expand : tensor([[0., 0., 1.]]), gumbel kernel : tensor([[0., 1.]])\n",
      "expand kernel flops :  tensor([1.])\n",
      "gumbel indices :  tensor([[0., 0., 1., 0., 1.]])\n",
      "fvcore 10310400  vs custom flops 10065408 :  244992\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for k, v in flops._analyze().counts.items():\n",
    "    if k.endswith('mobile_inverted_conv'):\n",
    "        if has_deep_attr(gumbel_model, k):\n",
    "            m = get_deep_attr(gumbel_model, k)\n",
    "            if isinstance(m, MBGumbelInvertedConvLayer):\n",
    "                print(\"module : \", k)\n",
    "                full_flops = m.compute_flops()\n",
    "                print(f\"fvcore vs custom flops : {v['conv']}, {full_flops}\", v['conv']- full_flops)\n",
    "                kernel_size_list, expand_ratio_list = m.kernel_size_list, m.expand_ratio_list\n",
    "                if len(kernel_size_list) > 1 and len(expand_ratio_list) > 1:\n",
    "                    print(\"max gumbel test\")\n",
    "                    mm = m\n",
    "                    m.initialize_flops = False\n",
    "                    gumbel = torch.zeros(len(expand_ratio_list)+len(kernel_size_list))\n",
    "                    gumbel[len(expand_ratio_list)-1]=1\n",
    "                    gumbel[len(expand_ratio_list)]=1\n",
    "                    gumbel = gumbel.unsqueeze(0).to(m.kernel_size_list.device)\n",
    "                    gumbel_flops = m.compute_gumbel_flops(gumbel)\n",
    "                    print(\"fvcore vs custom flops : \", int(v['conv']- gumbel_flops))                    \n",
    "                    for kk, vv in flops._analyze().counts.items():\n",
    "                        if k in kk and '.conv' in kk:\n",
    "                            print(kk, vv)\n",
    "                    print(f\"FLOPS\\ninverted conv : {m.inverted_flops} dw conv : {m.dw_flops} pw conv : {m.pw_flops}\")\n",
    "                    print(\"expand ratio list : \", expand_ratio_list)\n",
    "                    \n",
    "\n",
    "                    for i in range(len(kernel_size_list)):\n",
    "                        for j in range(len(expand_ratio_list)):\n",
    "                            gumbel = torch.zeros(len(expand_ratio_list)+len(kernel_size_list))\n",
    "                            gumbel[j]=1\n",
    "                            gumbel[len(expand_ratio_list)+i]=1\n",
    "                            gumbel = gumbel.unsqueeze(0).to(m.kernel_size_list.device)\n",
    "                            gumbel_flops = m.compute_gumbel_flops(gumbel)\n",
    "                            print(\"gumbel indices : \", gumbel)\n",
    "                            print(f\"fvcore {int(v['conv'])}  vs custom flops {int(gumbel_flops)} : \",  int(v['conv']- gumbel_flops))                    \n",
    "                                                    \n",
    "                print(\"==\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::hardtanh_ encountered 1 time(s)\n",
      "Unsupported operator aten::hardtanh_ encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn, shortcut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.kernel_transform_linear_list.1, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "Unsupported operator aten::add encountered 1 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn, shortcut\n",
      "Unsupported operator aten::hardtanh_ encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "mobile_inverted_conv.depth_conv.bn, mobile_inverted_conv.inverted_bottleneck.bn, mobile_inverted_conv.kernel_transform_linear_list.0, mobile_inverted_conv.point_linear.bn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Log Static & Dynamic Flops : 22964224, 104383200\n"
     ]
    }
   ],
   "source": [
    "gumbel_model.set_static_flops(torch.randn(1,3,160,160).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 5,  6,  7],\n",
       "        [10, 11, 12],\n",
       "        [15, 16, 17]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g =  torch.arange(20).reshape(4,5)\n",
    "\n",
    "g[:, 0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fvcore vs custom flops : 10944000, 10944000.0 0.0\n",
      "========================================\n",
      "fvcore vs custom flops : 11212800, 11212800.0 0.0\n",
      "max gumbel test\n",
      "blocks.3.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 3686400})\n",
      "blocks.3.mobile_inverted_conv.depth_conv.conv Counter({'conv': 3840000})\n",
      "blocks.3.mobile_inverted_conv.point_linear.conv Counter({'conv': 3686400})\n",
      "FLOPS\n",
      "inverted conv : 3686400.0 \n",
      "dw conv : 3840000.0 \n",
      "pw conv : 3686400.0\n",
      "expand ratio list :  tensor([1, 3, 4], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (3), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 40 / 40\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([96, 24, 1, 1])\n",
      "depth conv weight shape : torch.Size([96, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([24, 96, 1, 1])\n",
      "max expand size : 4\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800., 3686400.], device='cuda:0')\n",
      "dw convert flops :,  tensor([   0., 7776.], device='cuda:0')\n",
      "dw blocks flops : tensor([3840000., 1382400.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 921600., 2764800., 3686400.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.2500], device='cuda:0')\n",
      "full flops vs custom flops : 11212800.0, 2803200, 8409600\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.7500], device='cuda:0')\n",
      "full flops vs custom flops : 11212800.0, 8409600, 2803200\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 11212800.0, 11212800, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 8880000, 8880000.0 0.0\n",
      "max gumbel test\n",
      "blocks.4.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 4608000})\n",
      "blocks.4.mobile_inverted_conv.depth_conv.conv Counter({'conv': 2352000})\n",
      "blocks.4.mobile_inverted_conv.point_linear.conv Counter({'conv': 1920000})\n",
      "FLOPS\n",
      "inverted conv : 4608000.0 \n",
      "dw conv : 2352000.0 \n",
      "pw conv : 1920000.0\n",
      "expand ratio list :  tensor([1, 3, 4, 5], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (4), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 40 / 40\n",
      "stride : 2\n",
      "inverted conv input shape : torch.Size([120, 24, 1, 1])\n",
      "depth conv weight shape : torch.Size([120, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([40, 120, 1, 1])\n",
      "max expand size : 5\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800., 3686400., 4608000.], device='cuda:0')\n",
      "dw convert flops :,  tensor([    0., 75000., 84720.], device='cuda:0')\n",
      "dw blocks flops : tensor([2352000., 1200000.,  432000.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 384000., 1152000., 1536000., 1920000.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 0., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0., 0., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.2000], device='cuda:0')\n",
      "full flops vs custom flops : 8880000.0, 1776000, 7104000\n",
      "gumbel idx :  tensor([[0., 1., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1., 0., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.6000], device='cuda:0')\n",
      "full flops vs custom flops : 8880000.0, 5328000, 3552000\n",
      "gumbel idx :  tensor([[0., 0., 1., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.8000], device='cuda:0')\n",
      "full flops vs custom flops : 8880000.0, 7104000, 1776000\n",
      "gumbel idx :  tensor([[0., 0., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4, 5], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 8880000.0, 8880000, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 5696000, 5696000.0 0.0\n",
      "========================================\n",
      "fvcore vs custom flops : 8256000, 8256000.0 0.0\n",
      "max gumbel test\n",
      "blocks.6.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2560000})\n",
      "blocks.6.mobile_inverted_conv.depth_conv.conv Counter({'conv': 3136000})\n",
      "blocks.6.mobile_inverted_conv.point_linear.conv Counter({'conv': 2560000})\n",
      "FLOPS\n",
      "inverted conv : 2560000.0 \n",
      "dw conv : 3136000.0 \n",
      "pw conv : 2560000.0\n",
      "expand ratio list :  tensor([1, 3, 4], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (3), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 20 / 20\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([160, 40, 1, 1])\n",
      "depth conv weight shape : torch.Size([160, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([40, 160, 1, 1])\n",
      "max expand size : 4\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 640000., 1920000., 2560000.], device='cuda:0')\n",
      "dw convert flops :,  tensor([     0., 100000., 112960.], device='cuda:0')\n",
      "dw blocks flops : tensor([3136000., 1600000.,  576000.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 640000., 1920000., 2560000.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.2500], device='cuda:0')\n",
      "full flops vs custom flops : 8256000.0, 2064000, 6192000\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.7500], device='cuda:0')\n",
      "full flops vs custom flops : 8256000.0, 6192000, 2064000\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 8256000.0, 8256000, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 3468000, 3468000.0 0.0\n",
      "max gumbel test\n",
      "blocks.7.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 1920000})\n",
      "blocks.7.mobile_inverted_conv.depth_conv.conv Counter({'conv': 588000})\n",
      "blocks.7.mobile_inverted_conv.point_linear.conv Counter({'conv': 960000})\n",
      "FLOPS\n",
      "inverted conv : 1920000.0 \n",
      "dw conv : 588000.0 \n",
      "pw conv : 960000.0\n",
      "expand ratio list :  tensor([1, 3], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 20 / 20\n",
      "stride : 2\n",
      "inverted conv input shape : torch.Size([120, 40, 1, 1])\n",
      "depth conv weight shape : torch.Size([120, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([80, 120, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 640000., 1920000.], device='cuda:0')\n",
      "dw convert flops :,  tensor([    0., 75000., 84720.], device='cuda:0')\n",
      "dw blocks flops : tensor([588000., 300000., 108000.], device='cuda:0')\n",
      "pw blocks flops : tensor([320000., 960000.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.3333], device='cuda:0')\n",
      "full flops vs custom flops : 3468000.0, 1156000, 2312000\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 3468000.0, 3468000, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 4056000, 4056000.0 0.0\n",
      "========================================\n",
      "fvcore vs custom flops : 5016000, 5016000.0 0.0\n",
      "max gumbel test\n",
      "blocks.9.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 1920000})\n",
      "blocks.9.mobile_inverted_conv.depth_conv.conv Counter({'conv': 1176000})\n",
      "blocks.9.mobile_inverted_conv.point_linear.conv Counter({'conv': 1920000})\n",
      "FLOPS\n",
      "inverted conv : 1920000.0 \n",
      "dw conv : 1176000.0 \n",
      "pw conv : 1920000.0\n",
      "expand ratio list :  tensor([1, 3], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 10 / 10\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([240, 80, 1, 1])\n",
      "depth conv weight shape : torch.Size([240, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([80, 240, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 640000., 1920000.], device='cuda:0')\n",
      "dw convert flops :,  tensor([     0., 150000., 169440.], device='cuda:0')\n",
      "dw blocks flops : tensor([1176000.,  600000.,  216000.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 640000., 1920000.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.3333], device='cuda:0')\n",
      "full flops vs custom flops : 5016000.0, 1672000, 3344000\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 5016000.0, 5016000, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 5920000, 5920000.0 0.0\n",
      "========================================\n",
      "fvcore vs custom flops : 6249600, 6249600.0 0.0\n",
      "max gumbel test\n",
      "blocks.11.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2764800})\n",
      "blocks.11.mobile_inverted_conv.depth_conv.conv Counter({'conv': 720000})\n",
      "blocks.11.mobile_inverted_conv.point_linear.conv Counter({'conv': 2764800})\n",
      "FLOPS\n",
      "inverted conv : 2764800.0 \n",
      "dw conv : 720000.0 \n",
      "pw conv : 2764800.0\n",
      "expand ratio list :  tensor([1, 3], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 10 / 10\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([288, 96, 1, 1])\n",
      "depth conv weight shape : torch.Size([288, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([96, 288, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800.], device='cuda:0')\n",
      "dw convert flops :,  tensor([    0., 23328.], device='cuda:0')\n",
      "dw blocks flops : tensor([720000., 259200.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 921600., 2764800.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.3333], device='cuda:0')\n",
      "full flops vs custom flops : 6249600.0, 2083200, 4166400\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 6249600.0, 6249600, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 6249600, 6249600.0 0.0\n",
      "max gumbel test\n",
      "blocks.12.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2764800})\n",
      "blocks.12.mobile_inverted_conv.depth_conv.conv Counter({'conv': 720000})\n",
      "blocks.12.mobile_inverted_conv.point_linear.conv Counter({'conv': 2764800})\n",
      "FLOPS\n",
      "inverted conv : 2764800.0 \n",
      "dw conv : 720000.0 \n",
      "pw conv : 2764800.0\n",
      "expand ratio list :  tensor([1, 3], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 10 / 10\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([288, 96, 1, 1])\n",
      "depth conv weight shape : torch.Size([288, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([96, 288, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800.], device='cuda:0')\n",
      "dw convert flops :,  tensor([    0., 23328.], device='cuda:0')\n",
      "dw blocks flops : tensor([720000., 259200.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 921600., 2764800.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.3333], device='cuda:0')\n",
      "full flops vs custom flops : 6249600.0, 2083200, 4166400\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 6249600.0, 6249600, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 6000000, 6000000.0 0.0\n",
      "max gumbel test\n",
      "blocks.13.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 3686400})\n",
      "blocks.13.mobile_inverted_conv.depth_conv.conv Counter({'conv': 470400})\n",
      "blocks.13.mobile_inverted_conv.point_linear.conv Counter({'conv': 1843200})\n",
      "FLOPS\n",
      "inverted conv : 3686400.0 \n",
      "dw conv : 470400.0 \n",
      "pw conv : 1843200.0\n",
      "expand ratio list :  tensor([1, 3, 4], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (3), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 10 / 10\n",
      "stride : 2\n",
      "inverted conv input shape : torch.Size([384, 96, 1, 1])\n",
      "depth conv weight shape : torch.Size([384, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([192, 384, 1, 1])\n",
      "max expand size : 4\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800., 3686400.], device='cuda:0')\n",
      "dw convert flops :,  tensor([     0., 240000., 271104.], device='cuda:0')\n",
      "dw blocks flops : tensor([470400., 240000.,  86400.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 460800., 1382400., 1843200.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.2500], device='cuda:0')\n",
      "full flops vs custom flops : 6000000.0, 1500000, 4500000\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.7500], device='cuda:0')\n",
      "full flops vs custom flops : 6000000.0, 4500000, 1500000\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 6000000.0, 6000000, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 6235200, 6235200.0 0.0\n",
      "max gumbel test\n",
      "blocks.14.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2764800})\n",
      "blocks.14.mobile_inverted_conv.depth_conv.conv Counter({'conv': 705600})\n",
      "blocks.14.mobile_inverted_conv.point_linear.conv Counter({'conv': 2764800})\n",
      "FLOPS\n",
      "inverted conv : 2764800.0 \n",
      "dw conv : 705600.0 \n",
      "pw conv : 2764800.0\n",
      "expand ratio list :  tensor([1, 3], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (3)\n",
      "module config\n",
      "input h/w : 5 / 5\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([576, 192, 1, 1])\n",
      "depth conv weight shape : torch.Size([576, 1, 7, 7])\n",
      "point conv weight shape : torch.Size([192, 576, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 7\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800.], device='cuda:0')\n",
      "dw convert flops :,  tensor([     0., 360000., 406656.], device='cuda:0')\n",
      "dw blocks flops : tensor([705600., 360000., 129600.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 921600., 2764800.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.3333], device='cuda:0')\n",
      "full flops vs custom flops : 6235200.0, 2078400, 4156800\n",
      "gumbel idx :  tensor([[0., 1., 1., 0., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([7, 5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 6235200.0, 6235200, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 5889600, 5889600.0 0.0\n",
      "max gumbel test\n",
      "blocks.15.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 2764800})\n",
      "blocks.15.mobile_inverted_conv.depth_conv.conv Counter({'conv': 360000})\n",
      "blocks.15.mobile_inverted_conv.point_linear.conv Counter({'conv': 2764800})\n",
      "FLOPS\n",
      "inverted conv : 2764800.0 \n",
      "dw conv : 360000.0 \n",
      "pw conv : 2764800.0\n",
      "expand ratio list :  tensor([1, 3], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (2), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 5 / 5\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([576, 192, 1, 1])\n",
      "depth conv weight shape : torch.Size([576, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([192, 576, 1, 1])\n",
      "max expand size : 3\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800.], device='cuda:0')\n",
      "dw convert flops :,  tensor([    0., 46656.], device='cuda:0')\n",
      "dw blocks flops : tensor([360000., 129600.], device='cuda:0')\n",
      "pw blocks flops : tensor([ 921600., 2764800.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.3333], device='cuda:0')\n",
      "full flops vs custom flops : 5889600.0, 1963200, 3926400\n",
      "gumbel idx :  tensor([[0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 5889600.0, 5889600, 0\n",
      "========================================\n",
      "fvcore vs custom flops : 10310400, 10310400.0 0.0\n",
      "max gumbel test\n",
      "blocks.16.mobile_inverted_conv.inverted_bottleneck.conv Counter({'conv': 3686400})\n",
      "blocks.16.mobile_inverted_conv.depth_conv.conv Counter({'conv': 480000})\n",
      "blocks.16.mobile_inverted_conv.point_linear.conv Counter({'conv': 6144000})\n",
      "FLOPS\n",
      "inverted conv : 3686400.0 \n",
      "dw conv : 480000.0 \n",
      "pw conv : 6144000.0\n",
      "expand ratio list :  tensor([1, 3, 4], device='cuda:0')\n",
      "gumbel_max flop check\n",
      "initialize FLOPs table\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "expand ratio list > 1 (3), kernel size list > 1 (2)\n",
      "module config\n",
      "input h/w : 5 / 5\n",
      "stride : 1\n",
      "inverted conv input shape : torch.Size([768, 192, 1, 1])\n",
      "depth conv weight shape : torch.Size([768, 1, 5, 5])\n",
      "point conv weight shape : torch.Size([320, 768, 1, 1])\n",
      "max expand size : 4\n",
      "max kernel size : 5\n",
      "expand ratio check : True\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "inverted blocks flops : tensor([ 921600., 2764800., 3686400.], device='cuda:0')\n",
      "dw convert flops :,  tensor([    0., 62208.], device='cuda:0')\n",
      "dw blocks flops : tensor([480000., 172800.], device='cuda:0')\n",
      "pw blocks flops : tensor([1536000., 4608000., 6144000.], device='cuda:0')\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "fvcore vs gumbel_max flops :  0\n",
      "gumbel idx :  tensor([[1., 0., 0., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[1., 0., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.2500], device='cuda:0')\n",
      "full flops vs custom flops : 10310400.0, 2577600, 7732800\n",
      "gumbel idx :  tensor([[0., 1., 0., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 1., 0.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([0.7500], device='cuda:0')\n",
      "full flops vs custom flops : 10310400.0, 7732800, 2577600\n",
      "gumbel idx :  tensor([[0., 0., 1., 1., 0.]], device='cuda:0')\n",
      "==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*\n",
      "expand_ratio_list: tensor([1, 3, 4], device='cuda:0'), kernel_size_list: tensor([5, 3], device='cuda:0')\n",
      "gumbel expand : tensor([[0., 0., 1.]], device='cuda:0'), gumbel kernel : tensor([[1., 0.]], device='cuda:0')\n",
      "expand kernel flops :  tensor([1.], device='cuda:0')\n",
      "full flops vs custom flops : 10310400.0, 10310400, 0\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "total_flops = 0\n",
    "for k, v in flops._analyze().counts.items():\n",
    "    if k.endswith('mobile_inverted_conv'):\n",
    "        if has_deep_attr(gumbel_model, k):\n",
    "            m = get_deep_attr(gumbel_model, k)\n",
    "            if isinstance(m, MBGumbelInvertedConvLayer):\n",
    "                full_flops = m.compute_flops()\n",
    "                print(f\"fvcore vs custom flops : {v['conv']}, {full_flops}\", v['conv']- full_flops)\n",
    "                m.initialize_flops = False\n",
    "                kernel_size_list, expand_ratio_list = m.kernel_size_list, m.expand_ratio_list\n",
    "                total_flops += int(full_flops)\n",
    "                if len(kernel_size_list) > 1 and len(expand_ratio_list) > 1:\n",
    "                    print(\"max gumbel test\")\n",
    "                    for kk, vv in flops._analyze().counts.items():\n",
    "                        if k in kk and '.conv' in kk:\n",
    "                            print(kk, vv)\n",
    "                    print(f\"FLOPS\\ninverted conv : {m.inverted_flops} \\ndw conv : {m.dw_flops} \\npw conv : {m.pw_flops}\")\n",
    "                    print(\"expand ratio list : \", expand_ratio_list)\n",
    "                    \n",
    "                    print(\"gumbel_max flop check\")\n",
    "                    \n",
    "                    mm = m\n",
    "                    gumbel = torch.zeros(len(expand_ratio_list)+len(kernel_size_list))\n",
    "                    gumbel[len(expand_ratio_list)-1]=1\n",
    "                    gumbel[len(expand_ratio_list)]=1\n",
    "                    gumbel = gumbel.unsqueeze(0).to(m.kernel_size_list.device)\n",
    "                    gumbel_flops = m.compute_gumbel_flops(gumbel)\n",
    "                    \n",
    "                    print(\"fvcore vs gumbel_max flops : \", int(v['conv']- gumbel_flops))\n",
    "                    \n",
    "                    for i in range(len(expand_ratio_list)):    \n",
    "                        gumbel = torch.zeros(len(expand_ratio_list)+len(kernel_size_list))\n",
    "                        gumbel[len(expand_ratio_list)]=1\n",
    "                        gumbel[i]=1\n",
    "                        gumbel = gumbel.unsqueeze(0).to(m.kernel_size_list.device)\n",
    "                        gumbel_flops = m.compute_gumbel_flops(gumbel)\n",
    "                        print(f\"full flops vs custom flops : {full_flops}, {int(gumbel_flops)}, {int(full_flops - gumbel_flops)}\")\n",
    "\n",
    "                print(\"==\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104383200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359296 9437184\n",
      "2359296.0\n",
      "9437184.0\n"
     ]
    }
   ],
   "source": [
    "in_c = 16\n",
    "expand = 1\n",
    "inputs = torch.randn(1, in_c, 32, 32)\n",
    "conv1 = nn.Conv2d(in_c, in_c * expand, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "expand = 4\n",
    "conv2 = nn.Conv2d(in_c, in_c * expand, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "flops1 = FlopCountAnalysis(conv1, inputs).total()\n",
    "flops2 = FlopCountAnalysis(conv2, inputs).total()\n",
    "\n",
    "print(flops1, flops2)\n",
    "\n",
    "print(count_conv_gumbel_flops(conv1.weight.shape, inputs.size()[2], inputs.size()[3]))\n",
    "print(count_conv_gumbel_flops(conv2.weight.shape, inputs.size()[2], inputs.size()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 110 time(s)\n",
      "Unsupported operator aten::hardtanh_ encountered 76 time(s)\n",
      "Unsupported operator aten::empty_like encountered 26 time(s)\n",
      "Unsupported operator aten::exponential_ encountered 26 time(s)\n",
      "Unsupported operator aten::log encountered 26 time(s)\n",
      "Unsupported operator aten::neg encountered 26 time(s)\n",
      "Unsupported operator aten::add encountered 96 time(s)\n",
      "Unsupported operator aten::div encountered 26 time(s)\n",
      "Unsupported operator aten::softmax encountered 26 time(s)\n",
      "Unsupported operator aten::scatter_ encountered 26 time(s)\n",
      "Unsupported operator aten::sub encountered 68 time(s)\n",
      "Unsupported operator aten::mul_ encountered 68 time(s)\n",
      "Unsupported operator aten::mul encountered 125 time(s)\n",
      "Unsupported operator aten::pad encountered 25 time(s)\n",
      "Unsupported operator aten::mean encountered 2 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "blocks.10.mobile_inverted_conv.inverted_bottleneck.conv, blocks.11.mobile_inverted_conv.depth_conv.conv, blocks.11.mobile_inverted_conv.inverted_bottleneck.conv, blocks.11.shortcut, blocks.12.mobile_inverted_conv.depth_conv.conv, blocks.12.mobile_inverted_conv.inverted_bottleneck.conv, blocks.12.shortcut, blocks.13.mobile_inverted_conv.depth_conv.conv, blocks.13.mobile_inverted_conv.inverted_bottleneck.conv, blocks.14.mobile_inverted_conv.depth_conv.conv, blocks.14.mobile_inverted_conv.inverted_bottleneck.conv, blocks.14.shortcut, blocks.15.mobile_inverted_conv.depth_conv.conv, blocks.15.mobile_inverted_conv.inverted_bottleneck.conv, blocks.15.shortcut, blocks.16.mobile_inverted_conv.depth_conv.conv, blocks.16.mobile_inverted_conv.inverted_bottleneck.conv, blocks.2.mobile_inverted_conv.inverted_bottleneck.conv, blocks.2.shortcut, blocks.3.mobile_inverted_conv.depth_conv.conv, blocks.3.mobile_inverted_conv.inverted_bottleneck.conv, blocks.3.shortcut, blocks.4.mobile_inverted_conv.depth_conv.conv, blocks.4.mobile_inverted_conv.inverted_bottleneck.conv, blocks.5.mobile_inverted_conv.inverted_bottleneck.conv, blocks.5.shortcut, blocks.6.mobile_inverted_conv.depth_conv.conv, blocks.6.mobile_inverted_conv.inverted_bottleneck.conv, blocks.6.shortcut, blocks.7.mobile_inverted_conv.depth_conv.conv, blocks.7.mobile_inverted_conv.inverted_bottleneck.conv, blocks.8.mobile_inverted_conv.inverted_bottleneck.conv, blocks.8.shortcut, blocks.9.mobile_inverted_conv.depth_conv.conv, blocks.9.mobile_inverted_conv.inverted_bottleneck.conv, blocks.9.shortcut\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'': Counter({'conv': 172696800,\n",
       "          'batch_norm': 22180000,\n",
       "          'adaptive_avg_pool2d': 38400,\n",
       "          'linear': 2023520}),\n",
       " 'first_conv': Counter({'conv': 5529600, 'batch_norm': 1024000}),\n",
       " 'first_conv.conv': Counter({'conv': 5529600}),\n",
       " 'first_conv.bn': Counter({'batch_norm': 1024000}),\n",
       " 'first_conv.act': Counter(),\n",
       " 'blocks': Counter({'conv': 167167200,\n",
       "          'batch_norm': 21156000,\n",
       "          'linear': 1292896}),\n",
       " 'blocks.0': Counter({'conv': 5120000, 'batch_norm': 1536000}),\n",
       " 'blocks.0.mobile_inverted_conv': Counter({'conv': 5120000,\n",
       "          'batch_norm': 1536000}),\n",
       " 'blocks.0.mobile_inverted_conv.depth_conv': Counter({'conv': 1843200,\n",
       "          'batch_norm': 1024000}),\n",
       " 'blocks.0.mobile_inverted_conv.depth_conv.conv': Counter({'conv': 1843200}),\n",
       " 'blocks.0.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 1024000}),\n",
       " 'blocks.0.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.0.mobile_inverted_conv.point_linear': Counter({'conv': 3276800,\n",
       "          'batch_norm': 512000}),\n",
       " 'blocks.0.mobile_inverted_conv.point_linear.conv': Counter({'conv': 3276800}),\n",
       " 'blocks.0.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 512000}),\n",
       " 'blocks.1': Counter({'conv': 10521600, 'batch_norm': 2112000}),\n",
       " 'blocks.1.mobile_inverted_conv': Counter({'conv': 10521600,\n",
       "          'batch_norm': 2112000}),\n",
       " 'blocks.1.mobile_inverted_conv.inverted_bottleneck': Counter({'conv': 4915200,\n",
       "          'batch_norm': 1536000}),\n",
       " 'blocks.1.mobile_inverted_conv.inverted_bottleneck.conv': Counter({'conv': 4915200}),\n",
       " 'blocks.1.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 1536000}),\n",
       " 'blocks.1.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.1.mobile_inverted_conv.depth_conv': Counter({'conv': 3763200,\n",
       "          'batch_norm': 384000}),\n",
       " 'blocks.1.mobile_inverted_conv.depth_conv.conv': Counter({'conv': 3763200}),\n",
       " 'blocks.1.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 384000}),\n",
       " 'blocks.1.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.1.mobile_inverted_conv.point_linear': Counter({'conv': 1843200,\n",
       "          'batch_norm': 192000}),\n",
       " 'blocks.1.mobile_inverted_conv.point_linear.conv': Counter({'conv': 1843200}),\n",
       " 'blocks.1.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 192000}),\n",
       " 'blocks.2': Counter({'conv': 18316800, 'batch_norm': 3648000}),\n",
       " 'blocks.2.mobile_inverted_conv': Counter({'conv': 18316800,\n",
       "          'batch_norm': 3648000}),\n",
       " 'blocks.2.mobile_inverted_conv.kernel_transform_linear_list': Counter(),\n",
       " 'blocks.2.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 960000}),\n",
       " 'blocks.2.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.2.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 960000}),\n",
       " 'blocks.2.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.2.mobile_inverted_conv.depth_conv': Counter({'conv': 1728000,\n",
       "          'batch_norm': 960000}),\n",
       " 'blocks.2.mobile_inverted_conv.depth_conv.conv': Counter({'conv': 1728000}),\n",
       " 'blocks.2.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 960000}),\n",
       " 'blocks.2.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.2.mobile_inverted_conv.point_linear': Counter({'conv': 4608000,\n",
       "          'batch_norm': 192000}),\n",
       " 'blocks.2.mobile_inverted_conv.point_linear.conv': Counter({'conv': 4608000}),\n",
       " 'blocks.2.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 192000}),\n",
       " 'blocks.2.shortcut': Counter(),\n",
       " 'blocks.3': Counter({'conv': 16281600,\n",
       "          'batch_norm': 3264000,\n",
       "          'linear': 7776}),\n",
       " 'blocks.3.mobile_inverted_conv': Counter({'conv': 16281600,\n",
       "          'batch_norm': 3264000,\n",
       "          'linear': 7776}),\n",
       " 'blocks.3.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 7776}),\n",
       " 'blocks.3.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 7776}),\n",
       " 'blocks.3.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 768000}),\n",
       " 'blocks.3.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.3.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 768000}),\n",
       " 'blocks.3.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.3.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 1536000}),\n",
       " 'blocks.3.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.3.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 1536000}),\n",
       " 'blocks.3.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.3.mobile_inverted_conv.point_linear': Counter({'conv': 3686400,\n",
       "          'batch_norm': 192000}),\n",
       " 'blocks.3.mobile_inverted_conv.point_linear.conv': Counter({'conv': 3686400}),\n",
       " 'blocks.3.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 192000}),\n",
       " 'blocks.3.shortcut': Counter(),\n",
       " 'blocks.4': Counter({'conv': 17884800,\n",
       "          'batch_norm': 3296000,\n",
       "          'linear': 84720}),\n",
       " 'blocks.4.mobile_inverted_conv': Counter({'conv': 17884800,\n",
       "          'batch_norm': 3296000,\n",
       "          'linear': 84720}),\n",
       " 'blocks.4.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 84720}),\n",
       " 'blocks.4.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 75000}),\n",
       " 'blocks.4.mobile_inverted_conv.kernel_transform_linear_list.1': Counter({'linear': 9720}),\n",
       " 'blocks.4.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 960000}),\n",
       " 'blocks.4.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.4.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 960000}),\n",
       " 'blocks.4.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.4.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 720000}),\n",
       " 'blocks.4.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.4.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 720000}),\n",
       " 'blocks.4.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.4.mobile_inverted_conv.point_linear': Counter({'conv': 1920000,\n",
       "          'batch_norm': 80000}),\n",
       " 'blocks.4.mobile_inverted_conv.point_linear.conv': Counter({'conv': 1920000}),\n",
       " 'blocks.4.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 80000}),\n",
       " 'blocks.5': Counter({'conv': 8256000, 'batch_norm': 1040000}),\n",
       " 'blocks.5.mobile_inverted_conv': Counter({'conv': 8256000,\n",
       "          'batch_norm': 1040000}),\n",
       " 'blocks.5.mobile_inverted_conv.kernel_transform_linear_list': Counter(),\n",
       " 'blocks.5.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 320000}),\n",
       " 'blocks.5.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.5.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 320000}),\n",
       " 'blocks.5.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.5.mobile_inverted_conv.depth_conv': Counter({'conv': 576000,\n",
       "          'batch_norm': 320000}),\n",
       " 'blocks.5.mobile_inverted_conv.depth_conv.conv': Counter({'conv': 576000}),\n",
       " 'blocks.5.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 320000}),\n",
       " 'blocks.5.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.5.mobile_inverted_conv.point_linear': Counter({'conv': 2560000,\n",
       "          'batch_norm': 80000}),\n",
       " 'blocks.5.mobile_inverted_conv.point_linear.conv': Counter({'conv': 2560000}),\n",
       " 'blocks.5.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 80000}),\n",
       " 'blocks.5.shortcut': Counter(),\n",
       " 'blocks.6': Counter({'conv': 12992000,\n",
       "          'batch_norm': 1680000,\n",
       "          'linear': 112960}),\n",
       " 'blocks.6.mobile_inverted_conv': Counter({'conv': 12992000,\n",
       "          'batch_norm': 1680000,\n",
       "          'linear': 112960}),\n",
       " 'blocks.6.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 112960}),\n",
       " 'blocks.6.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 100000}),\n",
       " 'blocks.6.mobile_inverted_conv.kernel_transform_linear_list.1': Counter({'linear': 12960}),\n",
       " 'blocks.6.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 320000}),\n",
       " 'blocks.6.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.6.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 320000}),\n",
       " 'blocks.6.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.6.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 960000}),\n",
       " 'blocks.6.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.6.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 960000}),\n",
       " 'blocks.6.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.6.mobile_inverted_conv.point_linear': Counter({'conv': 2560000,\n",
       "          'batch_norm': 80000}),\n",
       " 'blocks.6.mobile_inverted_conv.point_linear.conv': Counter({'conv': 2560000}),\n",
       " 'blocks.6.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 80000}),\n",
       " 'blocks.6.shortcut': Counter(),\n",
       " 'blocks.7': Counter({'conv': 4516000, 'batch_norm': 540000, 'linear': 84720}),\n",
       " 'blocks.7.mobile_inverted_conv': Counter({'conv': 4516000,\n",
       "          'batch_norm': 540000,\n",
       "          'linear': 84720}),\n",
       " 'blocks.7.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 84720}),\n",
       " 'blocks.7.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 75000}),\n",
       " 'blocks.7.mobile_inverted_conv.kernel_transform_linear_list.1': Counter({'linear': 9720}),\n",
       " 'blocks.7.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 240000}),\n",
       " 'blocks.7.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.7.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 240000}),\n",
       " 'blocks.7.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.7.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 180000}),\n",
       " 'blocks.7.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.7.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 180000}),\n",
       " 'blocks.7.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.7.mobile_inverted_conv.point_linear': Counter({'conv': 960000,\n",
       "          'batch_norm': 40000}),\n",
       " 'blocks.7.mobile_inverted_conv.point_linear.conv': Counter({'conv': 960000}),\n",
       " 'blocks.7.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 40000}),\n",
       " 'blocks.8': Counter({'conv': 4696000, 'batch_norm': 320000}),\n",
       " 'blocks.8.mobile_inverted_conv': Counter({'conv': 4696000,\n",
       "          'batch_norm': 320000}),\n",
       " 'blocks.8.mobile_inverted_conv.kernel_transform_linear_list': Counter(),\n",
       " 'blocks.8.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 120000}),\n",
       " 'blocks.8.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.8.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 120000}),\n",
       " 'blocks.8.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.8.mobile_inverted_conv.depth_conv': Counter({'conv': 216000,\n",
       "          'batch_norm': 120000}),\n",
       " 'blocks.8.mobile_inverted_conv.depth_conv.conv': Counter({'conv': 216000}),\n",
       " 'blocks.8.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 120000}),\n",
       " 'blocks.8.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.8.mobile_inverted_conv.point_linear': Counter({'conv': 1920000,\n",
       "          'batch_norm': 40000}),\n",
       " 'blocks.8.mobile_inverted_conv.point_linear.conv': Counter({'conv': 1920000}),\n",
       " 'blocks.8.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 40000}),\n",
       " 'blocks.8.shortcut': Counter(),\n",
       " 'blocks.9': Counter({'conv': 6472000,\n",
       "          'batch_norm': 560000,\n",
       "          'linear': 169440}),\n",
       " 'blocks.9.mobile_inverted_conv': Counter({'conv': 6472000,\n",
       "          'batch_norm': 560000,\n",
       "          'linear': 169440}),\n",
       " 'blocks.9.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 169440}),\n",
       " 'blocks.9.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 150000}),\n",
       " 'blocks.9.mobile_inverted_conv.kernel_transform_linear_list.1': Counter({'linear': 19440}),\n",
       " 'blocks.9.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 120000}),\n",
       " 'blocks.9.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.9.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 120000}),\n",
       " 'blocks.9.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.9.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 360000}),\n",
       " 'blocks.9.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.9.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 360000}),\n",
       " 'blocks.9.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.9.mobile_inverted_conv.point_linear': Counter({'conv': 1920000,\n",
       "          'batch_norm': 40000}),\n",
       " 'blocks.9.mobile_inverted_conv.point_linear.conv': Counter({'conv': 1920000}),\n",
       " 'blocks.9.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 40000}),\n",
       " 'blocks.9.shortcut': Counter(),\n",
       " 'blocks.10': Counter({'conv': 8480000, 'batch_norm': 528000}),\n",
       " 'blocks.10.mobile_inverted_conv': Counter({'conv': 8480000,\n",
       "          'batch_norm': 528000}),\n",
       " 'blocks.10.mobile_inverted_conv.kernel_transform_linear_list': Counter(),\n",
       " 'blocks.10.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 160000}),\n",
       " 'blocks.10.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.10.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 160000}),\n",
       " 'blocks.10.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.10.mobile_inverted_conv.depth_conv': Counter({'conv': 288000,\n",
       "          'batch_norm': 160000}),\n",
       " 'blocks.10.mobile_inverted_conv.depth_conv.conv': Counter({'conv': 288000}),\n",
       " 'blocks.10.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 160000}),\n",
       " 'blocks.10.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.10.mobile_inverted_conv.point_linear': Counter({'conv': 3072000,\n",
       "          'batch_norm': 48000}),\n",
       " 'blocks.10.mobile_inverted_conv.point_linear.conv': Counter({'conv': 3072000}),\n",
       " 'blocks.10.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 48000}),\n",
       " 'blocks.11': Counter({'conv': 7430400,\n",
       "          'batch_norm': 528000,\n",
       "          'linear': 23328}),\n",
       " 'blocks.11.mobile_inverted_conv': Counter({'conv': 7430400,\n",
       "          'batch_norm': 528000,\n",
       "          'linear': 23328}),\n",
       " 'blocks.11.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 23328}),\n",
       " 'blocks.11.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 23328}),\n",
       " 'blocks.11.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 144000}),\n",
       " 'blocks.11.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.11.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 144000}),\n",
       " 'blocks.11.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.11.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 288000}),\n",
       " 'blocks.11.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.11.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 288000}),\n",
       " 'blocks.11.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.11.mobile_inverted_conv.point_linear': Counter({'conv': 2764800,\n",
       "          'batch_norm': 48000}),\n",
       " 'blocks.11.mobile_inverted_conv.point_linear.conv': Counter({'conv': 2764800}),\n",
       " 'blocks.11.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 48000}),\n",
       " 'blocks.11.shortcut': Counter(),\n",
       " 'blocks.12': Counter({'conv': 7430400,\n",
       "          'batch_norm': 528000,\n",
       "          'linear': 23328}),\n",
       " 'blocks.12.mobile_inverted_conv': Counter({'conv': 7430400,\n",
       "          'batch_norm': 528000,\n",
       "          'linear': 23328}),\n",
       " 'blocks.12.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 23328}),\n",
       " 'blocks.12.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 23328}),\n",
       " 'blocks.12.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 144000}),\n",
       " 'blocks.12.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.12.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 144000}),\n",
       " 'blocks.12.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.12.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 288000}),\n",
       " 'blocks.12.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.12.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 288000}),\n",
       " 'blocks.12.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.12.mobile_inverted_conv.point_linear': Counter({'conv': 2764800,\n",
       "          'batch_norm': 48000}),\n",
       " 'blocks.12.mobile_inverted_conv.point_linear.conv': Counter({'conv': 2764800}),\n",
       " 'blocks.12.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 48000}),\n",
       " 'blocks.12.shortcut': Counter(),\n",
       " 'blocks.13': Counter({'conv': 10012800,\n",
       "          'batch_norm': 552000,\n",
       "          'linear': 271104}),\n",
       " 'blocks.13.mobile_inverted_conv': Counter({'conv': 10012800,\n",
       "          'batch_norm': 552000,\n",
       "          'linear': 271104}),\n",
       " 'blocks.13.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 271104}),\n",
       " 'blocks.13.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 240000}),\n",
       " 'blocks.13.mobile_inverted_conv.kernel_transform_linear_list.1': Counter({'linear': 31104}),\n",
       " 'blocks.13.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 192000}),\n",
       " 'blocks.13.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.13.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 192000}),\n",
       " 'blocks.13.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.13.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 144000}),\n",
       " 'blocks.13.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.13.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 144000}),\n",
       " 'blocks.13.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.13.mobile_inverted_conv.point_linear': Counter({'conv': 1843200,\n",
       "          'batch_norm': 24000}),\n",
       " 'blocks.13.mobile_inverted_conv.point_linear.conv': Counter({'conv': 1843200}),\n",
       " 'blocks.13.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 24000}),\n",
       " 'blocks.14': Counter({'conv': 7646400,\n",
       "          'batch_norm': 336000,\n",
       "          'linear': 406656}),\n",
       " 'blocks.14.mobile_inverted_conv': Counter({'conv': 7646400,\n",
       "          'batch_norm': 336000,\n",
       "          'linear': 406656}),\n",
       " 'blocks.14.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 406656}),\n",
       " 'blocks.14.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 360000}),\n",
       " 'blocks.14.mobile_inverted_conv.kernel_transform_linear_list.1': Counter({'linear': 46656}),\n",
       " 'blocks.14.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 72000}),\n",
       " 'blocks.14.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.14.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 72000}),\n",
       " 'blocks.14.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.14.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 216000}),\n",
       " 'blocks.14.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.14.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 216000}),\n",
       " 'blocks.14.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.14.mobile_inverted_conv.point_linear': Counter({'conv': 2764800,\n",
       "          'batch_norm': 24000}),\n",
       " 'blocks.14.mobile_inverted_conv.point_linear.conv': Counter({'conv': 2764800}),\n",
       " 'blocks.14.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 24000}),\n",
       " 'blocks.14.shortcut': Counter(),\n",
       " 'blocks.15': Counter({'conv': 6940800,\n",
       "          'batch_norm': 264000,\n",
       "          'linear': 46656}),\n",
       " 'blocks.15.mobile_inverted_conv': Counter({'conv': 6940800,\n",
       "          'batch_norm': 264000,\n",
       "          'linear': 46656}),\n",
       " 'blocks.15.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 46656}),\n",
       " 'blocks.15.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 46656}),\n",
       " 'blocks.15.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 72000}),\n",
       " 'blocks.15.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.15.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 72000}),\n",
       " 'blocks.15.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.15.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 144000}),\n",
       " 'blocks.15.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.15.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 144000}),\n",
       " 'blocks.15.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.15.mobile_inverted_conv.point_linear': Counter({'conv': 2764800,\n",
       "          'batch_norm': 24000}),\n",
       " 'blocks.15.mobile_inverted_conv.point_linear.conv': Counter({'conv': 2764800}),\n",
       " 'blocks.15.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 24000}),\n",
       " 'blocks.15.shortcut': Counter(),\n",
       " 'blocks.16': Counter({'conv': 14169600,\n",
       "          'batch_norm': 424000,\n",
       "          'linear': 62208}),\n",
       " 'blocks.16.mobile_inverted_conv': Counter({'conv': 14169600,\n",
       "          'batch_norm': 424000,\n",
       "          'linear': 62208}),\n",
       " 'blocks.16.mobile_inverted_conv.kernel_transform_linear_list': Counter({'linear': 62208}),\n",
       " 'blocks.16.mobile_inverted_conv.kernel_transform_linear_list.0': Counter({'linear': 62208}),\n",
       " 'blocks.16.mobile_inverted_conv.inverted_bottleneck': Counter({'batch_norm': 96000}),\n",
       " 'blocks.16.mobile_inverted_conv.inverted_bottleneck.conv': Counter(),\n",
       " 'blocks.16.mobile_inverted_conv.inverted_bottleneck.bn': Counter({'batch_norm': 96000}),\n",
       " 'blocks.16.mobile_inverted_conv.inverted_bottleneck.act': Counter(),\n",
       " 'blocks.16.mobile_inverted_conv.depth_conv': Counter({'batch_norm': 192000}),\n",
       " 'blocks.16.mobile_inverted_conv.depth_conv.conv': Counter(),\n",
       " 'blocks.16.mobile_inverted_conv.depth_conv.bn': Counter({'batch_norm': 192000}),\n",
       " 'blocks.16.mobile_inverted_conv.depth_conv.act': Counter(),\n",
       " 'blocks.16.mobile_inverted_conv.point_linear': Counter({'conv': 6144000,\n",
       "          'batch_norm': 40000}),\n",
       " 'blocks.16.mobile_inverted_conv.point_linear.conv': Counter({'conv': 6144000}),\n",
       " 'blocks.16.mobile_inverted_conv.point_linear.bn': Counter({'batch_norm': 40000}),\n",
       " 'classifier': Counter({'linear': 320000}),\n",
       " 'classifier.linear': Counter({'linear': 320000}),\n",
       " 'avgpool_policy': Counter({'adaptive_avg_pool2d': 38400}),\n",
       " 'gumbel_features_flatten': Counter(),\n",
       " 'gumbel_fc1': Counter({'linear': 393216}),\n",
       " 'dropout': Counter(),\n",
       " 'gumbel_fc2': Counter({'linear': 17408}),\n",
       " 'gumbel_block': Counter()}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_model = GumbelMCUNet.build_from_config(ori_model.config, gubmel_config)\n",
    "#rm_bn_from_net(gumbel_model)\n",
    "gumbel_model_flops = FlopCountAnalysis(gumbel_model, torch.randn(1, 3, 160, 160))\n",
    "gumbel_model_flops.by_module_and_operator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'MobileInvertedResidualBlock',\n",
       " 'mobile_inverted_conv': {'name': 'MBInvertedConvLayer',\n",
       "  'in_channels': 192,\n",
       "  'out_channels': 320,\n",
       "  'kernel_size': 5,\n",
       "  'stride': 1,\n",
       "  'expand_ratio': 4,\n",
       "  'mid_channels': 768,\n",
       "  'act_func': 'relu6',\n",
       "  'use_se': False},\n",
       " 'shortcut': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_model.blocks[16].config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileGumbelInvertedResidualBlock(\n",
       "  (mobile_inverted_conv): MBGumbelInvertedConvLayer(\n",
       "    (kernel_transform_linear_list): ModuleList(\n",
       "      (0): Linear(in_features=9, out_features=9, bias=True)\n",
       "    )\n",
       "    (inverted_bottleneck): Sequential(\n",
       "      (conv): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU6(inplace=True)\n",
       "    )\n",
       "    (depth_conv): Sequential(\n",
       "      (conv): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): ReLU6(inplace=True)\n",
       "    )\n",
       "    (point_linear): Sequential(\n",
       "      (conv): Conv2d(768, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel_model.blocks[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_conv\n",
      "ori model parameter : 864\n",
      "gumbel model parameter: 928\n",
      "blocks\n",
      "ori model parameter : 1389368\n",
      "gumbel model parameter: 1415602\n",
      "classifier\n",
      "ori model parameter : 321000\n",
      "gumbel model parameter: 321000\n"
     ]
    }
   ],
   "source": [
    "for n, m in ori_model.named_modules():\n",
    "    if hasattr(gumbel_model, n):\n",
    "        print(n)\n",
    "        print(\"ori model parameter :\", sum([p.numel() for p in m.parameters()]))\n",
    "        print(\"gumbel model parameter:\", sum([p.numel() for p in getattr(gumbel_model, n).parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1297, -0.0050,  0.0143,  ..., -0.0158, -0.1057,  0.0024],\n",
       "        [ 0.1476, -0.1173, -0.0844,  ..., -0.0407, -0.0013,  0.1079],\n",
       "        [ 0.0353, -0.0408,  0.0923,  ..., -0.0025,  0.0013, -0.0519],\n",
       "        ...,\n",
       "        [-0.0136,  0.0784, -0.0716,  ..., -0.1794, -0.0974,  0.0618],\n",
       "        [ 0.1510,  0.0757,  0.1213,  ...,  0.0973, -0.0941, -0.1362],\n",
       "        [ 0.0133,  0.0498,  0.0714,  ..., -0.2346,  0.0590,  0.0044]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 16, 32, 32] to have 3 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb 셀 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m gumbel_model\u001b[39m.\u001b[39;49mset_static_flops(inputs\u001b[39m.\u001b[39;49mcuda())\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m gumbel_model\u001b[39m.\u001b[39mcompute_flops(inputs, gumbel_list)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7064685f62715f70745f32323130222c2273657474696e6773223a7b22686f7374223a227373683a2f2f37385f706468227d7d/home/pdh/torch/NAS/TinyML_NAS/mcunet/test_gumbel_net.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m gumbel_model\u001b[39m.\u001b[39mcompute_flops(inputs, gumbel_list)\n",
      "File \u001b[0;32m~/torch/NAS/TinyML_NAS/mcunet/mcunet/gumbel_module/gumbel_net.py:132\u001b[0m, in \u001b[0;36mGumbelMCUNet.set_static_flops\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_static_flops\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    131\u001b[0m     flops \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 132\u001b[0m     flops \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m FlopCountAnalysis(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfirst_conv, x)\u001b[39m.\u001b[39;49mtotal()\n\u001b[1;32m    133\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirst_conv(x)\n\u001b[1;32m    134\u001b[0m     \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:248\u001b[0m, in \u001b[0;36mJitModelAnalysis.total\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtotal\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m    Returns the total aggregated statistic across all operators\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m    for the requested module.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39m        int : The aggregated statistic.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_analyze()\n\u001b[1;32m    249\u001b[0m     module_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanonical_module_name(module_name)\n\u001b[1;32m    250\u001b[0m     total_count \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(stats\u001b[39m.\u001b[39mcounts[module_name]\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:551\u001b[0m, in \u001b[0;36mJitModelAnalysis._analyze\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_trace \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mno_tracer_warning\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    550\u001b[0m         warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, category\u001b[39m=\u001b[39mTracerWarning)\n\u001b[0;32m--> 551\u001b[0m     graph \u001b[39m=\u001b[39m _get_scoped_trace_graph(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aliases)\n\u001b[1;32m    553\u001b[0m \u001b[39m# Assures even modules not in the trace graph are initialized to zero count\u001b[39;00m\n\u001b[1;32m    554\u001b[0m counts \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/fvcore/nn/jit_analysis.py:176\u001b[0m, in \u001b[0;36m_get_scoped_trace_graph\u001b[0;34m(module, inputs, aliases)\u001b[0m\n\u001b[1;32m    173\u001b[0m     name \u001b[39m=\u001b[39m aliases[mod]\n\u001b[1;32m    174\u001b[0m     register_hooks(mod, name)\n\u001b[0;32m--> 176\u001b[0m graph, _ \u001b[39m=\u001b[39m _get_trace_graph(module, inputs)\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m handle \u001b[39min\u001b[39;00m hook_handles:\n\u001b[1;32m    179\u001b[0m     handle\u001b[39m.\u001b[39mremove()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:1175\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1174\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1175\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1184\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1185\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1186\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    128\u001b[0m     wrapper,\n\u001b[1;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1203\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1201\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1203\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1204\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1205\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1173\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1174\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1175\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/torch/NAS/TinyML_NAS/mcunet/mcunet/tinynas/nn/modules/layers.py:100\u001b[0m, in \u001b[0;36mMy2DLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     98\u001b[0m     \u001b[39m# similar to nn.Sequential\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m--> 100\u001b[0m         x \u001b[39m=\u001b[39m module(x)\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1203\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1201\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1203\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1204\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1205\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1173\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1174\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1175\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 16, 32, 32] to have 3 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "gumbel_model.set_static_flops(inputs.cuda())\n",
    "gumbel_model.compute_flops(inputs, gumbel_list)\n",
    "gumbel_model.compute_flops(inputs, gumbel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel_model.compute_flops(inputs, gumbel_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3,4])* torch.tensor(block.mobile_inverted_conv.expand_ratio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3,4] * [2,6,8,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, block in enumerate(gumbel_model.blocks):\n",
    "    if isinstance(block, MobileGumbelInvertedResidualBlock):\n",
    "        print(block.shortcut, block.mobile_inverted_conv.depth_conv.conv.stride)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel_model.forward(inputs)\n",
    "\n",
    "gumbel_model.eval()\n",
    "print(gumbel_model.training)\n",
    "out_origin = gumbel_model.forward(inputs)\n",
    "for i in range(10):\n",
    "    print(out_origin - gumbel_model.forward(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_gumbel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.test_layer = nn.Linear(20, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gumbel_input = self.test_layer(x)\n",
    "        if self.training:\n",
    "            gumbel_out = F.gumbel_softmax(gumbel_input, tau=1, hard=True, eps=1e-10, dim=-1)\n",
    "        else:\n",
    "            index = gumbel_input.max(dim=-1, keepdim=True)[1]\n",
    "            gumbel_out = torch.zeros_like(gumbel_input, memory_format=torch.legacy_contiguous_format).scatter_(-1, index, 1.0)\n",
    "        \n",
    "        return gumbel_out\n",
    "inputs = torch.randn(5, 20)\n",
    "gumbel = test_gumbel()\n",
    "\n",
    "# train\n",
    "gumbel.train()\n",
    "for i in range(5):\n",
    "    out = gumbel(inputs)\n",
    "    print(f\"{i} iter -> \", out)\n",
    "\n",
    "# test\n",
    "gumbel.eval()\n",
    "for i in range(5):\n",
    "    out = gumbel(inputs)\n",
    "    print(f\"{i} iter -> \", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchprofile import profile_macs\n",
    "\n",
    "total_mac = profile_macs(gumbel_model.cuda(), torch.randn(2, 3,160, 160).cuda())\n",
    "print(total_mac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchprofile.utils.flatten import Flatten\n",
    "import warnings\n",
    "with warnings.catch_warnings(record=True):\n",
    "    graph, _ = torch.jit._get_trace_graph(Flatten(gumbel_model.cuda()), torch.randn(2,3,160,160).cuda(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = dict()\n",
    "for x in graph.nodes():\n",
    "    for v in list(x.inputs()):\n",
    "        if 'tensor' in v.type().kind().lower():\n",
    "            print(v.debugName(), v.type().scalarType(), v.type().sizes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in graph.nodes():\n",
    "    if 'mul_' in x.kind().lower():\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model_size = sum([p.numel() for p in ori_model.parameters()]) * 4 / 2**20\n",
    "gumbel_model_size = sum([p.numel() for p in gumbel_model.parameters()]) * 4 / 2**20\n",
    "print(\"Ori model size : %.1f MB\" % ori_model_size)\n",
    "print(\"Gumbel model size : %.1f MB\" % gumbel_model_size)\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(gumbel_model, input_size=(4, 3, 160, 160), col_width=16, col_names=['kernel_size', 'output_size', 'num_params', 'mult_adds', 'params_percent'], depth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before forward grad : \", gumbel_model.gumbel_fc1.weight.grad)\n",
    "out = gumbel_model(torch.randn(32, 3, 160, 160))\n",
    "\n",
    "out.sum().backward()\n",
    "\n",
    "print(\"after forward grad : \\n\", gumbel_model.gumbel_fc1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in net.named_parameters():\n",
    "    if has_deep_attr(model, n):\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbconv_test = MBGumbelInvertedConvLayer.build_from_config(m.mobile_inverted_conv.config)\n",
    "mbconv_test.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(2, 16, 32, 32)\n",
    "gumbel_inputs = torch.randn(2, 4, 8, 8)\n",
    "gumbel_inputs.requires_grad = True\n",
    "gumbel_layer = nn.Linear(4*8*8, 5)\n",
    "gumbel_output = gumbel_layer(gumbel_inputs.view(2, -1))\n",
    "gumbel_index = F.gumbel_softmax(gumbel_output, tau=1, hard=True)\n",
    "print(gumbel_index)\n",
    "out = mbconv_test.forward(torch.randn(2, 16, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(2, 16, 32, 32)\n",
    "gumbel_inputs = torch.randn(2, 4, 8, 8)\n",
    "gumbel_inputs.requires_grad = True\n",
    "gumbel_layer = nn.Linear(4*8*8, 5)\n",
    "gumbel_output = gumbel_layer(gumbel_inputs.view(2, -1))\n",
    "gumbel_index = F.gumbel_softmax(gumbel_output, tau=1, hard=True)\n",
    "print(gumbel_index)\n",
    "out = mbconv_test.forward(torch.randn(2, 16, 32, 32), gumbel_index)\n",
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_mbconv_test_weight = copy.deepcopy(mbconv_test.depth_conv.conv.weight)\n",
    "print(original_mbconv_test_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.mobile_inverted_conv.depth_conv.conv.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in m.mobile_inverted_conv.named_parameters():\n",
    "    if has_deep_attr(mbconv_test, n):\n",
    "        print(n, p)\n",
    "        set_deep_attr(mbconv_test, n, p)\n",
    "        print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in m.mobile_inverted_conv.named_parameters():\n",
    "    if has_deep_attr(mbconv_test, n):\n",
    "        print(n)\n",
    "        print(get_deep_attr(mbconv_test, n) - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbconv_test.forward(torch.randn(1,32,16,16), gumbel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_layer = nn.BatchNorm2d(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 12, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 12\n",
    "out = F.batch_norm(x, bn_layer.running_mean[:feature_dim], bn_layer.running_var[:feature_dim], bn_layer.weight[:feature_dim], bn_layer.bias[:feature_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, img_size, desc = build_model(net_id='mcunet-in4', pretrained=True)\n",
    "\n",
    "backup_model = copy.deepcopy(model)\n",
    "model_copy = build_model(net_id='mcunet-in4', pretrained=False)[0]\n",
    "\n",
    "for (n1, p1), (n2, p2) in zip(backup_model.named_parameters(), model_copy.named_parameters()):\n",
    "    if n1 == n2:\n",
    "        print((p1 - p2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if has_deep_attr(model_copy, n):\n",
    "        print(n)\n",
    "        set_deep_attr(model_copy, n, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (n1, p1), (n2, p2) in zip(backup_model.named_parameters(), model_copy.named_parameters()):\n",
    "    if n1 == n2:\n",
    "        print((p1-p2).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
